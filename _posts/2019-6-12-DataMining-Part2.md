---
published: true
title: æ•°æ®æŒ–æ˜æœŸæœ«å¤ä¹ æ±‡æ€»ï¼ˆäºŒï¼‰
category: Data Mining
tags: 
  - DataMining
  - Clustering
layout: post

---

# èšç±»

## the general idea  
1. ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ï¼Ÿ
ç›‘ç£å­¦ä¹ é€šå¸¸ç”¨æ¥è¿›è¡Œåˆ¤åˆ«åˆ†æã€‚  
æ— ç›‘ç£å­¦ä¹ ï¼Œæ˜¯äº‹å…ˆä¸çŸ¥é“æ ·æœ¬å±äºå“ªä¸€ç±»ã€‚  
ä½†äº‹å…ˆçŸ¥é“æ ·æœ¬çš„åˆ†ç±»ä¹Ÿå¯ä»¥è¿›è¡Œèšç±»ï¼Œå¯ä»¥ç”¨æ¥åˆ¤æ–­åˆ†ç±»æ˜¯å¦æ­£ç¡®ã€‚  
èšç±»æ˜¯ä¸€ç§<b>æ— ç›‘ç£å­¦ä¹ æ–¹æ³•</b>ã€‚  
<br />  
  
2. ä»€ä¹ˆæ˜¯èšç±»ï¼Ÿ  
èšç±»ï¼Œå°±æ˜¯å°†æ•°æ®æŒ‰ä¸€å®šçš„è§„åˆ™ã€æ–¹æ³•åˆ†æˆå‡ ç±»çš„æ–¹æ³•ã€‚  
èšç±»çš„æœ€ç»ˆç›®æ ‡ï¼Œå°±æ˜¯ä½¿<b>åŒä¸€ç±»ä¹‹é—´çš„æ–¹å·®å°½é‡å°ï¼Œä½¿ä¸åŒç±»ä¹‹é—´çš„æ–¹å·®å°½é‡å¤§</b>ã€‚  
èšç±»é€šå¸¸çš„æ­¥éª¤æ˜¯ï¼š
>1ã€Patten representation/å‡†å¤‡å·¥ä½œï¼šäº‹å…ˆæƒ³ä¸€æƒ³å‡†å¤‡åˆ†æˆå‡ ç±»ï¼Œé€‰ç”¨å“ªäº›å˜é‡ï¼Œæƒ³å¯¹å¤šå°‘æ ·æœ¬è¿›è¡Œèšç±»ï¼Œæ•°æ®å¦‚ä½•è¿›è¡Œåˆæ­¥å¤„ç†ï¼ˆå¦‚æ ‡å‡†åŒ–ï¼Œæ­£æ€åŒ–ï¼ŒåŒå‘åŒ–ç­‰ï¼‰ã€‚å¾ˆå¤šè¿™äº›å‰æœŸå·¥ä½œå¯èƒ½æ— æ³•å®ç°ï¼Œæœ‰å¾ˆå¤šå¯èƒ½ä¹Ÿæ²¡æœ‰å¿…è¦ã€‚  
2ã€Patten proximity measure/è·ç¦»åº¦é‡æ–¹æ³•çš„ç¡®å®šï¼šç‚¹å’Œç‚¹ä¹‹é—´ã€ç±»å’Œç±»ä¹‹é—´çš„è·ç¦»åº”å¦‚ä½•åº¦é‡ï¼Ÿç”¨æ¬§å¼è·ç¦»ï¼Ÿé©¬æ°è·ç¦»ï¼Ÿå…°æ°è·ç¦»ï¼Ÿè¿˜æ˜¯è‡ªå·±åˆ›é€ ä¸€ç§æ›´ç¬¦åˆå®é™…æƒ…å†µçš„è·ç¦»ï¼Ÿ  
é™¤äº†è·ç¦»åº¦é‡æ–¹æ³•çš„ç¡®å®šï¼Œè¿˜æœ‰ç±»å’Œç±»ä¹‹é—´çš„è¿æ¥æ–¹æ³•çš„ç¡®å®šï¼Œæ˜¯åº”è¯¥ç”¨æœ€çŸ­è·ç¦»æ³•ï¼Œè¿˜æ˜¯ç”¨æœ€é•¿è·ç¦»æ³•ï¼Ÿä¸åŒçš„æ–¹æ³•å¯èƒ½å¸¦æ¥ä¸åŒçš„åˆ†ç±»ç»“æœã€‚  
3ã€Grouping/èšç±»ï¼šç¡®å®šäº†ä¸Šè¿°ä¸‰æ­¥ä¹‹åï¼Œå°±å¯ä»¥è¿›è¡Œèšç±»äº†ã€‚ä½†èšç±»è¿‡ç¨‹ä¹Ÿå¹¶éå®¹æ˜“ï¼Œå½“ä¸€ä¸ªç±»ä¸å¦å¤–ä¸¤ä¸ªç±»çš„è·ç¦»ç›¸ç­‰ï¼Œè¯¥å¦‚ä½•ç»§ç»­å‘¢ï¼Ÿ  
4ã€Data abstraction/æŠ½è±¡å‡ºå„ç±»çš„å«ä¹‰ï¼šè¿™ä¸€æ­¥å¹¶éå¿…è¦ã€‚å¯ä»¥æœ‰ä¹Ÿå¯ä»¥æ²¡æœ‰ã€‚ç»™æ¯ä¸ªç±»å®šä¹‰ä¸€ä¸ªå«ä¹‰ï¼Œè¿™æ ·å¯ä»¥æ›´ç›´è§‚ï¼Œæ›´æ–¹ä¾¿ã€‚  
5ã€Cluster assessment/ç±»è¯„ä¼°ï¼šè¯„ä»·ä¸€ä¸‹åˆ†ç±»æ˜¯å¦æœ‰æ„ä¹‰ï¼Œæ˜¯å¦å‡†ç¡®ã€‚  


 æ— è®ºæ˜¯ä»€ä¹ˆèšç±»æ–¹æ³•ï¼ŒåŸºæœ¬çš„æ­¥éª¤éƒ½å¦‚ä¸Šæ‰€ç¤ºã€‚åŒºåˆ«ä¸»è¦åœ¨äºï¼Œä¸åŒçš„èšç±»æ–¹æ³•åœ¨è®¡ç®—ç±»é—´è·ç¦»å’Œè¿æ¥æ–¹å¼æ—¶å€™æœ‰æ‰€åŒºåˆ«ã€‚
<br />  

3. ç›®å‰å¹¿æ³›çš„èšç±»æ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼š<b>ä¸€ç§æ˜¯ç³»ç»Ÿèšç±»ï¼Œä¸€ç§æ˜¯Kå‡å€¼èšç±»</b>ã€‚  
<br />  
<b>å±‚æ¬¡èšç±»/ç³»ç»Ÿèšç±»</b>ã€€  
 * ç³»ç»Ÿèšç±»æ— éœ€äº‹å…ˆç¡®å®šéœ€è¦åˆ†å‡ ç±»ã€‚  
 * ç³»ç»Ÿèšç±»ä¸€èˆ¬ç”¨æ ‘çŠ¶å›¾(dendrogram)è¡¨ç¤ºã€‚  

>å±‚æ¬¡èšç±»æ–¹æ³•å…·ä½“åˆå¯åˆ†ä¸ºå‡èšçš„ï¼Œåˆ†è£‚çš„ä¸¤ç§æ–¹æ¡ˆã€‚ ã€€ã€€  
1. ç”±nä¸ªç±»èšæˆ1ç±»â€”â€”**å‡èšçš„å±‚æ¬¡èšç±»**æ˜¯ä¸€ç§<b>è‡ªåº•å‘ä¸Š</b>çš„ç­–ç•¥ï¼ˆagglomerative approachï¼‰ï¼Œé¦–å…ˆå°†æ¯ä¸ªå¯¹è±¡ä½œä¸ºä¸€ä¸ªç°‡ï¼Œç„¶ååˆå¹¶è¿™äº›åŸå­ç°‡ä¸ºè¶Šæ¥è¶Šå¤§çš„ç°‡ï¼Œç›´åˆ°æ‰€æœ‰çš„å¯¹è±¡éƒ½åœ¨ä¸€ä¸ªç°‡ä¸­ï¼Œæˆ–è€…æŸä¸ªç»ˆç»“æ¡ä»¶è¢«æ»¡è¶³ï¼Œç»å¤§å¤šæ•°å±‚æ¬¡èšç±»æ–¹æ³•å±äºè¿™ä¸€ç±»ï¼Œå®ƒä»¬åªæ˜¯åœ¨ç°‡é—´ç›¸ä¼¼åº¦çš„å®šä¹‰ä¸Šæœ‰æ‰€ä¸åŒã€‚   
2. ç”±1ä¸ªç±»åˆ†æˆnç±»â€”â€”**åˆ†è£‚çš„å±‚æ¬¡èšç±»**ä¸å‡èšçš„å±‚æ¬¡èšç±»ç›¸åï¼Œé‡‡ç”¨<b>è‡ªé¡¶å‘ä¸‹</b>çš„ç­–ç•¥ï¼ˆdivisive approachï¼‰ï¼Œå®ƒé¦–å…ˆå°†æ‰€æœ‰å¯¹è±¡ç½®äºåŒä¸€ä¸ªç°‡ä¸­ï¼Œç„¶åé€æ¸ç»†åˆ†ä¸ºè¶Šæ¥è¶Šå°çš„ç°‡ï¼Œç›´åˆ°æ¯ä¸ªå¯¹è±¡è‡ªæˆä¸€ç°‡ï¼Œæˆ–è€…è¾¾åˆ°äº†æŸä¸ªç»ˆæ­¢æ¡ä»¶ã€‚  
æˆ‘ä»¬å¹¶ä¸é‡ç‚¹ä»‹ç»åˆ†ç¦»æ–¹æ³•ï¼Œå› ä¸ºå®ƒä»æœ¬è´¨ä¸Šå’Œç³»ç»Ÿèšç±»æ–¹æ³•æ˜¯ä¸€æ ·çš„ã€‚   
ä½†æ˜¯å®ƒä¹Ÿæœ‰**ä¼˜ç‚¹**ï¼Œå½“æ•°æ®å®é™…æƒ…å†µåªæ˜¯åˆ†æˆå¾ˆå°‘çš„å‡ ç±»æ—¶ï¼Œç”¨åˆ†ç¦»æ³•èƒ½å¾ˆå¿«åˆ†å‡ºæ¥ï¼Œç”¨ç³»ç»Ÿèšç±»å°±éœ€è¦åˆ†å¾ˆä¹…ã€‚   

<br />  
**K-meansèšç±»**  
* éœ€è¦äº‹å…ˆç¡®å®šç±»æ•°  
* æ›´é€‚ç”¨äºå¤§æ ·æœ¬é›†  

## Distances
>Minkowski Distance é—µæ°è·ç¦»é€šå¼  
$d_{ij}(q)=(\sum_{k=1}^{p}|x_{ik}-x_{jk}|^q)^{1/q}$  

<br />
![è·ç¦»å›¾](/assets/images/201906DataMining/clustering_pic01.png)  
åˆ°ä¸­å¿ƒç‚¹è·ç¦»ç›¸åŒçš„ç‚¹ç”¨åŒä¸€ç§é¢œè‰²è¡¨ç¤º  
<br />  

1. Manhattan distance / Block distance   
$d_{ij}(1)=\sum_{k=1}^{p}|x_{ik}-x_{jk}|$  
$d_{AB}=|x_A-x_B|+|y_A-y_B|$
2. Euclidean distance  
$d_{ij}(2)=(\sum_{k=1}^{p}|x_{ik}-x_{jk}|^2)^{1/2}$  
$d_{AB}=\sqrt{(x_A-x_B)^2+(y_A-y_B)^2}$  
 * <font color='#9a4238'>the most frequently used  
 * ä¼˜ç‚¹ï¼šæœ‰éå¸¸æ˜ç¡®çš„ç©ºé—´è·ç¦»æ¦‚å¿µ
 * ç¼ºç‚¹  
    æ²¡æœ‰åŒºåˆ†é‡çº²çš„ä¸åŒï¼Œå°†æ‰€æœ‰å˜é‡è§†ä¸ºå½±å“ç›¸åŒçš„ â€”â€” é€‚ç”¨äºå·²åšæ ‡å‡†åŒ–å¤„ç†çš„æ•°æ®ï¼ˆpreprocessed dataï¼‰  
    å—ç»´åº¦çš„å½±å“ï¼Ÿ</font>  

3. Chebyshev distance  
$d_{ij}(\inf)=max|x_{ik}-x_{jk}|$  

4. Mahalanobis distance é©¬æ°è·ç¦»  
$d_{ij}(M)=((X_i-X_j)' A^{-1} (X_i-X_j))^{\frac{1}{2}}$  
$ğ´$ represents the variance matrix, $ğ‘‹_ğ‘–$ represents sample ğ‘– with ğ‘ dimensions  
 * <font color='#9a4238'>é©¬æ°è·ç¦»åˆç§°å¹¿ä¹‰æ¬§æ°è·ç¦»  
 * Taken variablesâ€™ correlation into consideration, if variables are independent, Mahalanobis distance turn into weighted Euclidean distance(å¦‚æœå„å˜é‡ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œå³è§‚æµ‹å˜é‡çš„åæ–¹å·®çŸ©é˜µæ˜¯å¯¹è§’çŸ©é˜µï¼Œåˆ™é©¬æ°è·ç¦»å°±é€€åŒ–ä¸ºç”¨å„ä¸ªè§‚æµ‹æŒ‡æ ‡çš„**æ ‡å‡†å·®çš„å€’æ•°ä½œä¸ºæƒæ•°çš„åŠ æƒæ¬§æ°è·ç¦»**).   
 * ä¸å—é‡çº²å½±å“ã€‚Taken variablesâ€™ variation into consideration,  and dimension problems can be solved.</font>  

<br />  
**è·ç¦»çš„é€‰æ‹©ï¼š**   
å®é™…ä¸­ï¼Œèšç±»åˆ†æå‰ä¸å¦¨è¯•æ¢æ€§åœ°å¤šé€‰æ‹©å‡ ä¸ªè·ç¦»å…¬å¼åˆ†åˆ«è¿›è¡Œèšç±»ï¼Œç„¶åå¯¹èšç±»åˆ†æçš„ç»“æœè¿›è¡Œå¯¹æ¯”åˆ†æï¼Œä»¥ç¡®å®šæœ€åˆé€‚çš„è·ç¦»æµ‹åº¦æ–¹æ³•ã€‚
    


## Linkage  
ri,sj - the ith / jth observation from group r/s  
1. Single Linkage æœ€çŸ­è·ç¦»æ³•  
$d_c(r,s)=\min{d(x_{ri},x_{sj})}, i=1,2,...,n_r, j=1,2,...,n_s$  
 * most oftenly used  
 * disadvantages: **chainingé“¾å¼ç»“æœ**. This comes about when clusters are not well separatedå½“ä¸¤ä¸ªç±»ä¹‹é—´ä¸èƒ½å¾ˆå¥½åœ°åˆ†ç¦», and snake-like chains can form. Observations at opposite ends of the chain can be very dissimilar, but yet they end up in the same cluster. 
 * Another issue with single linkage is that **it does not take the cluster structure into account/æ²¡æœ‰è€ƒè™‘åˆ°è·ç¦»ç»“æ„**.  
2. Complete Linkage æœ€é•¿è·ç¦»æ³•  
$d_c(r,s)=\max{d(x_{ri},x_{sj})}, i=1,2,...,n_r, j=1,2,...,n_s$  
 * Complete linkage is not susceptible to chainingä¸æ˜“è¡¨ç°å‡ºé“¾å¼ç»“æœ.
 * The resulting clusters tend to be spherical, and it **has difficulty recovering nonspherical groups**.ä½†ç»“æœå‘ˆçƒå½¢ã€‚  
 * Like single linkage, complete linkage **does not account for cluster structure**.æ²¡æœ‰è€ƒè™‘ç±»çš„è·ç¦»ç»“æ„ã€‚  
3. Average Linkage å¹³å‡è·ç¦»æ³•   
$d_c(r,s)=\frac{1}{n_r n_s}\sum_{i=1}^{n_r} \sum_{j=1}^{n_s}d(x_{ri},x_{sj}),  i=1,2,...,n_r,  j=1,2,...,n_s$  
 * This method tends to combine clusters that have small variances, and it also tends to **produce clusters with approximately equal variance**.
 * It is relatively robust and **does take the cluster structure into account**.  
4. Centroild Linkageé‡å¿ƒæ³•   
$d_c(r,s)=d(\bar{x_r},\bar{x_s})$, where $\bar{x_r}$ is the avg of obs in the r-th cluster and $\bar{x_s}$ the avg of obs in the s-th cluster.è€ƒè™‘äº†å…¨éƒ¨ç‚¹çš„ä¿¡æ¯ï¼Œä½†å­˜åœ¨ä¿¡æ¯æŸå¤±ã€‚  

## K-means

K-means æ˜¯äº‹å…ˆç¡®å®šç±»æ•°ï¼ˆKï¼‰çš„ï¼Œå³Rä¸­kmeans()å‡½æ•°çš„centerså‚æ•°  
æ¯”å±‚æ¬¡èšç±»æ³•æ›´é€‚ç”¨äºå¤§æ•°æ®é›†ï¼ˆå¤§æ•°æ®é›†è¦ä»nç±»èšåˆ°1ç±»å¤ªéš¾äº†ï¼‰  
<br />  

K-meansèšç±»çš„ç›®æ ‡ï¼š  
To partition the data into k groups such that the within-group sum-of-squares is minimized.  
Distortion Function $J(c,\mu)=\sum_{i=1}^n ||x^{(i)}-\mu_{c^{(i)}}||^2$ measures the sum of squared distances between each training example $x^{(i)}$ and the cluster centroid $\mu_{c^{(i)}}$ to which it has been assigned.  
<br />  

**K-meansèšç±»çš„æ­¥éª¤**  
>1.Specify the number of clusters k.  
2. Determine initial cluster centroids (randomly).   
3. Calculate the distance between each observation and each cluster centroid.  
4. Assign every observation to the closest cluster.  
5. Calculate the centroid of every cluster using the observations that were just grouped there.  
6. Repeat steps 3 through 5 until no more changes in result.  

<br />  
K-means ç®—æ³•ä¸€å®šä¼šæ”¶æ•›è‡³å±€éƒ¨æœ€ä¼˜ï¼Œä½†ä¸ä¸€å®šæ”¶æ•›è‡³å…¨å±€æœ€ä¼˜ã€‚  
>J must monotonically decrease, and the value of J must converge. However, the distortion function J is a non-convex function, and so coordinate descent on J is not guaranteed to converge to the global minimum. In other words, k-means can be susceptible to local optima.  
To avoid this, **use different random initial values for the cluster centroids $\mu^j$**. Then, out of all the different clusterings found, **pick the one that gives the lowest distortion $J(c,\mu)$**.


use **heatmap** to get a basic idea of what kmeans does


```R
data(iris)
dmat=as.matrix(iris[sample(1:100),-5])
km=kmeans(dmat, centers=3)
opt(4,4)
image(t(dmat),yaxt="n")
image(t(dmat[order(km$cluster),]),yaxt="n")
```


![png](/assets/images/201906DataMining/output_46_0.png)



![png](/assets/images/201906DataMining/output_46_1.png)



```R
library(ggplot2)
library(ggthemes)
library(cluster)
data=read.csv("DRfull.csv",header=TRUE)
head(data)
```


<table>
<thead><tr><th scope=col>Country.or.Area</th><th scope=col>D1</th><th scope=col>D12</th><th scope=col>D13</th><th scope=col>D15</th><th scope=col>D16</th><th scope=col>D17</th><th scope=col>D18</th><th scope=col>D2</th><th scope=col>D5</th><th scope=col>...</th><th scope=col>R14F</th><th scope=col>R14M</th><th scope=col>R14B</th><th scope=col>R6F</th><th scope=col>R6M</th><th scope=col>R6B</th><th scope=col>R8R</th><th scope=col>R8U</th><th scope=col>R9R</th><th scope=col>R9U</th></tr></thead>
<tbody>
	<tr><td>Afghanistan        </td><td>-2.4               </td><td>29825              </td><td>24                 </td><td>16.20              </td><td> 3.82              </td><td>47.42              </td><td>5.14               </td><td> 60                </td><td>35.3               </td><td>...                </td><td> 9.5               </td><td> 8.9               </td><td> 9.2               </td><td> 3.200             </td><td>13.100             </td><td> 9.800             </td><td>56                 </td><td>90                 </td><td>23                 </td><td>47                 </td></tr>
	<tr><td>Albania            </td><td>-0.3               </td><td> 3162              </td><td>55                 </td><td>32.56              </td><td>14.93              </td><td>21.33              </td><td>1.76               </td><td>111                </td><td>12.8               </td><td>...                </td><td> 9.0               </td><td>10.3               </td><td> 9.6               </td><td> 6.700             </td><td>17.600             </td><td>11.800             </td><td>94                 </td><td>97                 </td><td>86                 </td><td>95                 </td></tr>
	<tr><td>Algeria            </td><td>-1.9               </td><td>38482              </td><td>74                 </td><td>26.62              </td><td> 7.17              </td><td>27.42              </td><td>2.82               </td><td> 98                </td><td>24.6               </td><td>...                </td><td> 9.3               </td><td> 9.0               </td><td> 9.2               </td><td> 5.700             </td><td>25.500             </td><td>13.800             </td><td>79                 </td><td>85                 </td><td>88                 </td><td>98                 </td></tr>
	<tr><td>Angola             </td><td>-3.1               </td><td>20821              </td><td>60                 </td><td>16.18              </td><td> 3.84              </td><td>47.58              </td><td>5.98               </td><td> 47                </td><td>44.8               </td><td>...                </td><td> 8.7               </td><td> 8.2               </td><td> 8.5               </td><td>26.003             </td><td>31.837             </td><td>29.098             </td><td>34                 </td><td>68                 </td><td>20                 </td><td>87                 </td></tr>
	<tr><td>Antigua and Barbuda</td><td>-1.1               </td><td>   89              </td><td>30                 </td><td>30.26              </td><td>12.35              </td><td>25.96              </td><td>2.10               </td><td>143                </td><td>16.6               </td><td>...                </td><td>12.0               </td><td>11.3               </td><td>11.7               </td><td>15.900             </td><td>24.300             </td><td>20.100             </td><td>98                 </td><td>98                 </td><td>91                 </td><td>91                 </td></tr>
	<tr><td>Argentina          </td><td>-0.9               </td><td>41087              </td><td>93                 </td><td>30.83              </td><td>14.97              </td><td>24.42              </td><td>2.19               </td><td>152                </td><td>16.9               </td><td>...                </td><td>10.3               </td><td>11.0               </td><td>10.6               </td><td>29.700             </td><td>26.100             </td><td>28.000             </td><td>95                 </td><td>99                 </td><td>99                 </td><td>97                 </td></tr>
</tbody>
</table>




```R
df=data.frame(k=1:8,sw=NA)
pamlist=paste0("pam",df$k)
for (k in 1:8){
  pamk=pam(data[,-1],k)
  df[k,]$sw=pamk$silinfo$avg.width
}
```


```R
opt(5,2.5)
# plot(df$k,df$sw)
ggplot(df)+
  theme_bw()+
  geom_point(aes(k,sw),pch=19,size=2,col="#525288")+
  geom_line(aes(k,sw),col="#525288")+
  scale_x_continuous(breaks=1:8)
```




![png](/assets/images/201906DataMining/output_49_1.png)



```R
opt(10,10)
plot(pamk,col=pamk$cluster)
```

    Warning message in if (color) {:
    "the condition has length > 1 and only the first element will be used"Warning message in if (color) c(2, 4, 6, 3) else 5:
    "the condition has length > 1 and only the first element will be used"


![png](/assets/images/201906DataMining/output_50_1.png)



![png](/assets/images/201906DataMining/output_50_2.png)


B: number of monte carlo bootstrap samples


```R
gskmn=clusGap(data[,-1],FUN=kmeans,K.max=8,B=500)
gskmn
```


    Clustering Gap statistic ["clusGap"] from call:
    clusGap(x = data[, -1], FUNcluster = kmeans, K.max = 8, B = 500)
    B=500 simulated reference sets, k = 1..8; spaceH0="scaledPCA"
     --> Number of clusters (method 'firstSEmax', SE.factor=1): 1
             logW   E.logW      gap     SE.sim
    [1,] 14.94491 16.79945 1.854544 0.03294085
    [2,] 14.44529 16.11440 1.669101 0.03483898
    [3,] 14.09952 15.71344 1.613925 0.03457841
    [4,] 13.91641 15.43862 1.522205 0.03382952
    [5,] 13.65569 15.23681 1.581113 0.03518960
    [6,] 13.55290 15.08178 1.528885 0.04347128
    [7,] 13.37492 14.95503 1.580110 0.05228916
    [8,] 13.32660 14.84724 1.520637 0.05024694


## EMï¼ˆExpectation Maximizationï¼‰  
å‚è€ƒhttps://blog.csdn.net/zhihua_oba/article/details/73776553  
EMç®—æ³•æœ‰å¾ˆå¤šçš„åº”ç”¨ï¼Œæœ€å¹¿æ³›çš„å°±æ˜¯GMMæ··åˆé«˜æ–¯æ¨¡å‹ã€èšç±»ã€HMMç­‰ç­‰ã€‚æˆ‘ä»¬è®¨è®ºçš„æ˜¯EMåœ¨èšç±»ä¸­çš„åº”ç”¨ã€‚  
<br />
We know that clustering methods can be devided into two types:  
1. hard clustering: clusters do not overlap  
2. soft clustering: clusters may overlap  

**Mixture models**:
probabilistically-grounded way of doing soft clustering  
think of **each cluster** as a generative model that corresponds to a **probability distribution/PD** (Gaussian or multinomial) $\rightarrow$ we want to discover the parameters of the PD, i.e. the mean and covariance of the pdf   
EM allows us to figure out those parameters  
<br />
>so basically, using EM, we will find out the pdf of all k clusters, therefore can calc the prob of all sample points belonging to each clusters, and assign the sample points to the most likely cluster  

![em](/assets/images/201906DataMining/em_pic01.png)

<font color='#9a4238'>  
**EMçš„æ­¥éª¤**  
1. åˆå§‹åŒ–åˆ†å¸ƒå‚æ•°$\theta$ï¼ˆå‘é‡ï¼‰  
2. Eæ­¥éª¤ã€èšç±»è¿‡ç¨‹ã€‘ï¼šæ ¹æ®å½“å‰å‚æ•°$\theta$çš„å€¼è®¡ç®—å‡ºæ ·æœ¬$x^{(i)}\in j$çš„åéªŒæ¦‚ç‡ä¼°è®¡å€¼ï¼ˆæœŸæœ›Expectation, hence Eï¼‰  
$$w_j^{(i)}=p(c^{(i)}=j|x^{(i)};\theta)$$  
å…¶ä¸­$c^{(i)}$æ˜¯ç±»åˆ«,$x^{(i)}$æ˜¯æ ·æœ¬  
3. Mæ­¥éª¤ã€æ›´æ–°centroidã€‘ï¼šå°†ä¼¼ç„¶å‡½æ•°(Maximization, hence M)æœ€å¤§åŒ–ä»¥è·å¾—æ–°çš„å‚æ•°å€¼:  
$$\theta=argmax_{\theta}\sum_i \sum_j w_j^{(i)}\log{\frac{p(x^{(i)},c^{(i)};\theta)}{w_j^{(i)}}}$$
é‡å¤Eã€Mæ­¥éª¤ç›´è‡³æ”¶æ•›ã€‚
</font>

EMä¸K-meansçš„å¼‚åŒï¼š  
>å¼‚ï¼šEM is very much like k-meansï¼Œ except that  
 * K-means is a hard clustering method -- in k-means, a point either contribute to this centroid or that, we get an assignment $c(i)$
 * EM is a soft clustering method -- in EM, a point gives a little bit of its mass to every centroid, we get the probability for i to belong to all cluster $w_j^{(i)}$  

>åŒï¼šSimilar to K-means, EM is also susceptible to local optimaï¼Œå¯¹åˆå§‹å€¼æ•æ„Ÿï¼Œå®¹æ˜“å¾—åˆ°å±€éƒ¨æœ€ä¼˜è§£è€Œéå…¨å±€æœ€ä¼˜è§£ã€‚ so reinitializing at several different initial parameters may be a good idea.

## ç³»ç»Ÿèšç±»

<font size=3>**to get a basic idea**</font>   
the heatmap function is very quick to visualize the organization of these high dimensional data


```R
data(iris)
set.seed(143)
dmat=as.matrix(iris[sample(1:50),-5])
opt(5,5)
heatmap(dmat)
```


![png](/assets/images/201906DataMining/output_58_0.png)



```R
hc=hclust(dist(dmat))
clus=cutree(hc,4)

opt(4,4)
image(t(dmat),yaxt="n")
opt(4,4)
image(t(dmat[order(clus),]),yaxt="n")
```


![png](/assets/images/201906DataMining/output_59_0.png)



![png](/assets/images/201906DataMining/output_59_1.png)



```R
data(iris)
n=nrow(iris)
loc=sample(n, .7*n)
train=iris[loc,]
test=iris[-loc,]
```

<font size=3><b>?dist</b></font>  
><font color="red"> é»˜è®¤æ¬§æ°è·ç¦»</font>  

æ³¨æ„ä¹‹å‰æåˆ°è¿‡ï¼Œæ¬§å¼è·ç¦»å—é‡çº²å½±å“ï¼Œå› æ­¤é€‚ç”¨äºç»è¿‡äº†æ ‡å‡†åŒ–çš„æ•°æ®  

<font size=3><b>?rect.hclust</b></font>  
> Draws rectangles around the branches of a dendrogram highlighting the corresponding clusters.


```R
iris.hcl=hclust(dist(iris[,1:4]))     ###hclust for Hierarchical Clustering å±‚æ¬¡èšç±»
opt(6,4)
plot(iris.hcl,labels=FALSE,hang=-1)
re=rect.hclust(iris.hcl,k=3,border=c("#C04851","#7A7374","#2E317C"))
```


![png](/assets/images/201906DataMining/output_63_0.png)



```R
iris.id=cutree(iris.hcl,3)
#æŒ‰ç…§åˆ†æˆ3ç±»ï¼Œå¾—åˆ°æ ·æœ¬çš„åˆ†ç±»ç»“æœ
iris.id
```


<ol class=list-inline>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>3</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
</ol>



<font size=4>choose(n,k)</font>  
>è¿”å›$C_n^k$çš„å€¼ï¼Œå³$\frac{n!}{(n-k)!k!}$

## èšç±»è¯„ä»·

ã€éœ€è¦æŒæ¡ã€‘è¯„ä»·èšç±»æ•ˆæœçš„å…³é”®æŒ‡æ ‡ï¼šè½®å»“ç³»æ•°å’Œgap

### SilhouetteæŒ‡æ•°/è½®å»“ç³»æ•°

1990å¹´,Kaufmanå’ŒRousseeuwæå‡ºäº†silhouetteç»Ÿè®¡é‡å¯¹æœ€ä¼˜èšç±»ä»¥åŠæœ€ä¼˜èšç±»æ•°è¿›è¡Œä¼°è®¡ã€‚  
  
<br />  
<font color='#9a4238'>æ¯”è¾ƒä¸€ä¸ªç‚¹åˆ°æ‰€åœ¨ç±»å„ç‚¹çš„å¹³å‡è·ç¦»å’Œ**æœ€è¿‘çš„**å…¶ä»–ç±»å„ç‚¹çš„å¹³å‡è·ç¦»</font>  
å¯¹è§‚æµ‹å€¼$i$ï¼Œå®šä¹‰$a_i$ä¸º$i$ä¸æ‰€åœ¨ç±»çš„å…¶å®ƒç‚¹ä¹‹é—´çš„å¹³å‡è·ç¦»  
å¯¹äºä»»ä½•ä¸€ä¸ªå…¶ä»–çš„ç±»$c$ï¼Œå®šä¹‰$d(i,c)$ä¸º$i$ä¸$c$ç±»ï¼ˆé™¤è‡ªèº«ï¼‰ä¸­çš„æ‰€æœ‰ç‚¹çš„å¹³å‡è·ç¦»ï¼Œè®©$b_i$ä»£è¡¨$d(i,c)$çš„æœ€å°å€¼  
  
<br />  
  
åˆ™å¯¹äºè§‚æµ‹å€¼$i$ï¼Œå…¶Silhouette widthä¸ºï¼š  
$sw_i = \frac{b_i-a_i}{max(a_i,b_i)}$  ,$sw_i \in (-1,1)$  
* $sw_i$ é è¿‘1ï¼š$a_i$é è¿‘0ï¼Œ$i$è·ç¦»å½“å‰æ‰€åœ¨ç±»è¾ƒè¿‘  
* $sw_i$ é è¿‘0ï¼š$a_i$å’Œ$b_i$å·®ä¸å¤šå¤§  
* $sw_i$ é è¿‘-1ï¼š$b_i$ç›¸å¯¹äº$a_i$å¾ˆå°ï¼Œè¯´æ˜$i$æ²¡æœ‰è¢«è¾ƒå¥½åœ°å½’ç±»  
  
å¯¹æ‰€æœ‰è§‚æµ‹å€¼å–å¹³å‡å¾—åˆ°avg swä¸ºï¼š  
$\bar{sw} = \frac{1}{n}\sum_{i=1}^{n}sw_i$  
ç”¨$\bar{sw}$è¯„ä»·èšç±»æ•ˆæœï¼š  
* <font color='#9a4238'>$\bar{sw}$å€¼è¶Šå¤§ï¼Œèšç±»æ•ˆæœè¶Šå¥½ï¼Œä»è€Œç¡®å®šç±»æ•°$k$</font>  
According to Kaufman and Rousseeuw, 
 * an average silhouette width greater than 0.5 indicates a reasonable partition of the data
 * a value of less than 0.2 would indicate that the data do not exhibit cluster structure.

>pam: a more robust version of k-means


```R
library(cluster)

data=iris[,1:4]

pam3=pam(data,3)
pam3$silinfo$avg.width
  # ç›¸å½“äºmean(pam3$silinfo$widths[,3])
min(pam3$silinfo$widths[,3])
max(pam3$silinfo$widths[,3])
# silhouette(pam3) # ç›¸å½“äºpam3$silinfo$widths
```


0.55281901235641



0.0263588124292912



0.853905051398459



```R
opt(4,4)
plot(pam3,col=pam3$cluster)#Silhouetteã€€Plot
```

    Warning message in if (color) {:
    "the condition has length > 1 and only the first element will be used"Warning message in if (color) c(2, 4, 6, 3) else 5:
    "the condition has length > 1 and only the first element will be used"


![png](/assets/images/201906DataMining/output_72_1.png)



![png](/assets/images/201906DataMining/output_72_2.png)



```R
pam4=pam(data,4)
pam4$silinfo$avg.width #pam3çš„sw_avgæ¯”pam4çš„å¤§ï¼Œè¯´æ˜èšæˆ3ç±»æ•ˆæœæ›´å¥½
opt(4,4)
plot(pam4,col=pam4$cluster)ã€€
```


0.48969717913026


    Warning message in if (color) {:
    "the condition has length > 1 and only the first element will be used"Warning message in if (color) c(2, 4, 6, 3) else 5:
    "the condition has length > 1 and only the first element will be used"


![png](/assets/images/201906DataMining/output_73_2.png)



![png](/assets/images/201906DataMining/output_73_3.png)


<font color='#9a4238'>åˆ†ä¸º3ç±»æ—¶çš„sw_avg(0.55)æ¯”4ç±»(0.49)å¤§ï¼Œè¯´æ˜èšæˆ3ç±»æ•ˆæœæ›´å¥½</font>

### WSS(Within-cluster Sum of Squares)  
$D_r=\sum_{i \in C_r}\sum_{j \in C_r}||x_i-x_j||^2=2n_r \sum_{i \in C_r}||x_i-\bar{x}||^2$  
$W_k=\sum_{r=1}^{k}\frac{1}{2n_r}D_r$  
![WSS](/assets/images/201906DataMining/clustering_pic02.png)  
>problem of using $W_k$ to determine $k$  
 - no reference clustering to compare?  
 - the differences $W_k-W_{k-1}$ are not normalized for comparison  

### GapæŒ‡æ•°

2001å¹´,Tibshirani et al.æå‡ºäº†Gap Statistic methodå¯¹æœ€ä¼˜èšç±»ä»¥åŠæœ€ä¼˜èšç±»æ•°è¿›è¡Œä¼°è®¡ã€‚  
å»ºç«‹åœ¨ä¸Šè¿°$W_k$ä¹‹ä¸Šï¼šå‡è®¾å­˜åœ¨ä¸€ä¸ªå‚è€ƒåˆ†å¸ƒï¼Œæ¯”è¾ƒ$W_k$å’Œ$W_k$åœ¨å‚è€ƒåˆ†å¸ƒä¸‹çš„æœŸæœ›å€¼çš„å·®å¼‚  
<br />  
**å‚è€ƒåˆ†å¸ƒçš„é€‰å–**  
* ä¸€ç»´æƒ…å†µï¼š$[0,1]$ä¸Šçš„å‡åŒ€åˆ†å¸ƒ  
* å¤šç»´æƒ…å†µï¼š 
  * ç®€å•å‡åŒ€åˆ†å¸ƒ  
  * åˆ©ç”¨å¥‡å¼‚å€¼åˆ†è§£(SVD)æ–¹æ³•å¾—åˆ°å…³äºåŸæ•°æ®çš„ä¸»æˆåˆ†éƒ¨åˆ†ã€‚ä»ä¸€ç»„æœä»å‡åŒ€åˆ†å¸ƒçš„ä¸»æˆåˆ†æ•°æ®ä¸­äº§ç”Ÿå‚è€ƒç‰¹å¾ã€‚  

<br />  
**å‚è€ƒåˆ†å¸ƒä¸‹æœŸæœ›å€¼çš„æ±‚å–**  
äº§ç”Ÿ$B$ä¸ªæœä»å‚è€ƒåˆ†å¸ƒçš„å‚è€ƒæ•°æ®é›†ï¼Œç”¨åœ¨è¿™äº›å‚è€ƒæ•°æ®é›†ä¸‹çš„$\log{(W_k^*)}$æ¥ä¼°è®¡$E_n^*[\log{(W_k^*)}]$  
<br />  
   
$Gap_n(k)=E_n^*(\log{W_k})-\log{W_k}$   
<font color='#9a4238'>the larger Gap the better (find the k that maximizes Gap(k)</font>  
![gap](/assets/images/201906DataMining/clustering_pic03.png)  
but according to '1-standard-error' theory, we don't expect to use the k with the largest gap statistics  
but instead, we use the minimum k that satisfies:  
$Gap(k)>Gap(k+1)-s_{k+1}$  


```R
library(cluster)
data(iris)
data=iris[,1:4]

gskmn=clusGap(data,FUN=kmeans,K.max=8,B=500)# clusGap: Gap Statistic æ–¹æ³•
gskmn#æ˜¾ç¤ºæœ€ä¼˜çš„èšç±»æ•°ï½‹
opt(5,5)
plot(gskmn)#ç”»å›¾Gapå€¼
```


    Clustering Gap statistic ["clusGap"] from call:
    clusGap(x = data, FUNcluster = kmeans, K.max = 8, B = 500)
    B=500 simulated reference sets, k = 1..8; spaceH0="scaledPCA"
     --> Number of clusters (method 'firstSEmax', SE.factor=1): 4
             logW   E.logW        gap     SE.sim
    [1,] 4.551642 4.640852 0.08921047 0.02854059
    [2,] 3.808397 4.191117 0.38271951 0.02429483
    [3,] 3.519407 4.012884 0.49347687 0.02641911
    [4,] 3.368124 3.920477 0.55235364 0.02470481
    [5,] 3.301857 3.838111 0.53625411 0.02421139
    [6,] 3.224683 3.763125 0.53844187 0.02420139
    [7,] 3.112055 3.700077 0.58802177 0.02524475
    [8,] 3.127535 3.647548 0.52001255 0.02804035



![png](/assets/images/201906DataMining/output_78_1.png)



```R
logW=gskmn$Tab[,1]
E.logW=gskmn$Tab[,2]
gap=gskmn$Tab[,3]
sum(gap!=E.logW-logW)

dft=data.frame(n=rep(1:length(logW),2),val=c(logW,E.logW),lab=rep(c("logW","E.logW"),each=length(logW)))
opt(7,4)
ggplot(dft)+
  theme_bw()+
  geom_line(aes(n,val,color=lab))+
  geom_point(aes(n,val,color=lab),pch=19)+
  scale_color_manual(name="",values=c("#f2572d","#0067a6"))
```


0





![png](/assets/images/201906DataMining/output_79_2.png)


### å…°å¾·æŒ‡æ•°(Rand Index, RI)

RandæŒ‡æ•°çš„å®è´¨ï¼šè§‚å¯Ÿä¸€å¯¹æ ·æœ¬ç‚¹$x_i$å’Œ$x_j$åœ¨ä¸¤ç§èšç±»æ–¹æ³•ä¸­è¢«ç»„åˆçš„æ–¹å¼æ˜¯å¦ç›¸ä¼¼  
* ç›¸ä¼¼ï¼š
 * åœ¨$G1$å’Œ$G2$ä¸­ï¼Œ$x_i$å’Œ$x_j$å‡åŒç±»  
 * åœ¨$G1$å’Œ$G2$ä¸­ï¼Œ$x_i$å’Œ$x_j$å‡ä¸åŒç±»  
* ä¸åŒï¼š
 * åœ¨$G1$ä¸­ï¼Œ$x_i$å’Œ$x_j$åŒç±»ï¼Œåœ¨$G2$ä¸­ä¸åŒç±»  
 * åœ¨$G1$ä¸­ä¸åŒç±»ï¼Œåœ¨$G2$ä¸­åŒç±»  

ä¾‹å¦‚æ ·æœ¬ä¸º${a,b,c,d,e,f}$ï¼ŒG1:${(a,b,c),(d,e,f)}$ï¼ŒG2:${(a,b),(c,d,e),f}$  
åˆ™ç”±ä¸‹è¡¨çŸ¥ï¼ŒRI=9/15=0.6  
![rand index](/assets/images/201906DataMining/clustering_pic04.jpg)

åŒ¹é…çŸ©é˜µ$N$  
$RI$å¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ªå¾ˆé•¿çš„å…¬å¼  


```R
choose(150,2)
prod(1:150)/prod(1:148)/prod(1:2)
```


11175



11175



```R
data(iris)
hc=hclust(dist(iris[,-5]))
iris.id=cutree(hc,3)
# è®¡ç®—Rand Index
n=table(iris.id, iris$Species); n
ntot2=sum(n^2)
nitot=rowSums(n)   ##ni.
nitot2=sum(nitot^2)
njtot=colSums(n)   ##n.j
njtot2=sum(njtot^2)

RI=(choose(150,2)+ntot2-0.5*nitot2-0.5*njtot2)/choose(150,2)
RI
```


           
    iris.id setosa versicolor virginica
          1     50          0         0
          2      0         23        49
          3      0         27         1



0.836778523489933



```R
#è°ƒæ•´çš„rand index: pdfCluster::adj.rand.index
library(pdfCluster)   
adj.rand.index(iris.id,iris[,5])   
```


0.64225125183629


### å…±è¡¨å‹ç›¸å…³ç³»æ•°

https://www.cnblogs.com/f-young/p/9248247.html  
Essentially: to measure èšç±»æ–¹æ³•å¾—åˆ°çš„ç»“æ„å’ŒåŸå§‹æ•°æ®ç»“æ„çš„ç›¸ä¼¼æ€§ï¼Œç›¸ä¼¼åˆ™ä¼˜ï¼ˆå³æœªç ´ååŸå§‹æ•°æ®çš„ç»“æ„ï¼‰  
<br />  

CopheneticçŸ©é˜µ$H$çš„æ•°å€¼æ˜¯æ ¹æ®åŸå§‹æ•°æ®ç‚¹ä¹‹é—´çš„è·ç¦»çŸ©é˜µå¾—æ¥çš„ï¼Œå…¶ä¸­$n_{ij}$æ˜¯æ ·æœ¬ç‚¹$i$å’Œ$j$ç¬¬ä¸€æ¬¡è¢«èšä¸ºä¸€ç±»æ—¶ï¼Œè¯¥ç±»å†…å„ç‚¹ä¹‹é—´è·ç¦»çš„æœ€å°å€¼ã€‚  
<br />  

1. å¾—åˆ°åŸå§‹æ•°æ®ç‚¹çš„è·ç¦»çŸ©é˜µ  
 ![è·ç¦»](/assets/images/201906DataMining/clustering_pic06.png)  
2. å¾—åˆ°å…±è¡¨(Cophenetic)çŸ©é˜µ  
  > å…±è¡¨çŸ©é˜µçš„è®¡ç®—ï¼šä½¿ç”¨åœ¨åˆå¹¶ç±»çš„è¿‡ç¨‹ä¸­ä½¿ç”¨çš„æœ€å°è·ç¦»æ¥å¡«å……è·ç¦»çŸ©é˜µçš„ä¸‹ä¸‰è§’ã€‚  
  ä¾‹ï¼šå¼€å§‹æœ‰6ä¸ªç±»ï¼šAï¼ŒBï¼ŒCï¼ŒDï¼ŒEï¼ŒF$\rightarrow$å°†è·ç¦»æœ€å°ä¸º0.5çš„Då’ŒFèšä¸ºä¸€ç±»(D,F)$\rightarrow$å°†è·ç¦»æœ€å°ä¸º0.71çš„Aå’ŒBèšä¸ºä¸€ç±»(A,B)$\rightarrow$å°†è·ç¦»æœ€å°ä¸º1.00çš„(D,F)å’ŒEèšä¸ºä¸€ç±»(D,F,E)$\rightarrow$å°†è·ç¦»æœ€å°ä¸º1.41çš„(D,F,E)å’ŒCèšä¸ºä¸€ç±»(D,F,E,C)$\rightarrow$å°†è·ç¦»æœ€å°ä¸º2.50çš„(D,F,E,C)å’Œ(A,B)èšä¸ºä¸€ç±»(D,F,E,C,A,B)$\rightarrow$èšç±»ç»“æŸ  
  å¯¹åº”çš„å…±è¡¨çŸ©é˜µæ˜¯ï¼š  
  ![å…±è¡¨](/assets/images/201906DataMining/clustering_pic05.png)
3. ä¸¤ä¸ªçŸ©é˜µï¼ˆå¯¹åº”ç”Ÿæˆä¸¤ä¸ªå‘é‡ï¼Ÿï¼‰çš„ç›¸å…³ç³»æ•°å³ä¸ºå…±è¡¨å‹ç›¸å…³ç³»æ•°  
  
<font color='#9a4238'>å…±è¡¨å‹ç›¸å…³ç³»æ•°ï¼šè¶Šå¤§è¶Šå¥½</font>


```R
d=dist(iris[,1:4])   #é¸¢å°¾å±æ¤ç‰©å„ä¸ªæ ·æœ¬ç‚¹çš„è·ç¦»çŸ©é˜µ

# æ¯”è¾ƒæœ€å°è·ç¦»æ³•ä¸æœ€å¤§è·ç¦»æ³•çš„ä¼˜åŠ£

hc1=hclust(d,"complete") #æœ€å¤§è·ç¦»æ³•
d1=cophenetic(hc1) #å…±è¡¨çŸ©é˜µ
cor(d,d1)  #å…±è¡¨çŸ©é˜µä¸è·ç¦»çŸ©é˜µä¹‹é—´çš„ç›¸å…³ç³»æ•°

hc2=hclust(d,"single")  ##æœ€å°è·ç¦»æ³•
d2=cophenetic(hc2) ##å…±è¡¨çŸ©é˜µ
cor(d,d2)  ##ç›¸å…³ç³»æ•°
```


0.726985683628462



0.863878677307658


è¾“å‡ºç»“æœåˆ†åˆ«ä¸º0.7269857ã€0.8638787  
é‡‡ç”¨æœ€å°è·ç¦»æ³•è¦å¥½äºæœ€å¤§è·ç¦»æ³•ã€‚