---
published: true
title: æ•°æ®æŒ–æ˜æœŸæœ«å¤ä¹ æ±‡æ€»
category: Data Mining
tags: 
  - DataMining
  - è¯¾å†…
layout: post

---

```R
library(ggplot2)
library(ggthemes)
opt=function(wid,hei) {options(repr.plot.width=wid,repr.plot.height=hei)}
# color - bright lemon - #F8D86A
```

# General View 

> * ä¸ç¨³å®šçš„ç®—æ³•: å†³ç­–æ ‘ï¼Œç¥ç»ç½‘ç»œï¼ŒMARS (multivariate splines)ï¼Œå’Œå­é›†å›å½’(subset regression)ç­‰ç­‰ï¼›  
* ç¨³å®šçš„ç®—æ³•: å²­å›å½’(ridge regression)ï¼Œæœ€è¿‘é‚»æ–¹æ³•(K-nearest neighbor)ï¼Œå’Œçº¿æ€§åˆ¤åˆ«æ–¹æ³•(Linear discriminate)ç­‰ç­‰ã€‚

## Classification Methods
FAMILY of classification methods!  
 * Logistic Regression  
 * Clustering (K Nearest Neighbors etc.)   
 * Decision Tress  
 * Naive Bayes?  
 * SVM  
 * Neural Networks  
 ![class](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/class_pic01.png)  
 SVMä½¿ç”¨äº†éçº¿æ€§åˆ†ç±»å™¨ï¼Œè€Œlogisticså’Œdecision treeéƒ½æ˜¯çº¿æ€§çš„ï¼Ÿ

## æŸå¤±å‡½æ•°
é€šå¸¸æœºå™¨å­¦ä¹ æ¯ä¸€ä¸ªç®—æ³•ä¸­éƒ½ä¼šæœ‰ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼Œç®—æ³•çš„æ±‚è§£è¿‡ç¨‹æ˜¯é€šè¿‡å¯¹è¿™ä¸ªç›®æ ‡å‡½æ•°ä¼˜åŒ–çš„è¿‡ç¨‹ã€‚åœ¨åˆ†ç±»æˆ–è€…å›å½’é—®é¢˜ä¸­ï¼Œé€šå¸¸ä½¿ç”¨æŸå¤±å‡½æ•°ä½œä¸ºå…¶ç›®æ ‡å‡½æ•°ã€‚æŸå¤±å‡½æ•°ç”¨æ¥è¯„ä»·æ¨¡å‹çš„é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¸ä¸€æ ·çš„ç¨‹åº¦ï¼ŒæŸå¤±å‡½æ•°è¶Šå¥½ï¼Œé€šå¸¸æ¨¡å‹çš„æ€§èƒ½è¶Šå¥½ã€‚ä¸åŒçš„ç®—æ³•ä½¿ç”¨çš„æŸå¤±å‡½æ•°ä¸ä¸€æ ·ã€‚  
<br />

<font color='#F8D86A'>æŸå¤±å‡½æ•°åˆ†ä¸ºç»éªŒé£é™©æŸå¤±å‡½æ•°å’Œç»“æ„é£é™©æŸå¤±å‡½æ•°ã€‚ç»éªŒé£é™©æŸå¤±å‡½æ•°æŒ‡é¢„æµ‹ç»“æœå’Œå®é™…ç»“æœçš„å·®åˆ«ï¼Œç»“æ„é£é™©æŸå¤±å‡½æ•°æ˜¯æŒ‡ç»éªŒé£é™©æŸå¤±å‡½æ•°åŠ ä¸Šæ­£åˆ™é¡¹</font>ã€‚é€šå¸¸è¡¨ç¤ºä¸ºå¦‚ä¸‹ï¼š  
$\theta^* = argmin \frac{1}{N} \sum_{i=1}^N L(y_i,f(x_i, \theta_i)+\lambda\Phi(\theta))$  
å…¶ä¸­$\frac{1}{N} \sum_{i=1}^N L(y_i,f(x_i, \theta_i)$æ˜¯ç»éªŒé£é™©å‡½æ•°ï¼Œ$L(\dot{})$è¡¨ç¤ºæŸå¤±å‡½æ•°  
$\lambda\Phi(\theta)$æ˜¯æ­£åˆ™åŒ–é¡¹(regularizer)æˆ–æƒ©ç½šé¡¹(penalty term)ï¼Œå®ƒå¯ä»¥æ˜¯$L1$ã€$L2$æˆ–è€…å…¶ä»–æ­£åˆ™å‡½æ•°ã€‚  

**å¸¸è§çš„æŸå¤±å‡½æ•°**  
1. 0-1æŸå¤±å‡½æ•°  
$L(Y,f(X))=\begin{cases}
  1,Y\neq f(X)\\
  0,Y= f(X)
\end{cases}$  
æ„ŸçŸ¥æœºç”¨çš„æ˜¯è¿™ç§æŸå¤±å‡½æ•°ï¼Œä½†ç›¸ç­‰è¿™ä¸€æ¡ä»¶è¿‡äºä¸¥æ ¼ï¼Œå¯ä»¥æ”¾å®½åˆ°æ»¡è¶³$|Y-f(X)|<T$æ—¶è®¤ä¸ºç›¸ç­‰  

2. ç»å¯¹å€¼æŸå¤±å‡½æ•°  
$L(Y,f(X))=|Y-f(X)|$  
  * æœ€å°ä¸€ä¹˜æ³•  
3. logå¯¹æ•°æŸå¤±å‡½æ•°  
$L(Y,P(Y|X))=-\log{P(Y|X)}$  
  * logisticå›å½’çš„æŸå¤±å‡½æ•°å°±æ˜¯å¯¹æ•°æŸå¤±å‡½æ•°  
4. æŒ‡æ•°æŸå¤±å‡½æ•°  
æ ‡å‡†å½¢å¼ï¼š$L(Y,f(X))=\exp{(-Y f(X))}$  
  * AdaBoosté‡‡ç”¨æŒ‡æ•°æŸå¤±å‡½æ•°  
5. å¹³æ–¹æŸå¤±å‡½æ•°  
$L(Y,f(X))=\sum_N{(Y-f(X))^2}$  
  * æœ€å°äºŒä¹˜æ³•  
6. Hinge Loss 
$L(y)=\max{(0,1-ty)}$  
  * ç”¨äºæœ€å¤§é—´éš”åˆ†ç±»ï¼Œä¾‹å¦‚æ”¯æŒå‘é‡æœºSVM  


# å¤šå…ƒå›å½’çš„å˜é‡é€‰æ‹©  
<font color=#F8D864>**ç®€å•è€ƒï¼Œé€‰æ‹©åˆ¤æ–­**</font>  
1. å¤šé‡å…±çº¿æ€§çš„å®šä¹‰  
 * å…±çº¿æ€§ï¼šæŸä¸€å¯¹è‡ªå˜é‡çš„ç›¸å…³ç³»æ•°å¾ˆé«˜(say 0.98)
 * å¤šé‡å…±çº¿æ€§ï¼šè™½ç„¶æ²¡æœ‰ä»»ä½•ä¸€å¯¹æé«˜çš„ç›¸å…³ç³»æ•°ï¼Œä½†å¤šå¯¹è‡ªå˜é‡ä¹‹é—´çš„ç›¸å…³ç³»æ•°éƒ½å……åˆ†é«˜ï¼ˆsay 0.85ï¼‰ï¼Œä¹Ÿå³ï¼Œè™½ç„¶ind3~ind1çš„æ‹Ÿåˆæ•ˆæœä¸ä½³ï¼ˆind1æ— æ³•å……åˆ†è§£é‡Šind3çš„æ–¹å·®ï¼‰ï¼Œind3~ind1+ind2çš„æ‹Ÿåˆæ•ˆæœå´å¾ˆå¥½ï¼ˆind3é€ æˆçš„æ–¹å·®å¯ä»¥å……åˆ†è¢«ind1å’Œind2è§£é‡Š/accounted forï¼‰
2. å¤šé‡å…±çº¿æ€§çš„å‡ºç°  
 * Fæ£€éªŒé€šè¿‡ï¼ŒTæ£€éªŒä¸é€šè¿‡
 >ä¸€å…ƒå›å½’ä¸­Få’ŒTæ£€éªŒç­‰ä»·ï¼Œä½†å¤šå…ƒå›å½’ä¸­Fæ£€éªŒç”¨æ¥æ£€éªŒæ€»ä½“å›å½’çš„æ˜¾è‘—æ€§ï¼Œè€ŒTæ£€éªŒæ˜¯æ£€éªŒå„ä¸ªå‚æ•°å„è‡ªçš„æ˜¾è‘—æ€§  
 * å›å½’ç³»æ•°tæ£€éªŒä¸æ˜¾è‘—ï¼Œæœ‰äº›ç³»æ•°absurdlyå˜å·  
 * æ˜¾è‘—æ€§æ°´å¹³ä¸‹é™ï¼Ÿï¼ˆR^2å¾ˆé«˜)
 * æ ‡å‡†å·®è†¨èƒ€ï¼ˆVIFï¼‰
 * å¢åˆ ä¸€ä¸ªè‡ªå˜é‡å¯¼è‡´æ¨¡å‹å‘ç”Ÿæ˜¾è‘—çš„å˜åŒ–
 * ã€worst caseã€‘æ¨¡å‹ä¸æ”¶æ•›ï¼Œno solution  
3. å¤šé‡å…±çº¿æ€§çš„åº¦é‡
 * Tolerance  
 $Tolerance=1-R^2$  
 è¶Šå¤§è¶Šå¥½  
 $<0.1(æˆ–0.2)$æ—¶å¯èƒ½å­˜åœ¨å¤šé‡å…±çº¿æ€§é—®é¢˜  
 * VIF(variance inflation factor)  
 $VIF=\frac{1}{Tolearance}=\frac{1}{1-R^2}$  
 è¶Šå°è¶Šå¥½  
 $>10(æˆ–4)$æ—¶å¯èƒ½å­˜åœ¨å¤šé‡å…±çº¿æ€§é—®é¢˜  
 ç›´è§‚ç†è§£ï¼š$VIF=10$ è¯´æ˜æ ‡å‡†å·®(standard errorï¼‰ä¼šè†¨èƒ€10å€ï¼ˆç›¸è¾ƒæ— å¤šé‡å…±çº¿æ€§çš„æƒ…å†µï¼‰  
4. å¤šé‡å…±çº¿æ€§çš„è§£å†³æ–¹æ³•  
 * åˆ å»ä¸€äº›å¤šä½™çš„å˜é‡ï¼šå­é›†å›å½’ã€å‘å‰å‘åé€æ­¥å›å½’   
 * å°†ç›¸ä¼¼çš„å˜é‡äºˆä»¥åŠ æ€»å†å¼•å…¥æ¨¡å‹
 * å¢å¤§æ ·æœ¬å®¹é‡ï¼ˆæ§åˆ¶$R^2$ç­‰ï¼Ÿï¼‰




```R
data(mtcars)
str(mtcars)
```

    'data.frame':	32 obs. of  11 variables:
     $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
     $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
     $ disp: num  160 160 108 258 360 ...
     $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
     $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
     $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
     $ qsec: num  16.5 17 18.6 19.4 17 ...
     $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
     $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
     $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
     $ carb: num  4 4 1 1 2 1 4 2 2 4 ...
    


```R
mfull=lm(mpg~.,data=mtcars)
summary(mfull)
```


    
    Call:
    lm(formula = mpg ~ ., data = mtcars)
    
    Residuals:
        Min      1Q  Median      3Q     Max 
    -3.4506 -1.6044 -0.1196  1.2193  4.6271 
    
    Coefficients:
                Estimate Std. Error t value Pr(>|t|)  
    (Intercept) 12.30337   18.71788   0.657   0.5181  
    cyl         -0.11144    1.04502  -0.107   0.9161  
    disp         0.01334    0.01786   0.747   0.4635  
    hp          -0.02148    0.02177  -0.987   0.3350  
    drat         0.78711    1.63537   0.481   0.6353  
    wt          -3.71530    1.89441  -1.961   0.0633 .
    qsec         0.82104    0.73084   1.123   0.2739  
    vs           0.31776    2.10451   0.151   0.8814  
    am           2.52023    2.05665   1.225   0.2340  
    gear         0.65541    1.49326   0.439   0.6652  
    carb        -0.19942    0.82875  -0.241   0.8122  
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    
    Residual standard error: 2.65 on 21 degrees of freedom
    Multiple R-squared:  0.869,	Adjusted R-squared:  0.8066 
    F-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07
    


note that AIC & BIC: the lower the merrier


```R
step(mfull, direction="backward")
```

    Start:  AIC=70.9
    mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb
    
           Df Sum of Sq    RSS    AIC
    - cyl   1    0.0799 147.57 68.915
    - vs    1    0.1601 147.66 68.932
    - carb  1    0.4067 147.90 68.986
    - gear  1    1.3531 148.85 69.190
    - drat  1    1.6270 149.12 69.249
    - disp  1    3.9167 151.41 69.736
    - hp    1    6.8399 154.33 70.348
    - qsec  1    8.8641 156.36 70.765
    <none>              147.49 70.898
    - am    1   10.5467 158.04 71.108
    - wt    1   27.0144 174.51 74.280
    
    Step:  AIC=68.92
    mpg ~ disp + hp + drat + wt + qsec + vs + am + gear + carb
    
           Df Sum of Sq    RSS    AIC
    - vs    1    0.2685 147.84 66.973
    - carb  1    0.5201 148.09 67.028
    - gear  1    1.8211 149.40 67.308
    - drat  1    1.9826 149.56 67.342
    - disp  1    3.9009 151.47 67.750
    - hp    1    7.3632 154.94 68.473
    <none>              147.57 68.915
    - qsec  1   10.0933 157.67 69.032
    - am    1   11.8359 159.41 69.384
    - wt    1   27.0280 174.60 72.297
    
    Step:  AIC=66.97
    mpg ~ disp + hp + drat + wt + qsec + am + gear + carb
    
           Df Sum of Sq    RSS    AIC
    - carb  1    0.6855 148.53 65.121
    - gear  1    2.1437 149.99 65.434
    - drat  1    2.2139 150.06 65.449
    - disp  1    3.6467 151.49 65.753
    - hp    1    7.1060 154.95 66.475
    <none>              147.84 66.973
    - am    1   11.5694 159.41 67.384
    - qsec  1   15.6830 163.53 68.200
    - wt    1   27.3799 175.22 70.410
    
    Step:  AIC=65.12
    mpg ~ disp + hp + drat + wt + qsec + am + gear
    
           Df Sum of Sq    RSS    AIC
    - gear  1     1.565 150.09 63.457
    - drat  1     1.932 150.46 63.535
    <none>              148.53 65.121
    - disp  1    10.110 158.64 65.229
    - am    1    12.323 160.85 65.672
    - hp    1    14.826 163.35 66.166
    - qsec  1    26.408 174.94 68.358
    - wt    1    69.127 217.66 75.350
    
    Step:  AIC=63.46
    mpg ~ disp + hp + drat + wt + qsec + am
    
           Df Sum of Sq    RSS    AIC
    - drat  1     3.345 153.44 62.162
    - disp  1     8.545 158.64 63.229
    <none>              150.09 63.457
    - hp    1    13.285 163.38 64.171
    - am    1    20.036 170.13 65.466
    - qsec  1    25.574 175.67 66.491
    - wt    1    67.572 217.66 73.351
    
    Step:  AIC=62.16
    mpg ~ disp + hp + wt + qsec + am
    
           Df Sum of Sq    RSS    AIC
    - disp  1     6.629 160.07 61.515
    <none>              153.44 62.162
    - hp    1    12.572 166.01 62.682
    - qsec  1    26.470 179.91 65.255
    - am    1    32.198 185.63 66.258
    - wt    1    69.043 222.48 72.051
    
    Step:  AIC=61.52
    mpg ~ hp + wt + qsec + am
    
           Df Sum of Sq    RSS    AIC
    - hp    1     9.219 169.29 61.307
    <none>              160.07 61.515
    - qsec  1    20.225 180.29 63.323
    - am    1    25.993 186.06 64.331
    - wt    1    78.494 238.56 72.284
    
    Step:  AIC=61.31
    mpg ~ wt + qsec + am
    
           Df Sum of Sq    RSS    AIC
    <none>              169.29 61.307
    - am    1    26.178 195.46 63.908
    - qsec  1   109.034 278.32 75.217
    - wt    1   183.347 352.63 82.790
    


    
    Call:
    lm(formula = mpg ~ wt + qsec + am, data = mtcars)
    
    Coefficients:
    (Intercept)           wt         qsec           am  
          9.618       -3.917        1.226        2.936  
    



```R
mmin=lm(mpg~1, data=mtcars)
step(mmin, direction='forward',scope=~cyl+disp+hp+drat+wt+qsec+vs+am+gear+carb,k=2)
```

    Start:  AIC=115.94
    mpg ~ 1
    
           Df Sum of Sq     RSS     AIC
    + wt    1    847.73  278.32  73.217
    + cyl   1    817.71  308.33  76.494
    + disp  1    808.89  317.16  77.397
    + hp    1    678.37  447.67  88.427
    + drat  1    522.48  603.57  97.988
    + vs    1    496.53  629.52  99.335
    + am    1    405.15  720.90 103.672
    + carb  1    341.78  784.27 106.369
    + gear  1    259.75  866.30 109.552
    + qsec  1    197.39  928.66 111.776
    <none>              1126.05 115.943
    
    Step:  AIC=73.22
    mpg ~ wt
    
           Df Sum of Sq    RSS    AIC
    + cyl   1    87.150 191.17 63.198
    + hp    1    83.274 195.05 63.840
    + qsec  1    82.858 195.46 63.908
    + vs    1    54.228 224.09 68.283
    + carb  1    44.602 233.72 69.628
    + disp  1    31.639 246.68 71.356
    <none>              278.32 73.217
    + drat  1     9.081 269.24 74.156
    + gear  1     1.137 277.19 75.086
    + am    1     0.002 278.32 75.217
    
    Step:  AIC=63.2
    mpg ~ wt + cyl
    
           Df Sum of Sq    RSS    AIC
    + hp    1   14.5514 176.62 62.665
    + carb  1   13.7724 177.40 62.805
    <none>              191.17 63.198
    + qsec  1   10.5674 180.60 63.378
    + gear  1    3.0281 188.14 64.687
    + disp  1    2.6796 188.49 64.746
    + vs    1    0.7059 190.47 65.080
    + am    1    0.1249 191.05 65.177
    + drat  1    0.0010 191.17 65.198
    
    Step:  AIC=62.66
    mpg ~ wt + cyl + hp
    
           Df Sum of Sq    RSS    AIC
    <none>              176.62 62.665
    + am    1    6.6228 170.00 63.442
    + disp  1    6.1762 170.44 63.526
    + carb  1    2.5187 174.10 64.205
    + drat  1    2.2453 174.38 64.255
    + qsec  1    1.4010 175.22 64.410
    + gear  1    0.8558 175.76 64.509
    + vs    1    0.0599 176.56 64.654
    


    
    Call:
    lm(formula = mpg ~ wt + cyl + hp, data = mtcars)
    
    Coefficients:
    (Intercept)           wt          cyl           hp  
       38.75179     -3.16697     -0.94162     -0.01804  
    



```R
library(leaps)
msub=regsubsets(mpg~., data=mtcars)
plot(msub)
```


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_11_0.png)


# Ridge & Lassoå›å½’

è§£å†³å¤šé‡å…±çº¿æ€§ - å²­å›å½’å’ŒLasso https://blog.csdn.net/bbbeoy/article/details/72526164  
Lasso - https://blog.csdn.net/qq_38650208/article/details/88766738 [è®²å¾—å¾ˆæ¸…æ¥š]  
Lars - https://blog.csdn.net/nya0731/article/details/79895307  
 https://blog.csdn.net/mousever/article/details/50513525


å²­å›å½’å’ŒLassoå›å½’åœ¨ç»éªŒæŸå¤±å‡½æ•°(adopted by OLS)çš„åŸºç¡€ä¸ŠåŠ å…¥äº†æ­£åˆ™é¡¹/æƒ©ç½šé¡¹ï¼Œä»¥é€‚å½“çš„ç¼©å°ç³»æ•°ï¼Œç”šè‡³ç¼©å°è‡³0ä»è€Œå‡å°‘è‡ªå˜é‡çš„ä¸ªæ•°ï¼ˆLassoï¼‰ã€‚  
* Hence, they have more bias than the least squared methods, but less variance  
* in other words, they provide a slightly worse fit with much better long-term prediction  

<br />  
  
traditional ***least squared regression*** seeks to minimize **the sum of the squared residual**  
$$\sum_{i=1}^{n}(\hat{y_i}-y_i)^2$$  

## Ridge Regression  
go to https://www.youtube.com/watch?v=Q81RR3yKn30  
<br />  

**Ridge regression** adds **a penalty to the slope**  on top of that:  
$$\sum_{i=1}^{n}(\hat{y_i}-y_i)^2 + \lambda\ast coef^2, \quad where\, \lambda \in [0,+\inf) $$  
$\lambda$ here is an important parameter, and is determined through **cross-validation**  
when $\lambda = 0$ ridge reg is no different from least squared reg  

>Why add penalties to the slope, though?  
 -- small sample size can lead to poorly fitted least squared reg (though the results look well for the **training set**, it doesn't do well for the **testing set**)  ç±»ä¼¼äºoverfitting   
 -- when the slope of the line is steep, the prediction for y is very sensitive to relatively small changes in x  
 -- ridge reg basically **shrink the paramiters**  
 
>if there is a lot of independent variables i.e. a lot of parameters/ coefficients to be estimated  
 ridge reg minimize $\lambda \ast (coef_1^2+coef_2^2+...+coef_p^2)$  
 note that the **y-intercept** is not included  
 
>Ridge Reg can also be applied to logistics regression  
 * because logit reg is solved using maximum likelihood rather than least squared, it tends to optimize(maximize) the sum of the likelihood  
 * ridge reg applied to logit tends to optimize the sum of the likelihood + $\lambda \ast coef^2$  
 
 > one REALLY COOL thing about ridge reg  
 -- we know that to estimate a model with 2 variables through least squared, there needs to be at least 2 obs in the datasets  
 to estimate a model with 10000 avriables through least squared (e.g. the influence of 10000 genes on the size of mice), there needs to be at leasts 10000 obs, which is very unlikely  
 but <font color='#F8D86A'> with Ridge Reg we can estimate the model with 10000 variables using only 500 or less obs  
 to summ up, Ridge Reg allows us to estimate a model where p>n or p>>n é«˜ç»´æ•°æ®é›†! </font> 
 how? think about it -- cross-validation has a lot to do with it.
 

## LASSO  
The Least Absolute **Shrinkage and Selectionator** Operator  
  
Lassoå›å½’æ¨¡å‹æ˜¯å¸¸ç”¨çº¿æ€§å›å½’çš„æ¨¡å‹ï¼Œå½“æ¨¡å‹ç»´åº¦è¾ƒé«˜æ—¶ï¼ŒLassoç®—æ³•é€šè¿‡æ±‚è§£**ç¨€ç–è§£**å¯¹æ¨¡å‹è¿›è¡Œ**å˜é‡é€‰æ‹©**ã€‚Larsç®—æ³•åˆ™æä¾›äº†ä¸€ç§å¿«é€Ÿæ±‚è§£è¯¥æ¨¡å‹çš„æ–¹æ³•ã€‚  

like ridge reg, it adds a penalty to the size of parameters, but in diff forms  
$$\sum_{i=1}^{n}(\hat{y_i}-y_i)^2 + \lambda\times |coef|, where \lambda \in [0,+\inf) $$  
$\lambda$ is again determined using cross-validation  
  
more variables?  
$\lambda \times (|coef_1|+|coef_2|+...+|coef_p|)$  
  

## summing up about Lasso and Rdige  
Lassoå›å½’å’ŒRidgeå›å½’éƒ½æ˜¯**Elastic Netå¹¿ä¹‰çº¿æ€§æ¨¡å‹çš„ç‰¹ä¾‹**ã€‚  
å‚æ•°$\alpha$æ§åˆ¶å¯¹é«˜ç›¸å…³æ€§æ•°æ®æ—¶å»ºæ¨¡çš„å½¢çŠ¶ã€‚
* Lassoå›å½’ï¼Œ$\alpha$=1(Rè¯­è¨€glmnetçš„é»˜è®¤å€¼),  
* Rigdeå›å½’ï¼Œ$\alpha$=0ï¼Œ  
* ä¸€èˆ¬çš„elastic net 0<$\alpha$<1.    
  
Lassoå›å½’å’ŒRidgeå›å½’**å¤æ‚åº¦è°ƒæ•´çš„ç¨‹åº¦ç”±å‚æ•°$\lambda$æ¥æ§åˆ¶**ï¼Œ<font color='#F8D86A'>$\lambda$è¶Šå¤§æ¨¡å‹å¤æ‚åº¦çš„æƒ©ç½šåŠ›åº¦è¶Šå¤§ï¼Œä»è€Œè·å¾—ä¸€ä¸ªè¾ƒå°‘å˜é‡çš„æ¨¡å‹ã€‚</font>  

ä»å‘é‡é€‰æ‹©çš„è§’åº¦ç›´è§‚åœ°ç†è§£Lassoå›å½’å’ŒLars  
  
æœ€åŸºç¡€çš„å‘é‡é€‰æ‹©æ¨¡å‹ï¼šstepwise  
->ridge æƒ©ç½šå’Œlasso æƒ©ç½šçš„ä¸åŒ  
->lassoæ±‚è§£ç³»æ•°çš„æ–¹æ³•ï¼šlars  
  
åŒï¼š
 * äºŒè€…éƒ½æ˜¯æœ‰åä¼°è®¡  
  
å¼‚ï¼š
 * Lassoæ„é€ çš„æ˜¯ä¸€é˜¶æƒ©ç½šå‡½æ•°ï¼ˆç»å¯¹å€¼å½¢å¼ï¼‰ï¼Œä»è€Œå…¶çº¦æŸæ¡ä»¶æ›´ä¸ºå°–é”ï¼Œå›å½’çš„ä¼°è®¡å‚æ•°æ›´æ˜“ä¸º0ï¼Œæ‰€ä»¥å¯ä»¥ä½¿ä¸€äº›å˜é‡ç³»æ•°ä¸º0ï¼Œ**å¾—åˆ°çš„æ¨¡å‹æ›´ç®€æ´**ï¼›è€Œå²­å›å½’ç³»æ•°ä¸º0çš„å¯èƒ½æ€§å¾ˆå°ã€‚  
 * **æ¯”èµ·å²­å›å½’ï¼ŒLasso æ”¶æ•›æ›´å¿«ã€‚**

LARSæ˜¯ä¸€ä¸ªé€‚ç”¨äºé«˜ç»´æ•°æ®çš„å›å½’ç®—æ³•ã€‚  
  
* ä¼˜ç‚¹ï¼š  
 * ç‰¹åˆ«é€‚åˆäºç‰¹å¾ç»´åº¦pè¿œé«˜äºæ ·æœ¬æ•°nçš„æƒ…å†µã€‚  
 * ç®—æ³•çš„æœ€åè®¡ç®—å¤æ‚åº¦å’Œæœ€å°äºŒä¹˜æ³•ç±»ä¼¼ï¼Œä½†æ˜¯å…¶è®¡ç®—é€Ÿåº¦å‡ ä¹å’Œå‰å‘é€‰æ‹©ç®—æ³•ä¸€æ ·  
 * å¯ä»¥äº§ç”Ÿåˆ†æ®µçº¿æ€§ç»“æœçš„å®Œæ•´è·¯å¾„ï¼Œè¿™åœ¨æ¨¡å‹çš„äº¤å‰éªŒè¯ä¸­æä¸ºæœ‰ç”¨  

* ç¼ºç‚¹ï¼š  
 * ç”±äºLARSçš„è¿­ä»£æ–¹å‘æ˜¯æ ¹æ®ç›®æ ‡çš„æ®‹å·®è€Œå®šï¼Œæ‰€ä»¥è¯¥ç®—æ³•å¯¹æ ·æœ¬çš„å™ªå£°æä¸ºæ•æ„Ÿã€‚


```R
library(lars)
data(diabetes)

length(diabetes$y)
dim(diabetes$x)
dim(diabetes$x2)

colnames(diabetes$x)
colnames(diabetes$x2)
```

    Warning message:
    "package 'lars' was built under R version 3.5.2"Loaded lars 1.2
    
    


442



<ol class=list-inline>
	<li>442</li>
	<li>10</li>
</ol>




<ol class=list-inline>
	<li>442</li>
	<li>64</li>
</ol>




<ol class=list-inline>
	<li>'age'</li>
	<li>'sex'</li>
	<li>'bmi'</li>
	<li>'map'</li>
	<li>'tc'</li>
	<li>'ldl'</li>
	<li>'hdl'</li>
	<li>'tch'</li>
	<li>'ltg'</li>
	<li>'glu'</li>
</ol>




<ol class=list-inline>
	<li>'age'</li>
	<li>'sex'</li>
	<li>'bmi'</li>
	<li>'map'</li>
	<li>'tc'</li>
	<li>'ldl'</li>
	<li>'hdl'</li>
	<li>'tch'</li>
	<li>'ltg'</li>
	<li>'glu'</li>
	<li>'age^2'</li>
	<li>'bmi^2'</li>
	<li>'map^2'</li>
	<li>'tc^2'</li>
	<li>'ldl^2'</li>
	<li>'hdl^2'</li>
	<li>'tch^2'</li>
	<li>'ltg^2'</li>
	<li>'glu^2'</li>
	<li>'age:sex'</li>
	<li>'age:bmi'</li>
	<li>'age:map'</li>
	<li>'age:tc'</li>
	<li>'age:ldl'</li>
	<li>'age:hdl'</li>
	<li>'age:tch'</li>
	<li>'age:ltg'</li>
	<li>'age:glu'</li>
	<li>'sex:bmi'</li>
	<li>'sex:map'</li>
	<li>'sex:tc'</li>
	<li>'sex:ldl'</li>
	<li>'sex:hdl'</li>
	<li>'sex:tch'</li>
	<li>'sex:ltg'</li>
	<li>'sex:glu'</li>
	<li>'bmi:map'</li>
	<li>'bmi:tc'</li>
	<li>'bmi:ldl'</li>
	<li>'bmi:hdl'</li>
	<li>'bmi:tch'</li>
	<li>'bmi:ltg'</li>
	<li>'bmi:glu'</li>
	<li>'map:tc'</li>
	<li>'map:ldl'</li>
	<li>'map:hdl'</li>
	<li>'map:tch'</li>
	<li>'map:ltg'</li>
	<li>'map:glu'</li>
	<li>'tc:ldl'</li>
	<li>'tc:hdl'</li>
	<li>'tc:tch'</li>
	<li>'tc:ltg'</li>
	<li>'tc:glu'</li>
	<li>'ldl:hdl'</li>
	<li>'ldl:tch'</li>
	<li>'ldl:ltg'</li>
	<li>'ldl:glu'</li>
	<li>'hdl:tch'</li>
	<li>'hdl:ltg'</li>
	<li>'hdl:glu'</li>
	<li>'tch:ltg'</li>
	<li>'tch:glu'</li>
	<li>'ltg:glu'</li>
</ol>



lars::lars()
>type - è¡¨ç¤ºæ‰€ä½¿ç”¨çš„å›å½’æ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼ˆlasso, lar, forward.stagewise, stepwiseï¼‰ï¼Œé€‰æ‹©ä¸åŒçš„å›å½’æ–¹æ³•å°†å¾—åˆ°ä¸åŒçš„è§£è·¯å¾„ï¼›    
normalize - è¡¨ç¤ºæ˜¯å¦å¯¹å˜é‡è¿›è¡Œå½’ä¸€åŒ–ï¼Œå½“ä¸ºTRUEæ—¶ï¼Œç¨‹åºå°†å¯¹xå’Œyè¿›è¡ŒL2æ­£åˆ™åŒ–ï¼›  
intercept - è¡¨ç¤ºæ˜¯å¦å¯¹å˜é‡è¿›è¡Œä¸­å¿ƒåŒ–ï¼Œå½“ä¸ºTRUEæ—¶ï¼Œç¨‹åºå°†å¯¹xå’Œyåˆ†åˆ«å‡å»å…¶å‡å€¼ã€‚


```R
attach(diabetes)

mlasso = lars(x,y,type="lasso")
mlars = lars(x,y,type="lar")
mforw = lars(x,y,type="forward.stagewise")
mstep = lars(x,y,type="stepwise")
```


```R
mlasso
```


    
    Call:
    lars(x = x, y = y, type = "lasso")
    R-squared: 0.518 
    Sequence of LASSO moves:
         bmi ltg map hdl sex glu tc tch ldl age hdl hdl
    Var    3   9   4   7   2  10  5   8   6   1  -7   7
    Step   1   2   3   4   5   6  7   8   9  10  11  12


å¯ä»¥æŸ¥çœ‹$R^2$  
"Sequence of LASSO moves" æ˜¯åœ¨è¿›è¡ŒLassoå›å½’æ—¶è‡ªå˜é‡è¢«é€‰å…¥çš„é¡ºåº


```R
opt(6,5)
opar=par(mfrow=c(2,2))
plot(mlasso)
plot(mlars)
plot(mforw)
plot(mstep)
```


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_25_0.png)


ç«–çº¿å¯¹åº”Lassoä¸­è¿­ä»£çš„æ¬¡æ•°ï¼ˆå¯¹åº”ä¸Šé¢æŸ¥çœ‹mlassoæ—¶çš„stepï¼‰ï¼Œå¯¹åº”çš„ç³»æ•°å€¼ä¸ä¸º0çš„å˜é‡å³ä¸ºè¢«é€‰å…¥çš„ã€‚

## é€‰å®šæ¨¡å‹


```R
summary(mlasso)
```


<table>
<thead><tr><th></th><th scope=col>Df</th><th scope=col>Rss</th><th scope=col>Cp</th></tr></thead>
<tbody>
	<tr><th scope=row>0</th><td> 1        </td><td>2621009   </td><td>453.726255</td></tr>
	<tr><th scope=row>1</th><td> 2        </td><td>2510465   </td><td>418.032217</td></tr>
	<tr><th scope=row>2</th><td> 3        </td><td>1700369   </td><td>143.801193</td></tr>
	<tr><th scope=row>3</th><td> 4        </td><td>1527165   </td><td> 86.741078</td></tr>
	<tr><th scope=row>4</th><td> 5        </td><td>1365734   </td><td> 33.695679</td></tr>
	<tr><th scope=row>5</th><td> 6        </td><td>1324118   </td><td> 21.505224</td></tr>
	<tr><th scope=row>6</th><td> 7        </td><td>1308932   </td><td> 18.327003</td></tr>
	<tr><th scope=row>7</th><td> 8        </td><td>1275355   </td><td>  8.877493</td></tr>
	<tr><th scope=row>8</th><td> 9        </td><td>1270233   </td><td>  9.131148</td></tr>
	<tr><th scope=row>9</th><td>10        </td><td>1269390   </td><td> 10.843547</td></tr>
	<tr><th scope=row>10</th><td>11        </td><td>1264977   </td><td> 11.338975</td></tr>
	<tr><th scope=row>11</th><td>10        </td><td>1264765   </td><td>  9.266761</td></tr>
	<tr><th scope=row>12</th><td>11        </td><td>1263983   </td><td> 11.000000</td></tr>
</tbody>
</table>



æœ€å‰é¢ä¸€åˆ—å¯¹åº”è¿­ä»£æ¬¡æ•°(step)ï¼ŒDf=3å¯¹åº”è¿­ä»£æ¬¡æ•°=2  
ä¸Šè¡¨æ˜¾ç¤ºäº†Lassoå›å½’ä¸­æ‰€æœ‰çš„cpå€¼ï¼ˆ**MallowsCpç»Ÿè®¡é‡ï¼Œæ˜¯å¯¹å…±çº¿æ€§çš„åˆ¤æ–­**ï¼‰ï¼Œé€‰å–Cpæœ€å°çš„  
æ­¤ä¾‹ä¸­å¯¹åº”æœ€å°Cpå€¼çš„step=7ï¼ˆor 8?ï¼‰


```R
mlasso$Cp
```


<dl class=dl-horizontal>
	<dt>0</dt>
		<dd>453.726255006272</dd>
	<dt>1</dt>
		<dd>418.032217290298</dd>
	<dt>2</dt>
		<dd>143.801193388963</dd>
	<dt>3</dt>
		<dd>86.7410780632947</dd>
	<dt>4</dt>
		<dd>33.6956791165279</dd>
	<dt>5</dt>
		<dd>21.5052237966864</dd>
	<dt>6</dt>
		<dd>18.3270029796993</dd>
	<dt>7</dt>
		<dd>8.87749261791703</dd>
	<dt>8</dt>
		<dd>9.13114827526573</dd>
	<dt>9</dt>
		<dd>10.8435467724511</dd>
	<dt>10</dt>
		<dd>11.3389749748865</dd>
	<dt>11</dt>
		<dd>9.26676056353836</dd>
	<dt>12</dt>
		<dd>11</dd>
</dl>




```R
mlasso$Cp[which.min(mlasso$Cp)]
```


<strong>7:</strong> 8.87749261791703


## è·å–ç³»æ•°


```R
mlasso$beta[which.min(mlasso$Cp),]
```


<dl class=dl-horizontal>
	<dt>age</dt>
		<dd>0</dd>
	<dt>sex</dt>
		<dd>-197.756501135154</dd>
	<dt>bmi</dt>
		<dd>522.26484701812</dd>
	<dt>map</dt>
		<dd>297.15973688909</dd>
	<dt>tc</dt>
		<dd>-103.946248766907</dd>
	<dt>ldl</dt>
		<dd>0</dd>
	<dt>hdl</dt>
		<dd>-223.926033335305</dd>
	<dt>tch</dt>
		<dd>0</dd>
	<dt>ltg</dt>
		<dd>514.749480847553</dd>
	<dt>glu</dt>
		<dd>54.7676806302465</dd>
</dl>




```R
# è·å–æˆªè·
predict(mlasso,data.frame(age=0,sex=0,bmi=0,map=0,tc=0,ld1=0,hd1=0,tch=0,ltg=0,glu=0),s=8)
# è¿™é‡Œçš„sæ˜¯ä¹‹å‰çš„Df,è¿­ä»£æ¬¡æ•°step+1ï¼Ÿ
# data.frameä¸­è‡ªå˜é‡çš„æ•°é‡å’Œæ•°æ®æ¡†ä¸­è¿›è¡Œlassoæ‹Ÿåˆçš„è‡ªå˜é‡æ•°ç›®ç›¸åŒï¼Œéƒ½è¦å†™ä¸Šã€‚
# è¾“å‡ºç»“æœä¸­çš„fitå³ä¸ºæ‰€æ±‚
```


<dl>
	<dt>$s</dt>
		<dd>8</dd>
	<dt>$fraction</dt>
		<dd>0.583333333333333</dd>
	<dt>$mode</dt>
		<dd>'step'</dd>
	<dt>$fit</dt>
		<dd>152.133484162896</dd>
</dl>



## äº¤å‰éªŒè¯  
cross validation <-> cv <-> cv.lars()


```R
cv1=cv.lars(x,y,type="lasso",mode="step")
cv1
```


<dl>
	<dt>$index</dt>
		<dd><ol class=list-inline>
	<li>1</li>
	<li>2</li>
	<li>3</li>
	<li>4</li>
	<li>5</li>
	<li>6</li>
	<li>7</li>
	<li>8</li>
	<li>9</li>
	<li>10</li>
	<li>11</li>
	<li>12</li>
	<li>13</li>
</ol>
</dd>
	<dt>$cv</dt>
		<dd><ol class=list-inline>
	<li>5954.14703966155</li>
	<li>5719.75327158815</li>
	<li>3936.26036159499</li>
	<li>3542.21475453098</li>
	<li>3174.73439926476</li>
	<li>3093.62293542858</li>
	<li>3052.86108022496</li>
	<li>3000.99425581899</li>
	<li>2991.04092539401</li>
	<li>2986.87193371072</li>
	<li>2996.37212226779</li>
	<li>3002.67997104454</li>
	<li>2994.06531553897</li>
</ol>
</dd>
	<dt>$cv.error</dt>
		<dd><ol class=list-inline>
	<li>382.751667681927</li>
	<li>378.33153118742</li>
	<li>251.721868718799</li>
	<li>237.04148107866</li>
	<li>198.178667993397</li>
	<li>197.207727753711</li>
	<li>189.444291666093</li>
	<li>185.876507776068</li>
	<li>184.786117379773</li>
	<li>178.732543497282</li>
	<li>181.337920241993</li>
	<li>178.638475876547</li>
	<li>179.153194912029</li>
</ol>
</dd>
	<dt>$mode</dt>
		<dd>'step'</dd>
</dl>




![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_36_1.png)



```R
cv1$index[which.min(cv1$cv)]
min(cv1$cv)
```


10



2986.87193371072



```R
# step=10å¯¹åº”çš„å›å½’ç³»æ•°
mlasso$beta[10,]
```


<dl class=dl-horizontal>
	<dt>age</dt>
		<dd>0</dd>
	<dt>sex</dt>
		<dd>-227.175798242917</dd>
	<dt>bmi</dt>
		<dd>526.390594350024</dd>
	<dt>map</dt>
		<dd>314.950467216784</dd>
	<dt>tc</dt>
		<dd>-237.34097311995</dd>
	<dt>ldl</dt>
		<dd>33.628274416008</dd>
	<dt>hdl</dt>
		<dd>-134.599352051782</dd>
	<dt>tch</dt>
		<dd>111.384128693504</dd>
	<dt>ltg</dt>
		<dd>545.48259721323</dd>
	<dt>glu</dt>
		<dd>64.606670131436</dd>
</dl>



# èšç±»

## the general idea  
1. ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ï¼Ÿ
ç›‘ç£å­¦ä¹ é€šå¸¸ç”¨æ¥è¿›è¡Œåˆ¤åˆ«åˆ†æã€‚  
æ— ç›‘ç£å­¦ä¹ ï¼Œæ˜¯äº‹å…ˆä¸çŸ¥é“æ ·æœ¬å±äºå“ªä¸€ç±»ã€‚  
ä½†äº‹å…ˆçŸ¥é“æ ·æœ¬çš„åˆ†ç±»ä¹Ÿå¯ä»¥è¿›è¡Œèšç±»ï¼Œå¯ä»¥ç”¨æ¥åˆ¤æ–­åˆ†ç±»æ˜¯å¦æ­£ç¡®ã€‚  
èšç±»æ˜¯ä¸€ç§<b>æ— ç›‘ç£å­¦ä¹ æ–¹æ³•</b>ã€‚  
<br />  
  
2. ä»€ä¹ˆæ˜¯èšç±»ï¼Ÿ  
èšç±»ï¼Œå°±æ˜¯å°†æ•°æ®æŒ‰ä¸€å®šçš„è§„åˆ™ã€æ–¹æ³•åˆ†æˆå‡ ç±»çš„æ–¹æ³•ã€‚  
èšç±»çš„æœ€ç»ˆç›®æ ‡ï¼Œå°±æ˜¯ä½¿<b>åŒä¸€ç±»ä¹‹é—´çš„æ–¹å·®å°½é‡å°ï¼Œä½¿ä¸åŒç±»ä¹‹é—´çš„æ–¹å·®å°½é‡å¤§</b>ã€‚  
èšç±»é€šå¸¸çš„æ­¥éª¤æ˜¯ï¼š
>1ã€Patten representation/å‡†å¤‡å·¥ä½œï¼šäº‹å…ˆæƒ³ä¸€æƒ³å‡†å¤‡åˆ†æˆå‡ ç±»ï¼Œé€‰ç”¨å“ªäº›å˜é‡ï¼Œæƒ³å¯¹å¤šå°‘æ ·æœ¬è¿›è¡Œèšç±»ï¼Œæ•°æ®å¦‚ä½•è¿›è¡Œåˆæ­¥å¤„ç†ï¼ˆå¦‚æ ‡å‡†åŒ–ï¼Œæ­£æ€åŒ–ï¼ŒåŒå‘åŒ–ç­‰ï¼‰ã€‚å¾ˆå¤šè¿™äº›å‰æœŸå·¥ä½œå¯èƒ½æ— æ³•å®ç°ï¼Œæœ‰å¾ˆå¤šå¯èƒ½ä¹Ÿæ²¡æœ‰å¿…è¦ã€‚  
2ã€Patten proximity measure/è·ç¦»åº¦é‡æ–¹æ³•çš„ç¡®å®šï¼šç‚¹å’Œç‚¹ä¹‹é—´ã€ç±»å’Œç±»ä¹‹é—´çš„è·ç¦»åº”å¦‚ä½•åº¦é‡ï¼Ÿç”¨æ¬§å¼è·ç¦»ï¼Ÿé©¬æ°è·ç¦»ï¼Ÿå…°æ°è·ç¦»ï¼Ÿè¿˜æ˜¯è‡ªå·±åˆ›é€ ä¸€ç§æ›´ç¬¦åˆå®é™…æƒ…å†µçš„è·ç¦»ï¼Ÿ  
é™¤äº†è·ç¦»åº¦é‡æ–¹æ³•çš„ç¡®å®šï¼Œè¿˜æœ‰ç±»å’Œç±»ä¹‹é—´çš„è¿æ¥æ–¹æ³•çš„ç¡®å®šï¼Œæ˜¯åº”è¯¥ç”¨æœ€çŸ­è·ç¦»æ³•ï¼Œè¿˜æ˜¯ç”¨æœ€é•¿è·ç¦»æ³•ï¼Ÿä¸åŒçš„æ–¹æ³•å¯èƒ½å¸¦æ¥ä¸åŒçš„åˆ†ç±»ç»“æœã€‚  
3ã€Grouping/èšç±»ï¼šç¡®å®šäº†ä¸Šè¿°ä¸‰æ­¥ä¹‹åï¼Œå°±å¯ä»¥è¿›è¡Œèšç±»äº†ã€‚ä½†èšç±»è¿‡ç¨‹ä¹Ÿå¹¶éå®¹æ˜“ï¼Œå½“ä¸€ä¸ªç±»ä¸å¦å¤–ä¸¤ä¸ªç±»çš„è·ç¦»ç›¸ç­‰ï¼Œè¯¥å¦‚ä½•ç»§ç»­å‘¢ï¼Ÿ  
4ã€Data abstraction/æŠ½è±¡å‡ºå„ç±»çš„å«ä¹‰ï¼šè¿™ä¸€æ­¥å¹¶éå¿…è¦ã€‚å¯ä»¥æœ‰ä¹Ÿå¯ä»¥æ²¡æœ‰ã€‚ç»™æ¯ä¸ªç±»å®šä¹‰ä¸€ä¸ªå«ä¹‰ï¼Œè¿™æ ·å¯ä»¥æ›´ç›´è§‚ï¼Œæ›´æ–¹ä¾¿ã€‚  
5ã€Cluster assessment/ç±»è¯„ä¼°ï¼šè¯„ä»·ä¸€ä¸‹åˆ†ç±»æ˜¯å¦æœ‰æ„ä¹‰ï¼Œæ˜¯å¦å‡†ç¡®ã€‚  


 æ— è®ºæ˜¯ä»€ä¹ˆèšç±»æ–¹æ³•ï¼ŒåŸºæœ¬çš„æ­¥éª¤éƒ½å¦‚ä¸Šæ‰€ç¤ºã€‚åŒºåˆ«ä¸»è¦åœ¨äºï¼Œä¸åŒçš„èšç±»æ–¹æ³•åœ¨è®¡ç®—ç±»é—´è·ç¦»å’Œè¿æ¥æ–¹å¼æ—¶å€™æœ‰æ‰€åŒºåˆ«ã€‚
<br />  

3. ç›®å‰å¹¿æ³›çš„èšç±»æ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼š<b>ä¸€ç§æ˜¯ç³»ç»Ÿèšç±»ï¼Œä¸€ç§æ˜¯Kå‡å€¼èšç±»</b>ã€‚  
<br />  
<b>å±‚æ¬¡èšç±»/ç³»ç»Ÿèšç±»</b>ã€€  
 * ç³»ç»Ÿèšç±»æ— éœ€äº‹å…ˆç¡®å®šéœ€è¦åˆ†å‡ ç±»ã€‚  
 * ç³»ç»Ÿèšç±»ä¸€èˆ¬ç”¨æ ‘çŠ¶å›¾(dendrogram)è¡¨ç¤ºã€‚  

>å±‚æ¬¡èšç±»æ–¹æ³•å…·ä½“åˆå¯åˆ†ä¸ºå‡èšçš„ï¼Œåˆ†è£‚çš„ä¸¤ç§æ–¹æ¡ˆã€‚ ã€€ã€€  
1. ç”±nä¸ªç±»èšæˆ1ç±»â€”â€”**å‡èšçš„å±‚æ¬¡èšç±»**æ˜¯ä¸€ç§<b>è‡ªåº•å‘ä¸Š</b>çš„ç­–ç•¥ï¼ˆagglomerative approachï¼‰ï¼Œé¦–å…ˆå°†æ¯ä¸ªå¯¹è±¡ä½œä¸ºä¸€ä¸ªç°‡ï¼Œç„¶ååˆå¹¶è¿™äº›åŸå­ç°‡ä¸ºè¶Šæ¥è¶Šå¤§çš„ç°‡ï¼Œç›´åˆ°æ‰€æœ‰çš„å¯¹è±¡éƒ½åœ¨ä¸€ä¸ªç°‡ä¸­ï¼Œæˆ–è€…æŸä¸ªç»ˆç»“æ¡ä»¶è¢«æ»¡è¶³ï¼Œç»å¤§å¤šæ•°å±‚æ¬¡èšç±»æ–¹æ³•å±äºè¿™ä¸€ç±»ï¼Œå®ƒä»¬åªæ˜¯åœ¨ç°‡é—´ç›¸ä¼¼åº¦çš„å®šä¹‰ä¸Šæœ‰æ‰€ä¸åŒã€‚   
2. ç”±1ä¸ªç±»åˆ†æˆnç±»â€”â€”**åˆ†è£‚çš„å±‚æ¬¡èšç±»**ä¸å‡èšçš„å±‚æ¬¡èšç±»ç›¸åï¼Œé‡‡ç”¨<b>è‡ªé¡¶å‘ä¸‹</b>çš„ç­–ç•¥ï¼ˆdivisive approachï¼‰ï¼Œå®ƒé¦–å…ˆå°†æ‰€æœ‰å¯¹è±¡ç½®äºåŒä¸€ä¸ªç°‡ä¸­ï¼Œç„¶åé€æ¸ç»†åˆ†ä¸ºè¶Šæ¥è¶Šå°çš„ç°‡ï¼Œç›´åˆ°æ¯ä¸ªå¯¹è±¡è‡ªæˆä¸€ç°‡ï¼Œæˆ–è€…è¾¾åˆ°äº†æŸä¸ªç»ˆæ­¢æ¡ä»¶ã€‚  
æˆ‘ä»¬å¹¶ä¸é‡ç‚¹ä»‹ç»åˆ†ç¦»æ–¹æ³•ï¼Œå› ä¸ºå®ƒä»æœ¬è´¨ä¸Šå’Œç³»ç»Ÿèšç±»æ–¹æ³•æ˜¯ä¸€æ ·çš„ã€‚   
ä½†æ˜¯å®ƒä¹Ÿæœ‰**ä¼˜ç‚¹**ï¼Œå½“æ•°æ®å®é™…æƒ…å†µåªæ˜¯åˆ†æˆå¾ˆå°‘çš„å‡ ç±»æ—¶ï¼Œç”¨åˆ†ç¦»æ³•èƒ½å¾ˆå¿«åˆ†å‡ºæ¥ï¼Œç”¨ç³»ç»Ÿèšç±»å°±éœ€è¦åˆ†å¾ˆä¹…ã€‚   

<br />  
**K-meansèšç±»**  
* éœ€è¦äº‹å…ˆç¡®å®šç±»æ•°  
* æ›´é€‚ç”¨äºå¤§æ ·æœ¬é›†  

## Distances
>Minkowski Distance é—µæ°è·ç¦»é€šå¼  
$d_{ij}(q)=(\sum_{k=1}^{p}|x_{ik}-x_{jk}|^q)^{1/q}$  

<br />
![è·ç¦»å›¾](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/clustering_pic01.png)  
åˆ°ä¸­å¿ƒç‚¹è·ç¦»ç›¸åŒçš„ç‚¹ç”¨åŒä¸€ç§é¢œè‰²è¡¨ç¤º  
<br />  

1. Manhattan distance / Block distance   
$d_{ij}(1)=\sum_{k=1}^{p}|x_{ik}-x_{jk}|$  
$d_{AB}=|x_A-x_B|+|y_A-y_B|$
2. Euclidean distance  
$d_{ij}(2)=(\sum_{k=1}^{p}|x_{ik}-x_{jk}|^2)^{1/2}$  
$d_{AB}=\sqrt{(x_A-x_B)^2+(y_A-y_B)^2}$  
 * <font color='#F8D86A'>the most frequently used  
 * ä¼˜ç‚¹ï¼šæœ‰éå¸¸æ˜ç¡®çš„ç©ºé—´è·ç¦»æ¦‚å¿µ
 * ç¼ºç‚¹  
    æ²¡æœ‰åŒºåˆ†é‡çº²çš„ä¸åŒï¼Œå°†æ‰€æœ‰å˜é‡è§†ä¸ºå½±å“ç›¸åŒçš„ â€”â€” é€‚ç”¨äºå·²åšæ ‡å‡†åŒ–å¤„ç†çš„æ•°æ®ï¼ˆpreprocessed dataï¼‰  
    å—ç»´åº¦çš„å½±å“ï¼Ÿ</font>  

3. Chebyshev distance  
$d_{ij}(\inf)=max|x_{ik}-x_{jk}|$  

4. Mahalanobis distance é©¬æ°è·ç¦»  
$d_{ij}(M)=((X_i-X_j)' A^{-1} (X_i-X_j))^{\frac{1}{2}}$  
$ğ´$ represents the variance matrix, $ğ‘‹_ğ‘–$ represents sample ğ‘– with ğ‘ dimensions  
 * <font color='#F8D86A'>é©¬æ°è·ç¦»åˆç§°å¹¿ä¹‰æ¬§æ°è·ç¦»  
 * Taken variablesâ€™ correlation into consideration, if variables are independent, Mahalanobis distance turn into weighted Euclidean distance(å¦‚æœå„å˜é‡ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œå³è§‚æµ‹å˜é‡çš„åæ–¹å·®çŸ©é˜µæ˜¯å¯¹è§’çŸ©é˜µï¼Œåˆ™é©¬æ°è·ç¦»å°±é€€åŒ–ä¸ºç”¨å„ä¸ªè§‚æµ‹æŒ‡æ ‡çš„**æ ‡å‡†å·®çš„å€’æ•°ä½œä¸ºæƒæ•°çš„åŠ æƒæ¬§æ°è·ç¦»**).   
 * ä¸å—é‡çº²å½±å“ã€‚Taken variablesâ€™ variation into consideration,  and dimension problems can be solved.</font>  

<br />  
**è·ç¦»çš„é€‰æ‹©ï¼š**   
å®é™…ä¸­ï¼Œèšç±»åˆ†æå‰ä¸å¦¨è¯•æ¢æ€§åœ°å¤šé€‰æ‹©å‡ ä¸ªè·ç¦»å…¬å¼åˆ†åˆ«è¿›è¡Œèšç±»ï¼Œç„¶åå¯¹èšç±»åˆ†æçš„ç»“æœè¿›è¡Œå¯¹æ¯”åˆ†æï¼Œä»¥ç¡®å®šæœ€åˆé€‚çš„è·ç¦»æµ‹åº¦æ–¹æ³•ã€‚
    


## Linkage  
ri,sj - the ith / jth observation from group r/s  
1. Single Linkage æœ€çŸ­è·ç¦»æ³•  
$d_c(r,s)=\min{d(x_{ri},x_{sj})}, i=1,2,...,n_r, j=1,2,...,n_s$  
 * most oftenly used  
 * disadvantages: **chainingé“¾å¼ç»“æœ**. This comes about when clusters are not well separatedå½“ä¸¤ä¸ªç±»ä¹‹é—´ä¸èƒ½å¾ˆå¥½åœ°åˆ†ç¦», and snake-like chains can form. Observations at opposite ends of the chain can be very dissimilar, but yet they end up in the same cluster. 
 * Another issue with single linkage is that **it does not take the cluster structure into account/æ²¡æœ‰è€ƒè™‘åˆ°è·ç¦»ç»“æ„**.  
2. Complete Linkage æœ€é•¿è·ç¦»æ³•  
$d_c(r,s)=\max{d(x_{ri},x_{sj})}, i=1,2,...,n_r, j=1,2,...,n_s$  
 * Complete linkage is not susceptible to chainingä¸æ˜“è¡¨ç°å‡ºé“¾å¼ç»“æœ.
 * The resulting clusters tend to be spherical, and it **has difficulty recovering nonspherical groups**.ä½†ç»“æœå‘ˆçƒå½¢ã€‚  
 * Like single linkage, complete linkage **does not account for cluster structure**.æ²¡æœ‰è€ƒè™‘ç±»çš„è·ç¦»ç»“æ„ã€‚  
3. Average Linkage å¹³å‡è·ç¦»æ³•   
$d_c(r,s)=\frac{1}{n_r n_s}\sum_{i=1}^{n_r} \sum_{j=1}^{n_s}d(x_{ri},x_{sj}),  i=1,2,...,n_r,  j=1,2,...,n_s$  
 * This method tends to combine clusters that have small variances, and it also tends to **produce clusters with approximately equal variance**.
 * It is relatively robust and **does take the cluster structure into account**.  
4. Centroild Linkageé‡å¿ƒæ³•   
$d_c(r,s)=d(\bar{x_r},\bar{x_s})$, where $\bar{x_r}$ is the avg of obs in the r-th cluster and $\bar{x_s}$ the avg of obs in the s-th cluster.è€ƒè™‘äº†å…¨éƒ¨ç‚¹çš„ä¿¡æ¯ï¼Œä½†å­˜åœ¨ä¿¡æ¯æŸå¤±ã€‚  

## K-means

K-means æ˜¯äº‹å…ˆç¡®å®šç±»æ•°ï¼ˆKï¼‰çš„ï¼Œå³Rä¸­kmeans()å‡½æ•°çš„centerså‚æ•°  
æ¯”å±‚æ¬¡èšç±»æ³•æ›´é€‚ç”¨äºå¤§æ•°æ®é›†ï¼ˆå¤§æ•°æ®é›†è¦ä»nç±»èšåˆ°1ç±»å¤ªéš¾äº†ï¼‰  
<br />  

K-meansèšç±»çš„ç›®æ ‡ï¼š  
To partition the data into k groups such that the within-group sum-of-squares is minimized.  
Distortion Function $J(c,\mu)=\sum_{i=1}^n ||x^{(i)}-\mu_{c^{(i)}}||^2$ measures the sum of squared distances between each training example $x^{(i)}$ and the cluster centroid $\mu_{c^{(i)}}$ to which it has been assigned.  
<br />  

**K-meansèšç±»çš„æ­¥éª¤**  
>1.Specify the number of clusters k.  
2. Determine initial cluster centroids (randomly).   
3. Calculate the distance between each observation and each cluster centroid.  
4. Assign every observation to the closest cluster.  
5. Calculate the centroid of every cluster using the observations that were just grouped there.  
6. Repeat steps 3 through 5 until no more changes in result.  

<br />  
K-means ç®—æ³•ä¸€å®šä¼šæ”¶æ•›è‡³å±€éƒ¨æœ€ä¼˜ï¼Œä½†ä¸ä¸€å®šæ”¶æ•›è‡³å…¨å±€æœ€ä¼˜ã€‚  
>J must monotonically decrease, and the value of J must converge. However, the distortion function J is a non-convex function, and so coordinate descent on J is not guaranteed to converge to the global minimum. In other words, k-means can be susceptible to local optima.  
To avoid this, **use different random initial values for the cluster centroids $\mu^j$**. Then, out of all the different clusterings found, **pick the one that gives the lowest distortion $J(c,\mu)$**.


use **heatmap** to get a basic idea of what kmeans does


```R
data(iris)
dmat=as.matrix(iris[sample(1:100),-5])
km=kmeans(dmat, centers=3)
opt(4,4)
image(t(dmat),yaxt="n")
image(t(dmat[order(km$cluster),]),yaxt="n")
```


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_46_0.png)



![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_46_1.png)



```R
library(ggplot2)
library(ggthemes)
library(cluster)
data=read.csv("DRfull.csv",header=TRUE)
head(data)
```


<table>
<thead><tr><th scope=col>Country.or.Area</th><th scope=col>D1</th><th scope=col>D12</th><th scope=col>D13</th><th scope=col>D15</th><th scope=col>D16</th><th scope=col>D17</th><th scope=col>D18</th><th scope=col>D2</th><th scope=col>D5</th><th scope=col>...</th><th scope=col>R14F</th><th scope=col>R14M</th><th scope=col>R14B</th><th scope=col>R6F</th><th scope=col>R6M</th><th scope=col>R6B</th><th scope=col>R8R</th><th scope=col>R8U</th><th scope=col>R9R</th><th scope=col>R9U</th></tr></thead>
<tbody>
	<tr><td>Afghanistan        </td><td>-2.4               </td><td>29825              </td><td>24                 </td><td>16.20              </td><td> 3.82              </td><td>47.42              </td><td>5.14               </td><td> 60                </td><td>35.3               </td><td>...                </td><td> 9.5               </td><td> 8.9               </td><td> 9.2               </td><td> 3.200             </td><td>13.100             </td><td> 9.800             </td><td>56                 </td><td>90                 </td><td>23                 </td><td>47                 </td></tr>
	<tr><td>Albania            </td><td>-0.3               </td><td> 3162              </td><td>55                 </td><td>32.56              </td><td>14.93              </td><td>21.33              </td><td>1.76               </td><td>111                </td><td>12.8               </td><td>...                </td><td> 9.0               </td><td>10.3               </td><td> 9.6               </td><td> 6.700             </td><td>17.600             </td><td>11.800             </td><td>94                 </td><td>97                 </td><td>86                 </td><td>95                 </td></tr>
	<tr><td>Algeria            </td><td>-1.9               </td><td>38482              </td><td>74                 </td><td>26.62              </td><td> 7.17              </td><td>27.42              </td><td>2.82               </td><td> 98                </td><td>24.6               </td><td>...                </td><td> 9.3               </td><td> 9.0               </td><td> 9.2               </td><td> 5.700             </td><td>25.500             </td><td>13.800             </td><td>79                 </td><td>85                 </td><td>88                 </td><td>98                 </td></tr>
	<tr><td>Angola             </td><td>-3.1               </td><td>20821              </td><td>60                 </td><td>16.18              </td><td> 3.84              </td><td>47.58              </td><td>5.98               </td><td> 47                </td><td>44.8               </td><td>...                </td><td> 8.7               </td><td> 8.2               </td><td> 8.5               </td><td>26.003             </td><td>31.837             </td><td>29.098             </td><td>34                 </td><td>68                 </td><td>20                 </td><td>87                 </td></tr>
	<tr><td>Antigua and Barbuda</td><td>-1.1               </td><td>   89              </td><td>30                 </td><td>30.26              </td><td>12.35              </td><td>25.96              </td><td>2.10               </td><td>143                </td><td>16.6               </td><td>...                </td><td>12.0               </td><td>11.3               </td><td>11.7               </td><td>15.900             </td><td>24.300             </td><td>20.100             </td><td>98                 </td><td>98                 </td><td>91                 </td><td>91                 </td></tr>
	<tr><td>Argentina          </td><td>-0.9               </td><td>41087              </td><td>93                 </td><td>30.83              </td><td>14.97              </td><td>24.42              </td><td>2.19               </td><td>152                </td><td>16.9               </td><td>...                </td><td>10.3               </td><td>11.0               </td><td>10.6               </td><td>29.700             </td><td>26.100             </td><td>28.000             </td><td>95                 </td><td>99                 </td><td>99                 </td><td>97                 </td></tr>
</tbody>
</table>




```R
df=data.frame(k=1:8,sw=NA)
pamlist=paste0("pam",df$k)
for (k in 1:8){
  pamk=pam(data[,-1],k)
  df[k,]$sw=pamk$silinfo$avg.width
}
```


```R
opt(5,2.5)
# plot(df$k,df$sw)
ggplot(df)+
  theme_bw()+
  geom_point(aes(k,sw),pch=19,size=2,col="#525288")+
  geom_line(aes(k,sw),col="#525288")+
  scale_x_continuous(breaks=1:8)
```




![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_49_1.png)



```R
opt(10,10)
plot(pamk,col=pamk$cluster)
```

    Warning message in if (color) {:
    "the condition has length > 1 and only the first element will be used"Warning message in if (color) c(2, 4, 6, 3) else 5:
    "the condition has length > 1 and only the first element will be used"


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_50_1.png)



![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_50_2.png)


B: number of monte carlo bootstrap samples


```R
gskmn=clusGap(data[,-1],FUN=kmeans,K.max=8,B=500)
gskmn
```


    Clustering Gap statistic ["clusGap"] from call:
    clusGap(x = data[, -1], FUNcluster = kmeans, K.max = 8, B = 500)
    B=500 simulated reference sets, k = 1..8; spaceH0="scaledPCA"
     --> Number of clusters (method 'firstSEmax', SE.factor=1): 1
             logW   E.logW      gap     SE.sim
    [1,] 14.94491 16.79945 1.854544 0.03294085
    [2,] 14.44529 16.11440 1.669101 0.03483898
    [3,] 14.09952 15.71344 1.613925 0.03457841
    [4,] 13.91641 15.43862 1.522205 0.03382952
    [5,] 13.65569 15.23681 1.581113 0.03518960
    [6,] 13.55290 15.08178 1.528885 0.04347128
    [7,] 13.37492 14.95503 1.580110 0.05228916
    [8,] 13.32660 14.84724 1.520637 0.05024694


## EMï¼ˆExpectation Maximizationï¼‰  
å‚è€ƒhttps://blog.csdn.net/zhihua_oba/article/details/73776553  
EMç®—æ³•æœ‰å¾ˆå¤šçš„åº”ç”¨ï¼Œæœ€å¹¿æ³›çš„å°±æ˜¯GMMæ··åˆé«˜æ–¯æ¨¡å‹ã€èšç±»ã€HMMç­‰ç­‰ã€‚æˆ‘ä»¬è®¨è®ºçš„æ˜¯EMåœ¨èšç±»ä¸­çš„åº”ç”¨ã€‚  
<br />
We know that clustering methods can be devided into two types:  
1. hard clustering: clusters do not overlap  
2. soft clustering: clusters may overlap  

**Mixture models**:
probabilistically-grounded way of doing soft clustering  
think of **each cluster** as a generative model that corresponds to a **probability distribution/PD** (Gaussian or multinomial) $\rightarrow$ we want to discover the parameters of the PD, i.e. the mean and covariance of the pdf   
EM allows us to figure out those parameters  
<br />
>so basically, using EM, we will find out the pdf of all k clusters, therefore can calc the prob of all sample points belonging to each clusters, and assign the sample points to the most likely cluster  

![em](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/em_pic01.png)

<font color='#F8D86A'>  
**EMçš„æ­¥éª¤**  
1. åˆå§‹åŒ–åˆ†å¸ƒå‚æ•°$\theta$ï¼ˆå‘é‡ï¼‰  
2. Eæ­¥éª¤ã€èšç±»è¿‡ç¨‹ã€‘ï¼šæ ¹æ®å½“å‰å‚æ•°$\theta$çš„å€¼è®¡ç®—å‡ºæ ·æœ¬$x^{(i)}\in j$çš„åéªŒæ¦‚ç‡ä¼°è®¡å€¼ï¼ˆæœŸæœ›Expectation, hence Eï¼‰  
$$w_j^{(i)}=p(c^{(i)}=j|x^{(i)};\theta)$$  
å…¶ä¸­$c^{(i)}$æ˜¯ç±»åˆ«,$x^{(i)}$æ˜¯æ ·æœ¬  
3. Mæ­¥éª¤ã€æ›´æ–°centroidã€‘ï¼šå°†ä¼¼ç„¶å‡½æ•°(Maximization, hence M)æœ€å¤§åŒ–ä»¥è·å¾—æ–°çš„å‚æ•°å€¼:  
$$\theta=argmax_{\theta}\sum_i \sum_j w_j^{(i)}\log{\frac{p(x^{(i)},c^{(i)};\theta)}{w_j^{(i)}}}$$
é‡å¤Eã€Mæ­¥éª¤ç›´è‡³æ”¶æ•›ã€‚
</font>

EMä¸K-meansçš„å¼‚åŒï¼š  
>å¼‚ï¼šEM is very much like k-meansï¼Œ except that  
 * K-means is a hard clustering method -- in k-means, a point either contribute to this centroid or that, we get an assignment $c(i)$
 * EM is a soft clustering method -- in EM, a point gives a little bit of its mass to every centroid, we get the probability for i to belong to all cluster $w_j^{(i)}$  

>åŒï¼šSimilar to K-means, EM is also susceptible to local optimaï¼Œå¯¹åˆå§‹å€¼æ•æ„Ÿï¼Œå®¹æ˜“å¾—åˆ°å±€éƒ¨æœ€ä¼˜è§£è€Œéå…¨å±€æœ€ä¼˜è§£ã€‚ so reinitializing at several different initial parameters may be a good idea.

## ç³»ç»Ÿèšç±»

<font size=3>**to get a basic idea**</font>   
the heatmap function is very quick to visualize the organization of these high dimensional data


```R
data(iris)
set.seed(143)
dmat=as.matrix(iris[sample(1:50),-5])
opt(5,5)
heatmap(dmat)
```


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_58_0.png)



```R
hc=hclust(dist(dmat))
clus=cutree(hc,4)

opt(4,4)
image(t(dmat),yaxt="n")
opt(4,4)
image(t(dmat[order(clus),]),yaxt="n")
```


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_59_0.png)



![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_59_1.png)



```R
data(iris)
n=nrow(iris)
loc=sample(n, .7*n)
train=iris[loc,]
test=iris[-loc,]
```

<font size=3><b>?dist</b></font>  
><font color="red"> é»˜è®¤æ¬§æ°è·ç¦»</font>  

æ³¨æ„ä¹‹å‰æåˆ°è¿‡ï¼Œæ¬§å¼è·ç¦»å—é‡çº²å½±å“ï¼Œå› æ­¤é€‚ç”¨äºç»è¿‡äº†æ ‡å‡†åŒ–çš„æ•°æ®  

<font size=3><b>?rect.hclust</b></font>  
> Draws rectangles around the branches of a dendrogram highlighting the corresponding clusters.


```R
iris.hcl=hclust(dist(iris[,1:4]))     ###hclust for Hierarchical Clustering å±‚æ¬¡èšç±»
opt(6,4)
plot(iris.hcl,labels=FALSE,hang=-1)
re=rect.hclust(iris.hcl,k=3,border=c("#C04851","#7A7374","#2E317C"))
```


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_63_0.png)



```R
iris.id=cutree(iris.hcl,3)
#æŒ‰ç…§åˆ†æˆ3ç±»ï¼Œå¾—åˆ°æ ·æœ¬çš„åˆ†ç±»ç»“æœ
iris.id
```


<ol class=list-inline>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>1</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>3</li>
	<li>2</li>
	<li>3</li>
	<li>3</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>3</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
	<li>2</li>
</ol>



<font size=4>choose(n,k)</font>  
>è¿”å›$C_n^k$çš„å€¼ï¼Œå³$\frac{n!}{(n-k)!k!}$

## èšç±»è¯„ä»·

ã€éœ€è¦æŒæ¡ã€‘è¯„ä»·èšç±»æ•ˆæœçš„å…³é”®æŒ‡æ ‡ï¼šè½®å»“ç³»æ•°å’Œgap

### SilhouetteæŒ‡æ•°/è½®å»“ç³»æ•°

1990å¹´,Kaufmanå’ŒRousseeuwæå‡ºäº†silhouetteç»Ÿè®¡é‡å¯¹æœ€ä¼˜èšç±»ä»¥åŠæœ€ä¼˜èšç±»æ•°è¿›è¡Œä¼°è®¡ã€‚  
  
<br />  
<font color='#F8D86A'>æ¯”è¾ƒä¸€ä¸ªç‚¹åˆ°æ‰€åœ¨ç±»å„ç‚¹çš„å¹³å‡è·ç¦»å’Œ**æœ€è¿‘çš„**å…¶ä»–ç±»å„ç‚¹çš„å¹³å‡è·ç¦»</font>  
å¯¹è§‚æµ‹å€¼$i$ï¼Œå®šä¹‰$a_i$ä¸º$i$ä¸æ‰€åœ¨ç±»çš„å…¶å®ƒç‚¹ä¹‹é—´çš„å¹³å‡è·ç¦»  
å¯¹äºä»»ä½•ä¸€ä¸ªå…¶ä»–çš„ç±»$c$ï¼Œå®šä¹‰$d(i,c)$ä¸º$i$ä¸$c$ç±»ï¼ˆé™¤è‡ªèº«ï¼‰ä¸­çš„æ‰€æœ‰ç‚¹çš„å¹³å‡è·ç¦»ï¼Œè®©$b_i$ä»£è¡¨$d(i,c)$çš„æœ€å°å€¼  
  
<br />  
  
åˆ™å¯¹äºè§‚æµ‹å€¼$i$ï¼Œå…¶Silhouette widthä¸ºï¼š  
$sw_i = \frac{b_i-a_i}{max(a_i,b_i)}$  ,$sw_i \in (-1,1)$  
* $sw_i$ é è¿‘1ï¼š$a_i$é è¿‘0ï¼Œ$i$è·ç¦»å½“å‰æ‰€åœ¨ç±»è¾ƒè¿‘  
* $sw_i$ é è¿‘0ï¼š$a_i$å’Œ$b_i$å·®ä¸å¤šå¤§  
* $sw_i$ é è¿‘-1ï¼š$b_i$ç›¸å¯¹äº$a_i$å¾ˆå°ï¼Œè¯´æ˜$i$æ²¡æœ‰è¢«è¾ƒå¥½åœ°å½’ç±»  
  
å¯¹æ‰€æœ‰è§‚æµ‹å€¼å–å¹³å‡å¾—åˆ°avg swä¸ºï¼š  
$\bar{sw} = \frac{1}{n}\sum_{i=1}^{n}sw_i$  
ç”¨$\bar{sw}$è¯„ä»·èšç±»æ•ˆæœï¼š  
* <font color='#F8D86A'>$\bar{sw}$å€¼è¶Šå¤§ï¼Œèšç±»æ•ˆæœè¶Šå¥½ï¼Œä»è€Œç¡®å®šç±»æ•°$k$</font>  
According to Kaufman and Rousseeuw, 
 * an average silhouette width greater than 0.5 indicates a reasonable partition of the data
 * a value of less than 0.2 would indicate that the data do not exhibit cluster structure.

>pam: a more robust version of k-means


```R
library(cluster)

data=iris[,1:4]

pam3=pam(data,3)
pam3$silinfo$avg.width
  # ç›¸å½“äºmean(pam3$silinfo$widths[,3])
min(pam3$silinfo$widths[,3])
max(pam3$silinfo$widths[,3])
# silhouette(pam3) # ç›¸å½“äºpam3$silinfo$widths
```


0.55281901235641



0.0263588124292912



0.853905051398459



```R
opt(4,4)
plot(pam3,col=pam3$cluster)#Silhouetteã€€Plot
```

    Warning message in if (color) {:
    "the condition has length > 1 and only the first element will be used"Warning message in if (color) c(2, 4, 6, 3) else 5:
    "the condition has length > 1 and only the first element will be used"


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_72_1.png)



![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_72_2.png)



```R
pam4=pam(data,4)
pam4$silinfo$avg.width #pam3çš„sw_avgæ¯”pam4çš„å¤§ï¼Œè¯´æ˜èšæˆ3ç±»æ•ˆæœæ›´å¥½
opt(4,4)
plot(pam4,col=pam4$cluster)ã€€
```


0.48969717913026


    Warning message in if (color) {:
    "the condition has length > 1 and only the first element will be used"Warning message in if (color) c(2, 4, 6, 3) else 5:
    "the condition has length > 1 and only the first element will be used"


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_73_2.png)



![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_73_3.png)


<font color='#F8D86A'>åˆ†ä¸º3ç±»æ—¶çš„sw_avg(0.55)æ¯”4ç±»(0.49)å¤§ï¼Œè¯´æ˜èšæˆ3ç±»æ•ˆæœæ›´å¥½</font>

### WSS(Within-cluster Sum of Squares)  
$D_r=\sum_{i \in C_r}\sum_{j \in C_r}||x_i-x_j||^2=2n_r \sum_{i \in C_r}||x_i-\bar{x}||^2$  
$W_k=\sum_{r=1}^{k}\frac{1}{2n_r}D_r$  
![WSS](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/clustering_pic02.png)  
>problem of using $W_k$ to determine $k$  
 - no reference clustering to compare?  
 - the differences $W_k-W_{k-1}$ are not normalized for comparison  

### GapæŒ‡æ•°

2001å¹´,Tibshirani et al.æå‡ºäº†Gap Statistic methodå¯¹æœ€ä¼˜èšç±»ä»¥åŠæœ€ä¼˜èšç±»æ•°è¿›è¡Œä¼°è®¡ã€‚  
å»ºç«‹åœ¨ä¸Šè¿°$W_k$ä¹‹ä¸Šï¼šå‡è®¾å­˜åœ¨ä¸€ä¸ªå‚è€ƒåˆ†å¸ƒï¼Œæ¯”è¾ƒ$W_k$å’Œ$W_k$åœ¨å‚è€ƒåˆ†å¸ƒä¸‹çš„æœŸæœ›å€¼çš„å·®å¼‚  
<br />  
**å‚è€ƒåˆ†å¸ƒçš„é€‰å–**  
* ä¸€ç»´æƒ…å†µï¼š$[0,1]$ä¸Šçš„å‡åŒ€åˆ†å¸ƒ  
* å¤šç»´æƒ…å†µï¼š 
  * ç®€å•å‡åŒ€åˆ†å¸ƒ  
  * åˆ©ç”¨å¥‡å¼‚å€¼åˆ†è§£(SVD)æ–¹æ³•å¾—åˆ°å…³äºåŸæ•°æ®çš„ä¸»æˆåˆ†éƒ¨åˆ†ã€‚ä»ä¸€ç»„æœä»å‡åŒ€åˆ†å¸ƒçš„ä¸»æˆåˆ†æ•°æ®ä¸­äº§ç”Ÿå‚è€ƒç‰¹å¾ã€‚  

<br />  
**å‚è€ƒåˆ†å¸ƒä¸‹æœŸæœ›å€¼çš„æ±‚å–**  
äº§ç”Ÿ$B$ä¸ªæœä»å‚è€ƒåˆ†å¸ƒçš„å‚è€ƒæ•°æ®é›†ï¼Œç”¨åœ¨è¿™äº›å‚è€ƒæ•°æ®é›†ä¸‹çš„$\log{(W_k^*)}$æ¥ä¼°è®¡$E_n^*[\log{(W_k^*)}]$  
<br />  
   
$Gap_n(k)=E_n^*(\log{W_k})-\log{W_k}$   
<font color='#F8D86A'>the larger Gap the better (find the k that maximizes Gap(k)</font>  
![gap](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/clustering_pic03.png)  
but according to '1-standard-error' theory, we don't expect to use the k with the largest gap statistics  
but instead, we use the minimum k that satisfies:  
$Gap(k)>Gap(k+1)-s_{k+1}$  


```R
library(cluster)
data(iris)
data=iris[,1:4]

gskmn=clusGap(data,FUN=kmeans,K.max=8,B=500)# clusGap: Gap Statistic æ–¹æ³•
gskmn#æ˜¾ç¤ºæœ€ä¼˜çš„èšç±»æ•°ï½‹
opt(5,5)
plot(gskmn)#ç”»å›¾Gapå€¼
```


    Clustering Gap statistic ["clusGap"] from call:
    clusGap(x = data, FUNcluster = kmeans, K.max = 8, B = 500)
    B=500 simulated reference sets, k = 1..8; spaceH0="scaledPCA"
     --> Number of clusters (method 'firstSEmax', SE.factor=1): 4
             logW   E.logW        gap     SE.sim
    [1,] 4.551642 4.640852 0.08921047 0.02854059
    [2,] 3.808397 4.191117 0.38271951 0.02429483
    [3,] 3.519407 4.012884 0.49347687 0.02641911
    [4,] 3.368124 3.920477 0.55235364 0.02470481
    [5,] 3.301857 3.838111 0.53625411 0.02421139
    [6,] 3.224683 3.763125 0.53844187 0.02420139
    [7,] 3.112055 3.700077 0.58802177 0.02524475
    [8,] 3.127535 3.647548 0.52001255 0.02804035



![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_78_1.png)



```R
logW=gskmn$Tab[,1]
E.logW=gskmn$Tab[,2]
gap=gskmn$Tab[,3]
sum(gap!=E.logW-logW)

dft=data.frame(n=rep(1:length(logW),2),val=c(logW,E.logW),lab=rep(c("logW","E.logW"),each=length(logW)))
opt(7,4)
ggplot(dft)+
  theme_bw()+
  geom_line(aes(n,val,color=lab))+
  geom_point(aes(n,val,color=lab),pch=19)+
  scale_color_manual(name="",values=c("#f2572d","#0067a6"))
```


0





![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_79_2.png)


### å…°å¾·æŒ‡æ•°(Rand Index, RI)

RandæŒ‡æ•°çš„å®è´¨ï¼šè§‚å¯Ÿä¸€å¯¹æ ·æœ¬ç‚¹$x_i$å’Œ$x_j$åœ¨ä¸¤ç§èšç±»æ–¹æ³•ä¸­è¢«ç»„åˆçš„æ–¹å¼æ˜¯å¦ç›¸ä¼¼  
* ç›¸ä¼¼ï¼š
 * åœ¨$G1$å’Œ$G2$ä¸­ï¼Œ$x_i$å’Œ$x_j$å‡åŒç±»  
 * åœ¨$G1$å’Œ$G2$ä¸­ï¼Œ$x_i$å’Œ$x_j$å‡ä¸åŒç±»  
* ä¸åŒï¼š
 * åœ¨$G1$ä¸­ï¼Œ$x_i$å’Œ$x_j$åŒç±»ï¼Œåœ¨$G2$ä¸­ä¸åŒç±»  
 * åœ¨$G1$ä¸­ä¸åŒç±»ï¼Œåœ¨$G2$ä¸­åŒç±»  

ä¾‹å¦‚æ ·æœ¬ä¸º${a,b,c,d,e,f}$ï¼ŒG1:${(a,b,c),(d,e,f)}$ï¼ŒG2:${(a,b),(c,d,e),f}$  
åˆ™ç”±ä¸‹è¡¨çŸ¥ï¼ŒRI=9/15=0.6  
![rand index](./clustering_pic04.jpg)

åŒ¹é…çŸ©é˜µ$N$  
$RI$å¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ªå¾ˆé•¿çš„å…¬å¼  


```R
choose(150,2)
prod(1:150)/prod(1:148)/prod(1:2)
```


11175



11175



```R
data(iris)
hc=hclust(dist(iris[,-5]))
iris.id=cutree(hc,3)
# è®¡ç®—Rand Index
n=table(iris.id, iris$Species); n
ntot2=sum(n^2)
nitot=rowSums(n)   ##ni.
nitot2=sum(nitot^2)
njtot=colSums(n)   ##n.j
njtot2=sum(njtot^2)

RI=(choose(150,2)+ntot2-0.5*nitot2-0.5*njtot2)/choose(150,2)
RI
```


           
    iris.id setosa versicolor virginica
          1     50          0         0
          2      0         23        49
          3      0         27         1



0.836778523489933



```R
#è°ƒæ•´çš„rand index: pdfCluster::adj.rand.index
library(pdfCluster)   
adj.rand.index(iris.id,iris[,5])   
```


0.64225125183629


### å…±è¡¨å‹ç›¸å…³ç³»æ•°

https://www.cnblogs.com/f-young/p/9248247.html  
Essentially: to measure èšç±»æ–¹æ³•å¾—åˆ°çš„ç»“æ„å’ŒåŸå§‹æ•°æ®ç»“æ„çš„ç›¸ä¼¼æ€§ï¼Œç›¸ä¼¼åˆ™ä¼˜ï¼ˆå³æœªç ´ååŸå§‹æ•°æ®çš„ç»“æ„ï¼‰  
<br />  

CopheneticçŸ©é˜µ$H$çš„æ•°å€¼æ˜¯æ ¹æ®åŸå§‹æ•°æ®ç‚¹ä¹‹é—´çš„è·ç¦»çŸ©é˜µå¾—æ¥çš„ï¼Œå…¶ä¸­$n_{ij}$æ˜¯æ ·æœ¬ç‚¹$i$å’Œ$j$ç¬¬ä¸€æ¬¡è¢«èšä¸ºä¸€ç±»æ—¶ï¼Œè¯¥ç±»å†…å„ç‚¹ä¹‹é—´è·ç¦»çš„æœ€å°å€¼ã€‚  
<br />  

1. å¾—åˆ°åŸå§‹æ•°æ®ç‚¹çš„è·ç¦»çŸ©é˜µ  
 ![è·ç¦»](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/clustering_pic06.png)  
2. å¾—åˆ°å…±è¡¨(Cophenetic)çŸ©é˜µ  
  > å…±è¡¨çŸ©é˜µçš„è®¡ç®—ï¼šä½¿ç”¨åœ¨åˆå¹¶ç±»çš„è¿‡ç¨‹ä¸­ä½¿ç”¨çš„æœ€å°è·ç¦»æ¥å¡«å……è·ç¦»çŸ©é˜µçš„ä¸‹ä¸‰è§’ã€‚  
  ä¾‹ï¼šå¼€å§‹æœ‰6ä¸ªç±»ï¼šAï¼ŒBï¼ŒCï¼ŒDï¼ŒEï¼ŒF$\rightarrow$å°†è·ç¦»æœ€å°ä¸º0.5çš„Då’ŒFèšä¸ºä¸€ç±»(D,F)$\rightarrow$å°†è·ç¦»æœ€å°ä¸º0.71çš„Aå’ŒBèšä¸ºä¸€ç±»(A,B)$\rightarrow$å°†è·ç¦»æœ€å°ä¸º1.00çš„(D,F)å’ŒEèšä¸ºä¸€ç±»(D,F,E)$\rightarrow$å°†è·ç¦»æœ€å°ä¸º1.41çš„(D,F,E)å’ŒCèšä¸ºä¸€ç±»(D,F,E,C)$\rightarrow$å°†è·ç¦»æœ€å°ä¸º2.50çš„(D,F,E,C)å’Œ(A,B)èšä¸ºä¸€ç±»(D,F,E,C,A,B)$\rightarrow$èšç±»ç»“æŸ  
  å¯¹åº”çš„å…±è¡¨çŸ©é˜µæ˜¯ï¼š  
  ![å…±è¡¨](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/clustering_pic05.png)
3. ä¸¤ä¸ªçŸ©é˜µï¼ˆå¯¹åº”ç”Ÿæˆä¸¤ä¸ªå‘é‡ï¼Ÿï¼‰çš„ç›¸å…³ç³»æ•°å³ä¸ºå…±è¡¨å‹ç›¸å…³ç³»æ•°  
  
<font color='#F8D86A'>å…±è¡¨å‹ç›¸å…³ç³»æ•°ï¼šè¶Šå¤§è¶Šå¥½</font>


```R
d=dist(iris[,1:4])   #é¸¢å°¾å±æ¤ç‰©å„ä¸ªæ ·æœ¬ç‚¹çš„è·ç¦»çŸ©é˜µ

# æ¯”è¾ƒæœ€å°è·ç¦»æ³•ä¸æœ€å¤§è·ç¦»æ³•çš„ä¼˜åŠ£

hc1=hclust(d,"complete") #æœ€å¤§è·ç¦»æ³•
d1=cophenetic(hc1) #å…±è¡¨çŸ©é˜µ
cor(d,d1)  #å…±è¡¨çŸ©é˜µä¸è·ç¦»çŸ©é˜µä¹‹é—´çš„ç›¸å…³ç³»æ•°

hc2=hclust(d,"single")  ##æœ€å°è·ç¦»æ³•
d2=cophenetic(hc2) ##å…±è¡¨çŸ©é˜µ
cor(d,d2)  ##ç›¸å…³ç³»æ•°
```


0.726985683628462



0.863878677307658


è¾“å‡ºç»“æœåˆ†åˆ«ä¸º0.7269857ã€0.8638787  
é‡‡ç”¨æœ€å°è·ç¦»æ³•è¦å¥½äºæœ€å¤§è·ç¦»æ³•ã€‚

# å…³è”è§„åˆ™  
Association Rules  
<font color='#F8D86A'>æŒæ¡support confidence liftç­‰æŒ‡æ ‡</font>

1. Support  
$supp(x)=prob(x)$  
2. Confidence  
$conf(x\rightarrow y)=\frac{prob(x\cap y)}{prob(x)}=prob(y|x)$  
 * supportå’Œconfidenceé€šå¸¸ç”¨50%ä½œä¸ºé˜ˆå€¼æ¥åˆ¤æ–­æ˜¯å¦æœ‰ç”¨  
3. Life / Improvement  
$lift(x\rightarrow y)=\frac{prob(x\cap y)}{prob(x)prob(y)}=\frac{prob(y|x)}{prob(y)}$
 * è‹¥$lift(x \rightarrow y)>1$ï¼Œåˆ™$x\rightarrow y$æ˜¯æœ‰ç”¨çš„è§„åˆ™ï¼Œå³ä¹°äº†xçš„äººæ›´å®¹æ˜“ä¹°y  
4. Conviction  
$conv(x\rightarrow y)=\frac{1-supp(x)}{1-conf(x\rightarrow y)}$

**Strengths of Association Rules**  
1. It produces easy to understand results
2. It supports undirected data mining
3. It works on variable length data
4. Rules are relatively easy to compute  

<br />  
**Weaknesses of Association Rules**  
1. It an exponentially growth algorithm
2. It is difficult to determine the optimal number of items
3. It discounts rare items
4. It limited on the support that it provides attributes


```R
# prep school

#rev:åˆ—è¡¨åå‘è¾“å‡º
#class:çœ‹å˜é‡å±æ€§
#æ‹†åˆ†å˜é‡ï¼šseparate(data,id,sep="_",c("code","date"))
#åˆå¹¶å˜é‡ï¼šunite(data,vs_am,c("vs","am"))
  #data(mtcars)
  #dtemp=mtcars[,c(8,9)]
  #dtemp=unite(dtemp,"vs_am",vs,am)
```


```R
library(Matrix)
library(grid)
library(arules)
# library(arulesViz)
# library(tidyr)
```

* æ•°æ®ç®€ä»‹

æŸå…¬å¸100åç”¨æˆ·2016å¹´åœ¨è¯¥å•†åº—çš„è´­ä¹°è®°å½•ã€‚è¿™100åç”¨æˆ·å…±æœ‰3365æ¡æœ‰æ•ˆçš„äº¤æ˜“è®°å½•ã€‚
ä¸»è¦å˜é‡åŒ…æ‹¬ç”¨æˆ·IDã€å•†åº—ã€è´­ä¹°æ—¶é—´ã€å“ç‰Œã€äº§å“ç±»å‹ç­‰ã€‚  
æœ¬æ¡ˆä¾‹ä¸­é€‰å–
>ç”¨æˆ·IDï¼ˆcustomer_sidï¼‰  
è´­ä¹°æ—¶é—´ï¼ˆdateï¼‰  
å“ç‰Œï¼ˆvendor_codeï¼‰ä»¥åŠ  
äº§å“ç±»å‹ï¼ˆåˆ†ä¸ºdepartment_nameã€category_nameã€subcategory_nameä¸‰ä¸ªçº§åˆ«ï¼‰  

è¿›è¡Œæ•°æ®åˆ†æã€‚


```R
data=read.csv("data.csv") #3365 obs
str(data)
levels(data$subcategory_name)
```

    'data.frame':	3365 obs. of  3 variables:
     $ X               : int  1 2 3 4 5 6 7 8 9 10 ...
     $ id              : Factor w/ 1403 levels "-9.16805e+18_2016/1/24",..: 1402 1402 1402 1402 1402 1402 1402 1402 1403 1403 ...
     $ subcategory_name: Factor w/ 79 levels "Accessories",..: 10 44 63 51 37 8 31 59 77 56 ...
    


<ol class=list-inline>
	<li>'Accessories'</li>
	<li>'Bags'</li>
	<li>'Base &amp; Top Coat'</li>
	<li>'BB &amp; CC Cream'</li>
	<li>'Blemish Solutions'</li>
	<li>'Blush'</li>
	<li>'Body'</li>
	<li>'Body Moisturiser'</li>
	<li>'Body Sunscreen'</li>
	<li>'Bronzer'</li>
	<li>'Brow'</li>
	<li>'Brush Cleaner'</li>
	<li>'Brush sets'</li>
	<li>'Call to Action / Redemption'</li>
	<li>'Candles'</li>
	<li>'Cleanser'</li>
	<li>'Collection'</li>
	<li>'Concealer'</li>
	<li>'Conditioner'</li>
	<li>'Dry Shampoo'</li>
	<li>'Electronic Devices'</li>
	<li>'Exfoliate &amp; Scrub'</li>
	<li>'Eye &amp; Brow'</li>
	<li>'Eye Care'</li>
	<li>'Eye Primer &amp; Base'</li>
	<li>'Eyelash Conditioning'</li>
	<li>'Eyeliner'</li>
	<li>'Face Sunscreen'</li>
	<li>'False Lashes'</li>
	<li>'Foundation'</li>
	<li>'Fragrance'</li>
	<li>'Grooming'</li>
	<li>'Hair'</li>
	<li>'Hand Cream'</li>
	<li>'Illuminator'</li>
	<li>'Kits'</li>
	<li>'Kits &amp; Sets'</li>
	<li>'Lip'</li>
	<li>'Lip Care'</li>
	<li>'Lip Crayon '</li>
	<li>'Lip Gloss'</li>
	<li>'Lip Liner'</li>
	<li>'Lip Prep'</li>
	<li>'Lipstick'</li>
	<li>'Loose Powder'</li>
	<li>'Makeup'</li>
	<li>'Makeup Remover'</li>
	<li>'Mani &amp; Pedi'</li>
	<li>'Mascara'</li>
	<li>'Masks'</li>
	<li>'Moisturiser'</li>
	<li>'Multi'</li>
	<li>'Nail Polish'</li>
	<li>'Nail Treatment'</li>
	<li>'Neck Care'</li>
	<li>'Oils &amp; Serums'</li>
	<li>'Other'</li>
	<li>'Packs'</li>
	<li>'Palettes &amp; Sets'</li>
	<li>'Peels'</li>
	<li>'Powder '</li>
	<li>'Pressed Powder'</li>
	<li>'Primer'</li>
	<li>'Scrubs &amp; Exfoliators'</li>
	<li>'Sets'</li>
	<li>'Setting Sprays'</li>
	<li>'Shadow'</li>
	<li>'Shampoo'</li>
	<li>'Skincare'</li>
	<li>'Soap'</li>
	<li>'Sprays and Diffusers'</li>
	<li>'Styling'</li>
	<li>'Tanning'</li>
	<li>'Tinted Moisturiser'</li>
	<li>'Tinted Sunscreen'</li>
	<li>'Toner &amp; Mist'</li>
	<li>'Travel'</li>
	<li>'Treatment'</li>
	<li>'Wash &amp; Bathe'</li>
</ol>




```R
data3=read.transactions("data.csv",format="single",sep=",",cols=c(2,3)) 

#æ¯ä¸ªäº¤æ˜“çš„é¡¹ç›®æ•°
basketSize=size(data3);
summary(basketSize) 
length(basketSize) #3366
sum(basketSize) #3366
```


       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
       1.00    1.00    2.00    2.16    3.00   12.00 



1404



3033



```R
#æ”¯æŒåº¦
itemFreq=itemFrequency(data3)
itemFreq[1:5]
sum(itemFreq)  #1ï¼š"å¹³å‡ä¸€ä¸ªtransactionè´­ä¹°çš„itemä¸ªæ•°"
```


<dl class=dl-horizontal>
	<dt>Accessories</dt>
		<dd>0.00498575498575499</dd>
	<dt>Bags</dt>
		<dd>0.00142450142450142</dd>
	<dt>Base &amp; Top Coat</dt>
		<dd>0.000712250712250712</dd>
	<dt>BB &amp; CC Cream</dt>
		<dd>0.0128205128205128</dd>
	<dt>Blemish Solutions</dt>
		<dd>0.0156695156695157</dd>
</dl>




2.16025641025641



```R
#æŸ¥çœ‹basketSizeçš„åˆ†å¸ƒï¼šå¯†åº¦æ›²çº¿
itemCount=(itemFreq/sum(itemFreq))*sum(basketSize)
  #itemCountè¡¨ç¤ºæ¯ä¸ªitemå‡ºç°çš„æ¬¡æ•°ã€‚Support(X) = Xs/N, Næ˜¯æ€»çš„äº¤æ˜“æ•°,Xså°±æ˜¯Item Xçš„countã€‚
summary(itemCount)
```


       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
       1.00    6.00   21.50   37.91   51.25  274.00 



```R
#æŒ‰æ¯ä¸ªé¡¹ç›®å‡ºç°æ¬¡æ•°æ’åº
orderedItem=sort(itemCount,decreasing=T)
orderedItem[1:10]
```


<dl class=dl-horizontal>
	<dt>Call to Action / Redemption</dt>
		<dd>274</dd>
	<dt>Lipstick</dt>
		<dd>169</dd>
	<dt>Foundation</dt>
		<dd>168</dd>
	<dt>Cleanser</dt>
		<dd>129</dd>
	<dt>Mascara</dt>
		<dd>126</dd>
	<dt>Concealer</dt>
		<dd>121</dd>
	<dt>Moisturiser</dt>
		<dd>119</dd>
	<dt>Primer</dt>
		<dd>92</dd>
	<dt>Fragrance</dt>
		<dd>89</dd>
	<dt>Shadow</dt>
		<dd>89</dd>
</dl>




```R
#æŒ‰æ”¯æŒåº¦supportæ’åº
orderedItemFreq = sort(itemFrequency(data3),decreasing=T)
orderedItemFreq[1:10]
```


<dl class=dl-horizontal>
	<dt>Call to Action / Redemption</dt>
		<dd>0.195156695156695</dd>
	<dt>Lipstick</dt>
		<dd>0.12037037037037</dd>
	<dt>Foundation</dt>
		<dd>0.11965811965812</dd>
	<dt>Cleanser</dt>
		<dd>0.0918803418803419</dd>
	<dt>Mascara</dt>
		<dd>0.0897435897435897</dd>
	<dt>Concealer</dt>
		<dd>0.0861823361823362</dd>
	<dt>Moisturiser</dt>
		<dd>0.0847578347578348</dd>
	<dt>Primer</dt>
		<dd>0.0655270655270655</dd>
	<dt>Fragrance</dt>
		<dd>0.0633903133903134</dd>
	<dt>Shadow</dt>
		<dd>0.0633903133903134</dd>
</dl>




```R
#ç»˜å›¾ï¼šæŒ‰æŸä¸€æœ€å°æ”¯æŒåº¦æŸ¥çœ‹
itemFrequencyPlot(data3, support=0.05,col = "blue",
  xlab=paste("Proportion of Market Baskets Containing Item",  
  "\n(Item Relative Frequency or Support)"))
```


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_104_0.png)



```R
#ç»˜å›¾ï¼šæŒ‰æ’åºæŸ¥çœ‹
itemFrequencyPlot(data3, topN=10,col = "blue",
  xlab = paste("Proportion of Market Baskets Containing Item",  
  "\n(Item Relative Frequency or Support)"))
```


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_105_0.png)


* è¿›è¡Œè§„åˆ™æŒ–æ˜


```R
data=data3

#æ±‚é¢‘ç¹é¡¹é›†
fsets=eclat(data, parameter=list(support =0.05,maxlen=10))
inspect(fsets[1:10])
inspect(sort(fsets, by = "support")[1:10])
```

    Eclat
    
    parameter specification:
     tidLists support minlen maxlen            target   ext
        FALSE    0.05      1     10 frequent itemsets FALSE
    
    algorithmic control:
     sparse sort verbose
          7   -2    TRUE
    
    Absolute minimum support count: 70 
    
    create itemset ... 
    set transactions ...[80 item(s), 1404 transaction(s)] done [0.00s].
    sorting and recoding items ... [15 item(s)] done [0.00s].
    creating sparse bit matrix ... [15 row(s), 1404 column(s)] done [0.00s].
    writing  ... [15 set(s)] done [0.00s].
    Creating S4 object  ... done [0.00s].
         items                         support    count
    [1]  {Call to Action / Redemption} 0.19515670 274  
    [2]  {Foundation}                  0.11965812 168  
    [3]  {Lipstick}                    0.12037037 169  
    [4]  {Concealer}                   0.08618234 121  
    [5]  {Mascara}                     0.08974359 126  
    [6]  {Cleanser}                    0.09188034 129  
    [7]  {Moisturiser}                 0.08475783 119  
    [8]  {Primer}                      0.06552707  92  
    [9]  {Shadow}                      0.06339031  89  
    [10] {Eyeliner}                    0.05840456  82  
         items                         support    count
    [1]  {Call to Action / Redemption} 0.19515670 274  
    [2]  {Lipstick}                    0.12037037 169  
    [3]  {Foundation}                  0.11965812 168  
    [4]  {Cleanser}                    0.09188034 129  
    [5]  {Mascara}                     0.08974359 126  
    [6]  {Concealer}                   0.08618234 121  
    [7]  {Moisturiser}                 0.08475783 119  
    [8]  {Primer}                      0.06552707  92  
    [9]  {Shadow}                      0.06339031  89  
    [10] {Fragrance}                   0.06339031  89  
    


```R
#æ±‚è§„åˆ™
rules=apriori(data,parameter=list(support=0.01,confidence=0.01,minlen=2))
summary(rules)
```

    Apriori
    
    Parameter specification:
     confidence minval smax arem  aval originalSupport maxtime support minlen
           0.01    0.1    1 none FALSE            TRUE       5    0.01      2
     maxlen target   ext
         10  rules FALSE
    
    Algorithmic control:
     filter tree heap memopt load sort verbose
        0.1 TRUE TRUE  FALSE TRUE    2    TRUE
    
    Absolute minimum support count: 14 
    
    set item appearances ...[0 item(s)] done [0.00s].
    set transactions ...[80 item(s), 1404 transaction(s)] done [0.00s].
    sorting and recoding items ... [48 item(s)] done [0.00s].
    creating transaction tree ... done [0.00s].
    checking subsets of size 1 2 3 done [0.00s].
    writing ... [48 rule(s)] done [0.00s].
    creating S4 object  ... done [0.00s].
    


    set of 48 rules
    
    rule length distribution (lhs + rhs):sizes
     2 
    48 
    
       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
          2       2       2       2       2       2 
    
    summary of quality measures:
        support          confidence           lift            count      
     Min.   :0.01068   Min.   :0.05474   Min.   :0.7320   Min.   :15.00  
     1st Qu.:0.01140   1st Qu.:0.10008   1st Qu.:0.8931   1st Qu.:16.00  
     Median :0.01211   Median :0.14168   Median :1.3696   Median :17.00  
     Mean   :0.01386   Mean   :0.14946   Mean   :1.5292   Mean   :19.46  
     3rd Qu.:0.01531   3rd Qu.:0.18134   3rd Qu.:1.8198   3rd Qu.:21.50  
     Max.   :0.02279   Max.   :0.34043   Max.   :3.0781   Max.   :32.00  
    
    mining info:
     data ntransactions support confidence
     data          1404    0.01       0.01



```R
inspect(sort(rules, by = "support")[1:5])   
print("----------------")
inspect(sort(rules, by = "confidence")[1:5])
print("----------------")
inspect(sort(rules, by = "lift")[1:5])
```

        lhs                              rhs                           support   
    [1] {Moisturiser}                 => {Cleanser}                    0.02279202
    [2] {Cleanser}                    => {Moisturiser}                 0.02279202
    [3] {Cleanser}                    => {Call to Action / Redemption} 0.02065527
    [4] {Call to Action / Redemption} => {Cleanser}                    0.02065527
    [5] {Foundation}                  => {Call to Action / Redemption} 0.01923077
        confidence lift      count
    [1] 0.2689076  2.9267149 32   
    [2] 0.2480620  2.9267149 32   
    [3] 0.2248062  1.1519267 29   
    [4] 0.1058394  1.1519267 29   
    [5] 0.1607143  0.8235141 27   
    [1] "----------------"
        lhs              rhs                           support    confidence
    [1] {Makeup}      => {Foundation}                  0.01139601 0.3404255 
    [2] {Lip Liner}   => {Lipstick}                    0.01210826 0.3400000 
    [3] {Moisturiser} => {Cleanser}                    0.02279202 0.2689076 
    [4] {Cleanser}    => {Moisturiser}                 0.02279202 0.2480620 
    [5] {Lip Care}    => {Call to Action / Redemption} 0.01068376 0.2343750 
        lift     count
    [1] 2.844985 16   
    [2] 2.824615 17   
    [3] 2.926715 32   
    [4] 2.926715 32   
    [5] 1.200958 15   
    [1] "----------------"
        lhs              rhs           support    confidence lift     count
    [1] {Eyeliner}    => {Shadow}      0.01139601 0.1951220  3.078104 16   
    [2] {Shadow}      => {Eyeliner}    0.01139601 0.1797753  3.078104 16   
    [3] {Cleanser}    => {Moisturiser} 0.02279202 0.2480620  2.926715 32   
    [4] {Moisturiser} => {Cleanser}    0.02279202 0.2689076  2.926715 32   
    [5] {Makeup}      => {Foundation}  0.01139601 0.3404255  2.844985 16   
    


```R
#æœç´¢è§„åˆ™
rules_Foundation = subset(rules,rhs %in% c("Foundation"))  
inspect(sort(rules_Foundation,by="support"))
inspect(sort(rules_Foundation,by="confidence"))

#ä¿å­˜ç»“æœ
rules_df = as(rules, "data.frame") 
head(rules_df)
# write.csv(rules_df,"rules.csv")
```

        lhs                              rhs          support    confidence
    [1] {Call to Action / Redemption} => {Foundation} 0.01923077 0.09854015
    [2] {Concealer}                   => {Foundation} 0.01851852 0.21487603
    [3] {Primer}                      => {Foundation} 0.01282051 0.19565217
    [4] {Shadow}                      => {Foundation} 0.01210826 0.19101124
    [5] {Mascara}                     => {Foundation} 0.01210826 0.13492063
    [6] {Makeup}                      => {Foundation} 0.01139601 0.34042553
    [7] {Lipstick}                    => {Foundation} 0.01139601 0.09467456
        lift      count
    [1] 0.8235141 27   
    [2] 1.7957497 26   
    [3] 1.6350932 18   
    [4] 1.5963082 17   
    [5] 1.1275510 17   
    [6] 2.8449848 16   
    [7] 0.7912088 16   
        lhs                              rhs          support    confidence
    [1] {Makeup}                      => {Foundation} 0.01139601 0.34042553
    [2] {Concealer}                   => {Foundation} 0.01851852 0.21487603
    [3] {Primer}                      => {Foundation} 0.01282051 0.19565217
    [4] {Shadow}                      => {Foundation} 0.01210826 0.19101124
    [5] {Mascara}                     => {Foundation} 0.01210826 0.13492063
    [6] {Call to Action / Redemption} => {Foundation} 0.01923077 0.09854015
    [7] {Lipstick}                    => {Foundation} 0.01139601 0.09467456
        lift      count
    [1] 2.8449848 16   
    [2] 1.7957497 26   
    [3] 1.6350932 18   
    [4] 1.5963082 17   
    [5] 1.1275510 17   
    [6] 0.8235141 27   
    [7] 0.7912088 16   
    


<table>
<thead><tr><th scope=col>rules</th><th scope=col>support</th><th scope=col>confidence</th><th scope=col>lift</th><th scope=col>count</th></tr></thead>
<tbody>
	<tr><td><span style=white-space:pre-wrap>{Lip Liner} =&gt; {Lipstick}                  </span></td><td>0.01210826                                                                            </td><td>0.34000000                                                                            </td><td>2.824615                                                                              </td><td>17                                                                                    </td></tr>
	<tr><td><span style=white-space:pre-wrap>{Lipstick} =&gt; {Lip Liner}                  </span></td><td>0.01210826                                                                            </td><td>0.10059172                                                                            </td><td>2.824615                                                                              </td><td>17                                                                                    </td></tr>
	<tr><td><span style=white-space:pre-wrap>{Makeup} =&gt; {Foundation}                   </span></td><td>0.01139601                                                                            </td><td>0.34042553                                                                            </td><td>2.844985                                                                              </td><td>16                                                                                    </td></tr>
	<tr><td><span style=white-space:pre-wrap>{Foundation} =&gt; {Makeup}                   </span></td><td>0.01139601                                                                            </td><td>0.09523810                                                                            </td><td>2.844985                                                                              </td><td>16                                                                                    </td></tr>
	<tr><td>{Lip Care} =&gt; {Call to Action / Redemption}</td><td>0.01068376                                    </td><td>0.23437500                                    </td><td>1.200958                                      </td><td>15                                            </td></tr>
	<tr><td>{Call to Action / Redemption} =&gt; {Lip Care}</td><td>0.01068376                                    </td><td>0.05474453                                    </td><td>1.200958                                      </td><td>15                                            </td></tr>
</tbody>
</table>



* å›¾å½¢å±•ç¤º


```R
#æ•£ç‚¹å›¾
opt(4,4)
plot(rules,control=list(jitter=2,col=rev(brewer.pal(9,"Greens")[4:9])),shading="lift") 
# problem here: can't deal with package "arulesViz"
```

    Warning message:
    "package 'arulesViz' was built under R version 3.5.3"


    Error: package or namespace load failed for 'arulesViz' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
     namespace 'jsonlite' 1.5 is already loaded, but >= 1.6 is required
    Traceback:
    

    1. library(arulesViz)

    2. tryCatch({
     .     attr(package, "LibPath") <- which.lib.loc
     .     ns <- loadNamespace(package, lib.loc)
     .     env <- attachNamespace(ns, pos = pos, deps)
     . }, error = function(e) {
     .     P <- if (!is.null(cc <- conditionCall(e))) 
     .         paste(" in", deparse(cc)[1L])
     .     else ""
     .     msg <- gettextf("package or namespace load failed for %s%s:\n %s", 
     .         sQuote(package), P, conditionMessage(e))
     .     if (logical.return) 
     .         message(paste("Error:", msg), domain = NA)
     .     else stop(msg, call. = FALSE, domain = NA)
     . })

    3. tryCatchList(expr, classes, parentenv, handlers)

    4. tryCatchOne(expr, names, parentenv, handlers[[1L]])

    5. value[[3L]](cond)

    6. stop(msg, call. = FALSE, domain = NA)



```R
# Grouped Matrix
plot(rules, method="grouped",control=list(col = rev(brewer.pal(9, "Greens")[4:9]))) 
#å…³è”è§„åˆ™çš„å›¾å½¢å±•ç¤º
plot(rules,measure="confidence",method="graph",control=list(type="items"),shading="lift")
```

# æ ‘

## Intuition  
![tree](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/tree_pic01.png)

## å•æ£µæ ‘

### Information Gain  
>def: æ‹†åˆ†å‰å’Œæ‹†åˆ†åçš„ç†µçš„å·®å€¼  
expected entropy drop after split  

$I=-(p_1 \log_2{p_1}+ ... +p_k \log_2{p_k})=-\sum_{i=1}^k p_i \log_2{p_i}$  
$Gain(X)=I(n,n_1)-E(X)$  
å…¶ä¸­$I(n,n_1)=-(p \log_2{p}+(1-p) \log_2{(1-p)})$ï¼Œ$p=\frac{n_1}{n}$  
$E(X)=\frac{m_1}{n}\ast I(m_1,m_{11})+\frac{m_2}{n}\ast I(m_2,m_{21})+ ... + \frac{m_k}{n}\ast I(m_k,m_{k1})$  
  
<br />  
* An Example  
![infogain](./information_gain.jpg)  
**æ‹†åˆ†å‰**çš„ç†µå€¼ï¼ˆä¸æŒ‰å¹´é¾„åŒºåˆ†æ—¶ï¼Œ16ä¸ªæ ·æœ¬ï¼Œ4æœ‰12æ— ï¼‰--$I(16,4)$  
$I(16,4)=-(4/16 * log_2{4/16}+12/16 * log_2{12/16})=0.8113$  
**æ‹†åˆ†å**çš„ç†µå€¼ï¼ˆæŒ‰å¹´é¾„åŒºåˆ†ï¼Œ6/16åœ¨ç¬¬ä¸€ç»„ï¼Œ10/16åœ¨ç¬¬äºŒç»„ï¼Œç¬¬ä¸€ç»„1æœ‰5æ— ï¼Œç¬¬äºŒç»„3æœ‰10æ— ï¼‰  
$E(å¹´é¾„)=(6/16)*I(6,1)+(10/16)*I(10,3)=0.7946$  
$Gain(å¹´é¾„)=I(16,4)-E(å¹´é¾„)=0.0167$  
<font color='#F8D86A'>ä¿¡æ¯å¢ç›Šè¶Šå¤§è¶Šå¥½ **ç­‰ä»·äº** æ‹†åˆ†åçš„ç†µå€¼è¶Šå°è¶Šå¥½ï¼ˆæ‹†åˆ†å‰çš„ç†µå€¼å¯¹äºæ‰€æœ‰ä¸‹çº§èŠ‚ç‚¹è€Œè¨€éƒ½ä¸€æ ·ï¼‰</font>  


```R
I=function(n,a){
    out=-((a/n)*log(a/n)/log(2)+
         (1-a/n)*log(1-a/n)/log(2))
}
# I(16,4)
a=I(16,4);a
# E(å¹´é¾„)
b=(6/16)*I(6,1)+(10/16)*I(10,3);b
# Gain(å¹´é¾„)
gain1=a-b;gain1
```


0.811278124459133



0.794565220137316



0.0167129043218172



```R
gain2=I(16,4)-9/16*I(9,1)-7/16*I(7,3);gain2
gain3=I(16,4)-6/16*I(6,1)-7/16*I(7,2)-3/16*I(3,1);gain3
```


0.0971580016328469



0.0177239987078822


### Gini Index  


* the same example using Gini Index  
![info_gain](./information_gain.jpg)  

$Gini(split_j)=weighted\, sum\,  of\,  Gini(node)=\frac{n_1}{n} Gini(n_1,n_{11})+ ... +\frac{n_k}{n} Gini(n_k,n_{k1})$  
å…¶ä¸­$Gini(n,a)=1-(\frac{a}{n})^2-(1-\frac{a}{n})^2$  
>worst case: $Gini(n,\frac{n}{2})=1-1/4-1/4=1/2$  
best case: $Gini(n,n)=Gini(n,0)=1-1-0=0$  
  
<font color='#F8D86A'>$Gini$ç³»æ•°è¶Šå°è¶Šå¥½</font>  


```R
gini=function(n,a){
    out=1-(a/n)^2-(1-a/n)^2;
}
gini1=6/16*gini(6,1)+10/16*gini(10,3);gini1
gini2=9/16*gini(9,1)+7/16*gini(7,3);gini2
gini3=6/16*gini(6,1)+7/16*gini(7,2)+3/16*gini(3,1);gini3
```


0.366666666666667



0.325396825396825



0.366071428571429



```R
rm(list=ls())
library(rpart)
library(rpart.plot)

data(iris)
n=nrow(iris)
loc=sample(n,.7*n)
train=iris[loc,]
test=iris[-loc,]
head(train)
```


<table>
<thead><tr><th></th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th></tr></thead>
<tbody>
	<tr><th scope=row>9</th><td>4.4       </td><td>2.9       </td><td>1.4       </td><td>0.2       </td><td>setosa    </td></tr>
	<tr><th scope=row>43</th><td>4.4       </td><td>3.2       </td><td>1.3       </td><td>0.2       </td><td>setosa    </td></tr>
	<tr><th scope=row>77</th><td>6.8       </td><td>2.8       </td><td>4.8       </td><td>1.4       </td><td>versicolor</td></tr>
	<tr><th scope=row>44</th><td>5.0       </td><td>3.5       </td><td>1.6       </td><td>0.6       </td><td>setosa    </td></tr>
	<tr><th scope=row>105</th><td>6.5       </td><td>3.0       </td><td>5.8       </td><td>2.2       </td><td>virginica </td></tr>
	<tr><th scope=row>87</th><td>6.7       </td><td>3.1       </td><td>4.7       </td><td>1.5       </td><td>versicolor</td></tr>
</tbody>
</table>



### ç”Ÿæˆ


```R
model1=rpart(Species~., data=train, control=rpart.control(cp=.05))
summary(model1)
```

    Call:
    rpart(formula = Species ~ ., data = train, control = rpart.control(cp = 0.05))
      n= 105 
    
             CP nsplit  rel error    xerror       xstd
    1 0.5294118      0 1.00000000 1.0882353 0.06873741
    2 0.4117647      1 0.47058824 0.5588235 0.07241452
    3 0.0500000      2 0.05882353 0.1323529 0.04218454
    
    Variable importance
     Petal.Width Petal.Length Sepal.Length  Sepal.Width 
              35           30           19           16 
    
    Node number 1: 105 observations,    complexity param=0.5294118
      predicted class=setosa      expected loss=0.647619  P(node) =1
        class counts:    37    36    32
       probabilities: 0.352 0.343 0.305 
      left son=2 (37 obs) right son=3 (68 obs)
      Primary splits:
          Petal.Length < 2.6  to the left,  improve=35.98431, (0 missing)
          Petal.Width  < 0.8  to the left,  improve=35.98431, (0 missing)
          Sepal.Length < 5.45 to the left,  improve=22.82005, (0 missing)
          Sepal.Width  < 3.15 to the right, improve=17.26667, (0 missing)
      Surrogate splits:
          Petal.Width  < 0.8  to the left,  agree=1.000, adj=1.000, (0 split)
          Sepal.Length < 5.45 to the left,  agree=0.905, adj=0.730, (0 split)
          Sepal.Width  < 3.15 to the right, agree=0.848, adj=0.568, (0 split)
    
    Node number 2: 37 observations
      predicted class=setosa      expected loss=0  P(node) =0.352381
        class counts:    37     0     0
       probabilities: 1.000 0.000 0.000 
    
    Node number 3: 68 observations,    complexity param=0.4117647
      predicted class=versicolor  expected loss=0.4705882  P(node) =0.647619
        class counts:     0    36    32
       probabilities: 0.000 0.529 0.471 
      left son=6 (40 obs) right son=7 (28 obs)
      Primary splits:
          Petal.Width  < 1.65 to the left,  improve=26.682350, (0 missing)
          Petal.Length < 4.75 to the left,  improve=24.857240, (0 missing)
          Sepal.Length < 5.75 to the left,  improve= 7.686275, (0 missing)
          Sepal.Width  < 3.15 to the left,  improve= 4.374156, (0 missing)
      Surrogate splits:
          Petal.Length < 5.05 to the left,  agree=0.882, adj=0.714, (0 split)
          Sepal.Length < 6.15 to the left,  agree=0.721, adj=0.321, (0 split)
          Sepal.Width  < 2.95 to the left,  agree=0.706, adj=0.286, (0 split)
    
    Node number 6: 40 observations
      predicted class=versicolor  expected loss=0.1  P(node) =0.3809524
        class counts:     0    36     4
       probabilities: 0.000 0.900 0.100 
    
    Node number 7: 28 observations
      predicted class=virginica   expected loss=0  P(node) =0.2666667
        class counts:     0     0    28
       probabilities: 0.000 0.000 1.000 
    
    


```R
# è¾“å‡ºèŠ‚ç‚¹ï¼ˆæ–‡å­—ï¼‰
path.rpart(model1,node=c(1,2,3))

# ç»˜å›¾æ³•1
opt(6,4)
rpart.plot(model1,cex=.8)

# ç»˜å›¾æ³•2
if (FALSE){
    plot(model1,asp=4,compress=T)
    text(model1,cex=1,use.n=T)
}
```

    
     node number: 1 
       root
    
     node number: 2 
       root
       Petal.Length< 2.6
    
     node number: 3 
       root
       Petal.Length>=2.6
    


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_126_1.png)


### å‰ªæ

#### é¢„å‰ªæ pre-pruning  
é€šè¿‡æå‰åœæ­¢æ ‘çš„æ„å»ºå¯¹æ ‘å‰ªæï¼Œä¸€æ—¦åœæ­¢ï¼ŒèŠ‚ç‚¹å°±æ˜¯æ ‘å¶ï¼Œè¯¥æ ‘å¶æŒæœ‰å­é›†å…ƒç´ ä¸­æœ€é¢‘ç¹çš„ç±»ã€‚åœæ­¢å†³ç­–æ ‘ç”Ÿé•¿æœ€ç®€å•çš„æ–¹æ³•æœ‰ï¼š  
>1. å®šä¹‰ä¸€ä¸ªé«˜åº¦ï¼Œå½“å†³ç­–æ ‘è¾¾åˆ°è¯¥é«˜åº¦æ—¶å°±åœæ­¢å†³ç­–æ ‘çš„ç”Ÿé•¿
2. è¾¾åˆ°æŸä¸ªèŠ‚ç‚¹çš„å®ä¾‹å…·æœ‰ç›¸åŒçš„ç‰¹å¾å‘é‡ï¼ŒåŠæ—¶è¿™äº›å®ä¾‹ä¸å±äºåŒä¸€ç±»ï¼Œä¹Ÿå¯ä»¥åœæ­¢å†³ç­–æ ‘çš„ç”Ÿé•¿ã€‚è¿™ä¸ªæ–¹æ³•å¯¹äºå¤„ç†æ•°æ®çš„æ•°æ®å†²çªé—®é¢˜æ¯”è¾ƒæœ‰æ•ˆã€‚
3. å®šä¹‰ä¸€ä¸ªé˜ˆå€¼ï¼Œå½“è¾¾åˆ°æŸä¸ªèŠ‚ç‚¹çš„å®ä¾‹ä¸ªæ•°å°äºé˜ˆå€¼æ—¶å°±å¯ä»¥åœæ­¢å†³ç­–æ ‘çš„ç”Ÿé•¿
4. å®šä¹‰ä¸€ä¸ªé˜ˆå€¼ï¼Œé€šè¿‡è®¡ç®—æ¯æ¬¡æ‰©å¼ å¯¹ç³»ç»Ÿæ€§èƒ½çš„å¢ç›Š(ä¾‹å¦‚information gain > c?)ï¼Œå¹¶æ¯”è¾ƒå¢ç›Šå€¼ä¸è¯¥é˜ˆå€¼å¤§å°æ¥å†³å®šæ˜¯å¦åœæ­¢å†³ç­–æ ‘çš„ç”Ÿé•¿ã€‚


#### åå‰ªæ post-pruning  
é¦–å…ˆ**æ„é€ å®Œæ•´çš„å†³ç­–æ ‘ï¼Œå…è®¸æ ‘è¿‡åº¦æ‹Ÿåˆè®­ç»ƒæ•°æ®**ï¼Œç„¶åå¯¹é‚£äº›ç½®ä¿¡åº¦ä¸å¤Ÿçš„ç»“ç‚¹å­æ ‘ç”¨å¶å­ç»“ç‚¹æ¥ä»£æ›¿ï¼Œè¯¥å¶å­çš„ç±»æ ‡å·ç”¨è¯¥ç»“ç‚¹å­æ ‘ä¸­æœ€é¢‘ç¹çš„ç±»æ ‡è®°ã€‚ç›¸æ¯”äºå…ˆå‰ªæï¼Œè¿™ç§æ–¹æ³•æ›´å¸¸ç”¨ï¼Œæ­£æ˜¯å› ä¸ºåœ¨å…ˆå‰ªææ–¹æ³•ä¸­ç²¾ç¡®åœ°ä¼°è®¡ä½•æ—¶åœæ­¢æ ‘å¢é•¿å¾ˆå›°éš¾ã€‚å¸¸è§çš„åå‰ªææ–¹æ³•ï¼š  
>1. Reduced-Error Pruning(REP,é”™è¯¯ç‡é™ä½å‰ªæï¼‰
2. Pesimistic-Error Pruning(PEP,æ‚²è§‚é”™è¯¯å‰ªæï¼‰
3. Cost-Complexity Pruningï¼ˆCCPï¼Œä»£ä»·å¤æ‚åº¦å‰ªæ)
4. EBP(Error-Based Pruning)ï¼ˆåŸºäºé”™è¯¯çš„å‰ªæï¼‰  
<br />  

* Reduced Error Pruning (REP)  
REPæ–¹æ³•æ˜¯ä¸€ç§æ¯”è¾ƒç®€å•çš„åå‰ªæçš„æ–¹æ³•ï¼Œåœ¨è¯¥æ–¹æ³•ä¸­ï¼Œå¯ç”¨çš„æ•°æ®è¢«åˆ†æˆä¸¤ä¸ªæ ·ä¾‹é›†åˆï¼š**ä¸€ä¸ªè®­ç»ƒé›†ç”¨æ¥å½¢æˆå­¦ä¹ åˆ°çš„å†³ç­–æ ‘ï¼Œä¸€ä¸ªåˆ†ç¦»çš„éªŒè¯é›†ç”¨æ¥è¯„ä¼°è¿™ä¸ªå†³ç­–æ ‘åœ¨åç»­æ•°æ®ä¸Šçš„ç²¾åº¦**ï¼Œç¡®åˆ‡åœ°è¯´æ˜¯ç”¨æ¥è¯„ä¼°ä¿®å‰ªè¿™ä¸ªå†³ç­–æ ‘çš„å½±å“ã€‚  
è¿™ä¸ªæ–¹æ³•çš„åŠ¨æœºæ˜¯ï¼šå³ä½¿å­¦ä¹ å™¨å¯èƒ½ä¼šè¢«è®­ç»ƒé›†ä¸­çš„éšæœºé”™è¯¯å’Œå·§åˆè§„å¾‹æ‰€è¯¯å¯¼ï¼Œä½†**éªŒè¯é›†åˆä¸å¤§å¯èƒ½è¡¨ç°å‡ºåŒæ ·çš„éšæœºæ³¢åŠ¨**ã€‚æ‰€ä»¥éªŒè¯é›†å¯ä»¥ç”¨æ¥å¯¹è¿‡åº¦æ‹Ÿåˆè®­ç»ƒé›†ä¸­çš„è™šå‡ç‰¹å¾æä¾›é˜²æŠ¤æ£€éªŒã€‚  
REPæ˜¯æœ€ç®€å•çš„åå‰ªææ–¹æ³•ä¹‹ä¸€ï¼Œä¸è¿‡ç”±äºä½¿ç”¨ç‹¬ç«‹çš„æµ‹è¯•é›†ï¼ŒåŸå§‹å†³ç­–æ ‘ç›¸æ¯”ï¼Œä¿®æ”¹åçš„å†³ç­–æ ‘**å¯èƒ½åå‘äºè¿‡åº¦ä¿®å‰ª**ã€‚
<br />  
  
* Minimal cost-complexity  pruning ä»£ä»·-å¤æ‚åº¦å‰ªæ  
$R_\alpha (T) = R(T)+\alpha |\tilde{T}|$  
å…¶ä¸­$T$ä»£è¡¨æ ‘æ¨¡å‹ï¼Œ$R(T)$æ˜¯è¯¯å·®å¤§å°ï¼Œ$|\tilde{T}|$è¡¨ç¤ºæ ‘çš„å¶èŠ‚ç‚¹ä¸ªæ•°ï¼Œå³è¡¨ç¤ºæ ‘çš„å¤æ‚åº¦ã€‚  
$\alpha$æ˜¯å¯¹å¤æ‚åº¦çš„æƒ©ç½šç³»æ•°ï¼Œå¯ä»¥ç”±ç”¨æˆ·è®¾ç½®ï¼š  
 * $\alpha$è¶Šå¤§ï¼Œå¶èŠ‚ç‚¹çš„ä¸ªæ•°å¯¹æŸå¤±å‡½æ•°çš„å½±å“è¶Šå¤§ï¼Œå‰ªæä¹‹åçš„å†³ç­–æ ‘æ›´å®¹æ˜“é€‰æ‹©å¤æ‚åº¦è¾ƒå°çš„æ ‘ï¼›
 * $\alpha$è¶Šå°ï¼Œè¡¨ç¤ºå¶èŠ‚ç‚¹çš„ä¸ªæ•°å¯¹æŸå¤±å‡½æ•°çš„å½±å“è¶Šå°ã€‚



* CCPç®—æ³•çš„æ­¥éª¤  
 1. å¯¹äºå®Œå…¨å†³ç­–æ ‘$ğ‘‡$çš„æ¯ä¸ªéå¶ç»“ç‚¹è®¡ç®—$\alpha$å€¼ï¼Œå¾ªç¯å‰ªæ‰å…·æœ‰æœ€å°$\alpha$å€¼çš„å­æ ‘ï¼Œç›´åˆ°å‰©ä¸‹æ ¹èŠ‚ç‚¹ã€‚åœ¨è¯¥æ­¥å¯å¾—åˆ°**ä¸€ç³»åˆ—çš„å‰ªææ ‘$ï½›ğ‘‡_0ï¼Œğ‘‡_1ï¼Œğ‘‡_2, â€¦, ğ‘‡_ğ‘šï½$**,å…¶ä¸­$ğ‘‡_0$ä¸ºåŸæœ‰çš„å®Œå…¨å†³ç­–æ ‘ï¼Œ$ğ‘‡_ğ‘š$ä¸ºæ ¹ç»“ç‚¹ï¼Œ$ğ‘‡_(ğ‘–+1)$ä¸ºå¯¹$ğ‘‡_ğ‘–$è¿›è¡Œå‰ªæçš„ç»“æœï¼›
 2. **ä»å­æ ‘åºåˆ—ä¸­**ï¼Œæ ¹æ®çœŸå®çš„è¯¯å·®ä¼°è®¡é€‰æ‹©æœ€ä½³å†³ç­–æ ‘ã€‚

#### é¢„å‰ªæå’Œåå‰ªæçš„æ¯”è¾ƒ
* åå‰ªæå†³ç­–æ ‘é€šå¸¸æ¯”é¢„å‰ªæå†³ç­–æ ‘ä¿ç•™äº†æ›´å¤šçš„åˆ†æ”¯ï¼›
* åå‰ªæå†³ç­–æ ‘çš„æ¬ æ‹Ÿåˆé£é™©å¾ˆå°ï¼Œæ³›åŒ–æ€§èƒ½å¾€å¾€ä¼˜äºé¢„å‰ªæå†³ç­–æ ‘ï¼›
* åå‰ªæå†³ç­–æ ‘è®­ç»ƒæ—¶é—´å¼€é”€æ¯”æœªå‰ªæå†³ç­–æ ‘å’Œé¢„å‰ªæå†³ç­–æ ‘éƒ½è¦å¤§çš„å¤šã€‚

#### ä¸€å€æ ‡å‡†å·®è§„åˆ™  
åœ¨å‰ªæç†è®ºä¸­ï¼Œæ¯”è¾ƒè‘—åçš„è§„åˆ™å°±æ˜¯1-SEï¼ˆ1æ ‡å‡†å·®ï¼‰è§„åˆ™ã€‚  
é¦–å…ˆè¦ä¿è¯é¢„æµ‹è¯¯å·®ï¼ˆé€šè¿‡äº¤å‰éªŒè¯è·å¾—ï¼Œåœ¨ç¨‹åºä¸­è¡¨ç¤ºä¸ºxerrorï¼‰å°½é‡å°ï¼Œä½†ä¸ä¸€å®šè¦å–æœ€å°å€¼ï¼Œè€Œæ˜¯å…è®¸å®ƒåœ¨â€œæœ€å°çš„è¯¯å·®Â±ä¸€ä¸ªæ ‡å‡†å·®â€çš„èŒƒå›´å†…ï¼Œç„¶ååœ¨æ­¤èŒƒå›´å†…é€‰å–å°½é‡å°çš„å¤æ‚æ€§å‚é‡ï¼Œè¿›è€Œä»¥å®ƒä¸ºä¾æ®è¿›è¡Œå‰ªæã€‚  
è¿™ä¸ªè§„åˆ™ä½“ç°äº†**å…¼é¡¾æ ‘çš„è§„æ¨¡ï¼ˆå¤æ‚æ€§ï¼‰å’Œè¯¯å·®å¤§å°çš„æ€æƒ³**ã€‚ä¸€èˆ¬è¯´æ¥ï¼Œéšç€æ‹†åˆ†çš„å¢å¤šï¼Œå¤æ‚æ€§å‚é‡ä¼šå•è°ƒä¸‹é™ï¼ˆçº¯åº¦è¶Šæ¥è¶Šé«˜ï¼‰ï¼Œä½†æ˜¯é¢„æµ‹è¯¯å·®åˆ™ä¼šå…ˆé™åå‡ï¼Œè¿™æ ·ï¼Œæˆ‘ä»¬å°±**æ— æ³•ä½¿å¤æ‚æ€§å’Œè¯¯å·®åŒæ—¶é™åˆ°æœ€ä½**ï¼Œå› æ­¤å…è®¸è¯¯å·®å¯ä»¥åœ¨ä¸€ä¸ªæ ‡å‡†å·®å†…æ³¢åŠ¨ã€‚

#### å‰ªæçš„Rå®ç°


```R
if (FALSE){ #æ³•1
    model2=snip.rpart(model1,toss=2)
    opt(6,4)
    rpart.plot(model2)
}
if (TRUE) { #æ³•2
    model2=prune(model1, cp=.01)
    opt(6,4)
    rpart.plot(model2,cex=.8)
}
```


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_134_0.png)


### æ£€éªŒ


```R
# æ··æ·†çŸ©é˜µ
class=predict(model2, newdata=test, type="class")
table(class, test$Species, dnn=c("Pred","True"))
```


                True
    Pred         setosa versicolor virginica
      setosa         12          0         0
      versicolor      0         14         2
      virginica       0          1        16


å¤šæ£µæ ‘ï¼šæŒæ¡åŸºæœ¬æ€æƒ³ã€é€‚ç”¨æƒ…å†µï¼ˆå¼ºåˆ†ç±»å™¨ï¼Ÿå¼±åˆ†ç±»å™¨ï¼Ÿå¤šå¼±å¤šå¼ºï¼Ÿï¼‰

## boosting

Gradient Boosting | R - https://blog.csdn.net/dingming001/article/details/72989547  
XGBoost | R - https://blog.csdn.net/qq_26074991/article/details/51737030  
XGBoost - https://blog.csdn.net/qq_34264472/article/details/53363384  
XGBoost - https://blog.csdn.net/langyichao1/article/details/70183127

## bagging


**æ­¥éª¤**  
è®¾è®­ç»ƒæ ·æœ¬é›†åˆLä¸º${(x_n,y_n),n=1,2,3,...,N}$ï¼Œ$x_n$ä¸º$p$ç»´å‘é‡ï¼Œ$y_n$ä¸ºå› å˜é‡ï¼Œæ˜¯å–å€¼${1,2,...,J}$çš„åˆ†ç±»å˜é‡ã€‚  
æ„å»º$M$æ£µå†³ç­–æ ‘$H_m(x,L_m),m=1,2,...,M$ï¼Œç»„åˆ$M$æ£µæ ‘å¾—åˆ°æœ€ç»ˆåˆ†ç±»å™¨$H_{agg}$ã€‚  
  
1. å¯¹æ ·æœ¬é›†åˆ$L$è¿›è¡ŒBootstrapæŠ½æ ·å¾—åˆ°è®­ç»ƒæ ·æœ¬é›†$L_m$  
>å¯¹${(x_n,y_n),n=1,2,...,N}$ä¸­çš„$N$ä¸ªæ ·æœ¬ç‚¹è¿›è¡Œæ¦‚ç‡ä¸º$\frac{1}{N}$çš„ç­‰æ¦‚ç‡æœ‰æ”¾å›æŠ½æ ·ï¼Œæ ·æœ¬é‡ä¸º$N$ã€‚  

å¯¹$L_m$æ„å»ºåˆ†ç±»å™¨ï¼ˆå†³ç­–æ ‘ï¼‰$H_m(x,Lm)$  
2. ç»„åˆ$M$æ£µå†³ç­–æ ‘å¾—åˆ°æœ€ç»ˆåˆ†ç±»å™¨$H_B$ï¼Œ$H_B$å¯¹æ ·æœ¬xçš„é¢„æµ‹ä¸º$argmax_j \sum_{m=1}^M {I(H_m(x,L_m)=j)}$, $I()$ä¸ºç¤ºæ€§å‡½æ•°  

* å¦‚æœç”ŸæˆåŸºé¢„æµ‹å™¨çš„ç®—æ³•æ˜¯ä¸ç¨³å®šçš„(unstable)ï¼Œé€šè¿‡baggingå¾—åˆ°çš„æœ€ç»ˆé¢„æµ‹æ¨¡å‹è™½ç„¶å¤±å»äº†ç®€å•çš„å¯è§£é‡Šçš„æ ‘å‹ç»“æ„ï¼Œä½†é¢„æµ‹ç²¾åº¦å¾€å¾€ä¼šå¤§å¤§é«˜äºå•ä¸ªåŸºé¢„æµ‹å™¨çš„é¢„æµ‹ç²¾åº¦ã€‚  
* Baggingç®—æ³•å¯¹äºåŸºé¢„æµ‹å™¨ä¸ç¨³å®šçš„æƒ…å†µå¾ˆæœ‰ä½œç”¨ï¼Œå¯¹äºç¨³å®šçš„åŸºé¢„æµ‹å™¨ï¼Œbaggingå¹¶ä¸æœ‰æ•ˆã€‚  
* ä»åå·®æ–¹å·®åˆ†è§£çš„è§’åº¦çœ‹ï¼ŒBaggingç®—æ³•å¯ä»¥æé«˜ä¸ç¨³å®šåŸºé¢„æµ‹å™¨çš„é¢„æµ‹ç²¾åº¦ï¼Œå®è´¨ä¸Šæ˜¯å‡å°äº†é¢„æµ‹çš„æ–¹å·®(variance)ï¼Œä½†å¹¶æ²¡æœ‰é™ä½åå·®(bias)ã€‚  
> * ä¸ç¨³å®šçš„ç®—æ³•: å†³ç­–æ ‘ï¼Œç¥ç»ç½‘ç»œï¼ŒMARS (multivariate splines)ï¼Œå’Œå­é›†å›å½’(subset regression)ç­‰ç­‰ï¼›  
* ç¨³å®šçš„ç®—æ³•: å²­å›å½’(ridge regression)ï¼Œæœ€è¿‘é‚»æ–¹æ³•(K-nearest neighbor)ï¼Œå’Œçº¿æ€§åˆ¤åˆ«æ–¹æ³•(Linear discriminate)ç­‰ç­‰ã€‚  


* æ¯æ¬¡è¿›è¡ŒbootstrapæŠ½æ ·çš„æ—¶å€™ï¼Œæˆ‘ä»¬é€‰æ‹©çš„æ ·æœ¬é‡ç›¸ç­‰äºåŸå§‹è®­ç»ƒé›†çš„æ ·æœ¬é‡ã€‚å› ä¸ºbootstrapæ˜¯æœ‰æ”¾å›çš„é‡å¤æŠ½æ ·ï¼Œæ‰€ä»¥æœ‰äº›æ ·æœ¬ç‚¹è¢«æŠ½ä¸­çš„æ¬¡æ•°è¶…è¿‡ä¸€æ¬¡ï¼Œæœ‰äº›æ ·æœ¬ç‚¹æ²¡æœ‰è¢«æŠ½ä¸­ã€‚æ ¹æ®bootstrapæŠ½æ ·çš„ç†è®ºï¼Œå½“æ ·æœ¬é‡ä¸º$N$æ—¶ï¼Œå¤§çº¦æœ‰37%çš„æ ·æœ¬ç‚¹æ²¡æœ‰è¢«æŠ½ä¸­ã€‚  
* ç ”ç©¶è¡¨æ˜ï¼Œæé«˜bootstrapæŠ½æ ·çš„æ ·æœ¬é‡å¹¶ä¸èƒ½æ”¹å–„baggingç®—æ³•çš„é¢„æµ‹ç²¾åº¦ã€‚  

## AdaBoost

è‡ªä»AdaBoost æ–¹æ³•æå‡ºä¹‹åï¼Œå¤§é‡æ¨¡æ‹Ÿå’Œåº”ç”¨ç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•å¯ä»¥å¤§å¤§é™ä½åŸºåˆ†ç±»å™¨(æ¯”å¦‚å†³ç­–æ ‘)çš„é¢„æµ‹è¯¯å·®ï¼ŒBreiman(1996)æ›¾æŒ‡å‡ºAdaBoost æ–¹æ³•æ˜¯ç°æœ‰çš„åˆ†ç±»æ–¹æ³•ä¸­æœ€å¥½çš„ä¸€ç§ã€‚Breiman(1996)è®¤ä¸º<font color='#F8D86A'>æœ€ä¸»è¦çš„åŸå› æ˜¯AdaBoostæ–¹æ³•çš„**è‡ªé€‚åº”çš„é‡æ–°æŠ½æ ·çš„æ–¹æ³•**ï¼Œè€Œä¸æ˜¯å…¶å…·ä½“å®šä¹‰çš„æ›´æ–°p(n)çš„å…¬å¼ã€‚</font>  

<font color='#F8D86A'>äºŒåˆ†ç±»Adaboostç®—æ³•æ˜¯æœ€å°åŒ–æŒ‡æ•°æŸå¤±$L(y,f(x))=\exp{(-yf(x))}$çš„åˆ†æ­¥å‘å‰å¯åŠ æ¨¡å‹ã€‚</font>


```R
rm(list=ls())
library(adabag)

data(iris)
n=nrow(iris)
loc=sample(n, .7*n)
train=iris[loc,]
test=iris[-loc,]
```

    Warning message:
    "package 'adabag' was built under R version 3.5.3"Loading required package: rpart
    Loading required package: caret
    Warning message:
    "package 'caret' was built under R version 3.5.3"Loading required package: lattice
    Loading required package: ggplot2
    Loading required package: foreach
    Warning message:
    "package 'foreach' was built under R version 3.5.3"Loading required package: doParallel
    Warning message:
    "package 'doParallel' was built under R version 3.5.3"Loading required package: iterators
    Warning message:
    "package 'iterators' was built under R version 3.5.3"Loading required package: parallel
    


```R
if (TRUE){
    err=c()
    for (i in 1:20){ #åŸºåˆ†ç±»å™¨ä¸ªæ•°ä»1åˆ°20
      print(paste0("i=",i))
      iada=boosting(Species~., data=train, mfinal=i)
      ipred=predict.boosting(iada, newdata=test)
      err=c(err, ipred$error)
    }
    derr=data.frame(i=1:20, error=err); derr
}

mycol=RColorBrewer::brewer.pal(5,"Set1")[2]
fig0=ggplot(derr)+
  theme_bw()+
  geom_point(aes(i, error), shape=19, size=2, col=mycol)+
  geom_line(aes(i, error), col=mycol)+
  scale_x_continuous(breaks=seq(1,20,1))+
  xlab("The Number of Basic Classifiers")+ylab("Error")
fig0
```


<table>
<thead><tr><th scope=col>i</th><th scope=col>error</th></tr></thead>
<tbody>
	<tr><td> 1        </td><td>0.06666667</td></tr>
	<tr><td> 2        </td><td>0.02222222</td></tr>
	<tr><td> 3        </td><td>0.06666667</td></tr>
	<tr><td> 4        </td><td>0.06666667</td></tr>
	<tr><td> 5        </td><td>0.02222222</td></tr>
	<tr><td> 6        </td><td>0.04444444</td></tr>
	<tr><td> 7        </td><td>0.02222222</td></tr>
	<tr><td> 8        </td><td>0.06666667</td></tr>
	<tr><td> 9        </td><td>0.02222222</td></tr>
	<tr><td>10        </td><td>0.04444444</td></tr>
	<tr><td>11        </td><td>0.08888889</td></tr>
	<tr><td>12        </td><td>0.06666667</td></tr>
	<tr><td>13        </td><td>0.04444444</td></tr>
	<tr><td>14        </td><td>0.06666667</td></tr>
	<tr><td>15        </td><td>0.06666667</td></tr>
	<tr><td>16        </td><td>0.06666667</td></tr>
	<tr><td>17        </td><td>0.04444444</td></tr>
	<tr><td>18        </td><td>0.04444444</td></tr>
	<tr><td>19        </td><td>0.04444444</td></tr>
	<tr><td>20        </td><td>0.04444444</td></tr>
</tbody>
</table>






![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_148_2.png)



```R
#é€‰å–i=9çš„æ¨¡å‹
iada=boosting(Species~., data=train, mfinal=10)
ipred=predict.boosting(iada, newdata=test)

ipred$confusion #æ··æ·†çŸ©é˜µ
#ç›¸å½“äºtable(dft$Species1,dft$Species0, dnn=c("Observed","Predicted"))
```


                   Observed Class
    Predicted Class setosa versicolor virginica
         setosa         15          0         0
         versicolor      0         11         3
         virginica       0          0        16



```R
importanceplot(iada)
```


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_150_0.png)


## Random Forest

ä¸baggingåªåœ¨æ•°æ®ï¼ˆè¡Œï¼‰çš„å±‚é¢ä¸Šéšæœºä¸åŒï¼Œ<font color='#F8D86A'>éšæœºæ£®æ—åœ¨å˜é‡ï¼ˆåˆ—ï¼‰çš„ä½¿ç”¨å’Œæ•°æ®ï¼ˆè¡Œï¼‰çš„ä½¿ç”¨ä¸Šéƒ½è¿›è¡ŒéšæœºåŒ–ï¼Œç”Ÿæˆå¤šé¢—åˆ†ç±»æ ‘ï¼Œæœ€åæ±‡æ€»å…¶ç»“æœã€‚</font> è¿™æ ·éšæœºæ£®æ—ç®—æ³•å°±å¯ä»¥**é™ä½æ ‘ä¸æ ‘ä¹‹é—´çš„ç›¸å…³å…³ç³»**ï¼Œå¹¶ä¸”å°½å¯èƒ½çš„æ§åˆ¶å¹³å‡æ–¹å·®ã€‚  

<br />  
å¯¹äºæ¯æ£µæ ‘$ğ‘=1,2,â€¦,ğµ$,  
1. ä»å…¨éƒ¨è®­ç»ƒæ ·æœ¬å•å…ƒä¸­ï¼Œé‡‡ç”¨bootstrapæŠ½æ ·æ–¹æ³•æŠ½å–nä¸ªæ ·æœ¬å•å…ƒæ„æˆbootstrapæ ·æœ¬é›† $ğ‘^âˆ—$, åŸºäº$ğ‘^âˆ—$æ„é€ ä¸€æ£µæ ‘$ğ»_ğ‘$ï¼Œ  
2. å¯¹æ ‘ä¸Šçš„**æ¯ä¸ªèŠ‚ç‚¹**ï¼Œé‡å¤ä»¥ä¸‹æ­¥éª¤ï¼Œç›´åˆ°èŠ‚ç‚¹çš„æ ·æœ¬æ•°è¾¾åˆ°æŒ‡å®šçš„æœ€å°é™å®šå€¼ğ‘›_ğ‘šğ‘–ğ‘›ï¼š  
 * ä»å…¨éƒ¨pä¸ªå˜é‡ä¸­éšæœºé€‰å–mä¸ªï¼›
 * ä»mä¸ªéšæœºå˜é‡ä¸­å–æœ€ä½³çš„åˆ†æå˜é‡ï¼›
 * åœ¨è¿™ä¸€èŠ‚ç‚¹åˆ†è£‚æˆä¸¤ä¸ªå­èŠ‚ç‚¹ã€‚  
 
<br />
 éšæœºæ£®æ—åœ¨è¿ç®—é‡æ²¡æœ‰æ˜¾è‘—æé«˜çš„å‰æä¸‹æé«˜äº†é¢„æµ‹ç²¾åº¦ã€‚éšæœºæ£®æ—å¯¹å¤šå…ƒçº¿æ€§ä¸æ•æ„Ÿï¼Œç»“æœå¯¹ç¼ºå¤±æ•°æ®å’Œéå¹³è¡¡çš„æ•°æ®æ¯”è¾ƒç¨³å¥ï¼Œå¯ä»¥å¾ˆå¥½åœ°é¢„æµ‹å¤šè¾¾å‡ åƒä¸ªè§£é‡Šå˜é‡çš„ä½œç”¨ï¼ˆé€‚ç”¨äºå˜é‡å¤šæƒ…å½¢ï¼‰ã€‚  


```R
rm(list=ls())
library(randomForest)

data(iris)
n=nrow(iris)
loc=sample(n,.7*n)
train=iris[loc,]
test=iris[-loc,]
```


```R
# set.seed(71) # why do we need to set seed here?
iris.rf <- randomForest(Species ~ ., data=train, importance=TRUE, proximity=TRUE)
print(iris.rf)
```

    
    Call:
     randomForest(formula = Species ~ ., data = train, importance = TRUE,      proximity = TRUE) 
                   Type of random forest: classification
                         Number of trees: 500
    No. of variables tried at each split: 2
    
            OOB estimate of  error rate: 3.81%
    Confusion matrix:
               setosa versicolor virginica class.error
    setosa         34          0         0  0.00000000
    versicolor      0         35         2  0.05405405
    virginica       0          2        32  0.05882353
    


```R
#Look at variable importance:
round(importance(iris.rf), 2)
```


<table>
<thead><tr><th></th><th scope=col>setosa</th><th scope=col>versicolor</th><th scope=col>virginica</th><th scope=col>MeanDecreaseAccuracy</th><th scope=col>MeanDecreaseGini</th></tr></thead>
<tbody>
	<tr><th scope=row>Sepal.Length</th><td> 6.74</td><td> 8.64</td><td> 6.16</td><td>10.42</td><td> 6.41</td></tr>
	<tr><th scope=row>Sepal.Width</th><td> 3.98</td><td> 1.87</td><td> 0.93</td><td> 3.15</td><td> 1.51</td></tr>
	<tr><th scope=row>Petal.Length</th><td>22.24</td><td>31.32</td><td>26.03</td><td>33.15</td><td>31.05</td></tr>
	<tr><th scope=row>Petal.Width</th><td>22.14</td><td>32.65</td><td>30.06</td><td>34.41</td><td>30.34</td></tr>
</tbody>
</table>




```R
ipred=predict(iris.rf, newdata=test)
table(ipred, test$Species, dnn=c("Pred","True"))
```


                True
    Pred         setosa versicolor virginica
      setosa         16          0         0
      versicolor      0         11         0
      virginica       0          2        16


# LDA & SVM

also to solve the classification problem  
  
read the following if interested  
>https://blog.csdn.net/v_july_v/article/details/7624837#t34  
https://www.zhihu.com/question/24627  
https://www.csie.ntu.edu.tw/~cjlin/libsvm/![image.png](attachment:image.png)

## LDA (Linear Discriminant)


```R
library(MASS) #è¦ç”¨åˆ°MASS::ldaå‡½æ•°
data(iris)
iris.lda=lda(Species~.,data=iris)
iris.lda
```


    Call:
    lda(Species ~ ., data = iris)
    
    Prior probabilities of groups:
        setosa versicolor  virginica 
     0.3333333  0.3333333  0.3333333 
    
    Group means:
               Sepal.Length Sepal.Width Petal.Length Petal.Width
    setosa            5.006       3.428        1.462       0.246
    versicolor        5.936       2.770        4.260       1.326
    virginica         6.588       2.974        5.552       2.026
    
    Coefficients of linear discriminants:
                        LD1         LD2
    Sepal.Length -0.8293776  0.02410215
    Sepal.Width  -1.5344731  2.16452123
    Petal.Length  2.2012117 -0.93192121
    Petal.Width   2.8104603  2.83918785
    
    Proportion of trace:
       LD1    LD2 
    0.9912 0.0088 



```R
ipred=predict(iris.lda)$class
table(ipred, iris$Species, dnn=c("Pred","True"))
```


                True
    Pred         setosa versicolor virginica
      setosa         50          0         0
      versicolor      0         48         1
      virginica       0          2        49


## SVM (Support Vector Machine)

* <font color='#F8D86A'>æŒæ¡çº¿æ€§å¯åˆ†æƒ…å†µï¼Œå…¶ä»–æƒ…å†µäº†è§£ï¼Œå›å½’æƒ…å†µä¸è¦æ±‚  
* æ ¸å‡½æ•°ä¸ºä»€ä¹ˆä¼šæå‡ºæ¥?</font>  
<br /> 
æ”¯æŒå‘é‡æœº(SupportVectorMachine)æ˜¯Corteså’ŒVapnikäº1995å¹´é¦–å…ˆæå‡ºçš„ä¸€ç§åˆ†ç±»ç®—æ³•ã€‚ 
<br /> 
* ä¼˜ç‚¹  
 * å®ƒåœ¨è§£å†³å°æ ·æœ¬ï¼ˆå¤§æ ·æœ¬æ—¶ï¼Œç¨€ç–ä¹Ÿå¯ä»¥ï¼Œä½†éœ€è¦å°å¿ƒï¼‰ã€éçº¿æ€§åŠé«˜ç»´æ¨¡å¼è¯†åˆ«ä¸­è¡¨ç°å‡ºè®¸å¤šç‰¹æœ‰çš„ä¼˜åŠ¿ã€‚  
 * å›å½’å’Œåˆ†ç±»å‡å¯ä»¥å¤„ç†ï¼Œç›¸å¯¹å®¹æ˜“è®­ç»ƒï¼›ä¸åƒç¥ç»ç½‘ç»œä¼šå¯¼è‡´å±€éƒ¨æœ€ä¼˜ï¼Œæ”¯æŒå‘é‡æœºå¯»æ±‚çš„æ˜¯**æ€»ä½“æœ€ä¼˜æ€§**ï¼›  
 * å®ƒå‘é«˜ç»´æ•°æ®å˜æ¢è¿‡æ¸¡æ¯”è¾ƒæ–¹ä¾¿ï¼›  
 * åœ¨å¹³è¡¡åˆ†ç±»å™¨çš„å¤æ‚æ€§å’Œè¯¯å·®æ—¶ï¼Œèƒ½å¤Ÿå¾ˆç›´è§‚åœ°æ§åˆ¶ï¼›å¯ä»¥æŠŠè¯¸å¦‚å­—ç¬¦ä¸²é‚£æ ·çš„**éå‘é‡æ•°æ®**ä½œä¸ºè¾“å…¥ã€‚  
<br /> 
* ç¼ºç‚¹  
 * **å¯»æ‰¾åˆé€‚çš„æ ¸å‡½æ•°ï¼Œè°ƒæ•´æœ€ä¼˜çš„å‚æ•°($c$/$\gamma$ç­‰ï¼‰**è®¡ç®—é‡å¾ˆå¤§ã€‚  
 * å½“æ•°æ®ç»´åº¦på¤§äºæ ·æœ¬é‡næ—¶ï¼Œæ•ˆæœä¸å¥½ï¼Ÿï¼Ÿï¼Ÿ  
 * ä¸ç›´æ¥æä¾›probability estmationï¼Œéœ€è¦ç”¨K-fold cross validationæ¥è®¡ç®—ï¼Ÿ  

### çº¿æ€§åˆ†ç±»æƒ…å†µ

1. the SPLIT  
äºŒç»´å¹³é¢ä¸Šæœ‰ä¸¤ç§ä¸åŒçš„æ•°æ®ï¼ˆåœˆå’Œå‰ï¼‰ã€‚ç”±äºè¿™äº›æ•°æ®æ˜¯çº¿æ€§å¯åˆ†çš„ï¼Œæ‰€ä»¥å¯ä»¥ç”¨ä¸€æ¡ç›´çº¿å°†è¿™ä¸¤ç±»æ•°æ®åˆ†å¼€ï¼Œè¿™æ¡ç›´çº¿å°±ç›¸å½“äºä¸€ä¸ªè¶…å¹³é¢ï¼Œè¶…å¹³é¢ä¸€è¾¹çš„æ•°æ®ç‚¹æ‰€å¯¹åº”çš„yå…¨æ˜¯-1 ï¼Œå¦ä¸€è¾¹æ‰€å¯¹åº”çš„yå…¨æ˜¯1ã€‚  
è¿™ä¸ªè¶…å¹³é¢å¯ä»¥ç”¨åˆ†ç±»å‡½æ•°è¡¨ç¤º$f(x)=w^T \ast x+b$è¡¨ç¤º  
![svmç¤ºæ„](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/svm_pic01.png)  
åœ¨è¿›è¡Œåˆ†ç±»çš„æ—¶å€™ï¼Œé‡åˆ°ä¸€ä¸ªæ–°çš„æ•°æ®ç‚¹xï¼Œå°†xä»£å…¥f(x) ä¸­ï¼Œå¦‚æœf(x)å°äº0åˆ™å°†xçš„ç±»åˆ«èµ‹ä¸º-1ï¼Œå¦‚æœf(x)å¤§äº0åˆ™å°†xçš„ç±»åˆ«èµ‹ä¸º1ã€‚  
<br />
2. the BEST SPLIT  
å¯¹ä¸€ä¸ªæ•°æ®ç‚¹è¿›è¡Œåˆ†ç±»ï¼Œå½“è¶…å¹³é¢ç¦»æ•°æ®ç‚¹çš„â€œé—´éš”â€(margin)è¶Šå¤§ï¼Œåˆ†ç±»çš„ç¡®ä¿¡åº¦ï¼ˆconfidenceï¼‰ä¹Ÿè¶Šå¤§ã€‚æ‰€ä»¥ï¼Œä¸ºäº†ä½¿å¾—åˆ†ç±»çš„ç¡®ä¿¡åº¦å°½é‡é«˜ï¼Œéœ€è¦è®©æ‰€é€‰æ‹©çš„è¶…å¹³é¢èƒ½å¤Ÿæœ€å¤§åŒ–è¿™ä¸ªâ€œé—´éš”â€å€¼ã€‚è¿™ä¸ªé—´éš”å°±æ˜¯ä¸‹å›¾ä¸­çš„Gapçš„ä¸€åŠã€‚  
![svmç¤ºæ„2](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/svm_pic02.png)

SVMçš„ä½œç”¨æ˜¯åˆ†ç±»å™¨ï¼Œæœ¬è´¨æ˜¯æœ‰çº¦æŸçš„æœ€ä¼˜åŒ–é—®é¢˜(constrained optimization problemï¼‰  
1. æœ€ä¼˜åŒ– - è®©è·¯æœ€å®½  
$max \tilde{\gamma}$  
å¯ä»¥somehowè½¬åŒ–æˆ$max \frac{1}{||\omega||} $  
2. çº¦æŸæ¡ä»¶ - â€œç‚¹éƒ½ä¸åœ¨è·¯ä¸Šâ€  
$y_i(\omega^T x_i+b)>=1,  i=1,2,...,n$  
![svmç¤ºæ„3](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/svm_pic03.png)  
ä¸Šå›¾ä¸­ï¼Œä¸­é—´çš„å®çº¿ä¾¿æ˜¯å¯»æ‰¾åˆ°çš„æœ€ä¼˜è¶…å¹³é¢ï¼ˆOptimal Hyper Planeï¼‰ï¼Œå…¶åˆ°ä¸¤æ¡è™šçº¿è¾¹ç•Œçš„è·ç¦»ç›¸ç­‰ï¼Œè¿™ä¸ªè·ç¦»ä¾¿æ˜¯å‡ ä½•é—´éš”$\tilde{\gamma}$ï¼Œä¸¤æ¡è™šçº¿é—´éš”è¾¹ç•Œä¹‹é—´çš„è·ç¦»ç­‰äº$2 \tilde{\gamma}$ï¼Œè€Œ**è™šçº¿é—´éš”è¾¹ç•Œä¸Šçš„ç‚¹åˆ™æ˜¯æ”¯æŒå‘é‡**ã€‚ç”±äºè¿™äº›æ”¯æŒå‘é‡åˆšå¥½åœ¨è™šçº¿é—´éš”è¾¹ç•Œä¸Šï¼Œæ‰€ä»¥å®ƒä»¬æ»¡è¶³$y(w^T x+b)=1$ï¼ˆä¸ºæ–¹ä¾¿æ¨å¯¼å¯ä»¥ä»¤$\hat{\gamma}=1$ï¼‰ï¼Œè€Œå¯¹äºæ‰€æœ‰ä¸æ˜¯æ”¯æŒå‘é‡çš„ç‚¹ï¼Œåˆ™æ˜¾ç„¶æœ‰$y(w^T x+b)>1$ã€‚

### éçº¿æ€§æƒ…å†µ   
**æ ¸å‡½æ•°Kernel**  
* å¯¹äºéçº¿æ€§çš„æƒ…å†µï¼ŒSVM çš„å¤„ç†æ–¹æ³•æ˜¯é€‰æ‹©ä¸€ä¸ªæ ¸å‡½æ•°$K(Â·,Â·)$ ï¼Œé€šè¿‡å°†æ•°æ®æ˜ å°„åˆ°é«˜ç»´ç©ºé—´ï¼Œæ¥è§£å†³åœ¨åŸå§‹ç©ºé—´ä¸­çº¿æ€§ä¸å¯åˆ†çš„é—®é¢˜ã€‚  
* æ ¸å‡½æ•°å®šä¹‰ï¼šè®¡ç®—ä¸¤ä¸ªå‘é‡åœ¨éšå¼æ˜ å°„è¿‡åçš„ç©ºé—´ä¸­çš„å†…ç§¯çš„å‡½æ•°??  
* interestingly, åœ¨SVMä¸­éœ€è¦è®¡ç®—çš„åœ°æ–¹æ•°æ®å‘é‡æ€»æ˜¯ä»¥å†…ç§¯çš„å½¢å¼å‡ºç°çš„  

<br />
æ ¸å‡½æ•°çš„ä»·å€¼åœ¨äºå®ƒè™½ç„¶ä¹Ÿæ˜¯å°†ç‰¹å¾è¿›è¡Œä»ä½ç»´åˆ°é«˜ç»´çš„è½¬æ¢ï¼Œä½†å®ƒåªéœ€åœ¨ä½ç»´ä¸Šè¿›è¡Œè®¡ç®—ï¼Œå°±å¯å°†å®è´¨ä¸Šçš„åˆ†ç±»æ•ˆæœè¡¨ç°åœ¨äº†é«˜ç»´ä¸Šï¼Œä»è€Œé¿å…ç›´æ¥åœ¨é«˜ç»´ç©ºé—´ä¸­è¿›è¡Œå¤æ‚è®¡ç®—ã€‚  

<br />  
**æ ¸å‡½æ•°å½¢å¼**  
* çº¿æ€§æ ¸: $K(x,y)=x^T  y$  
* å¤šé¡¹å¼æ ¸: $K(x,y)=(x^T  y + 1)^{d}$  
* é«˜æ–¯æ ¸ ï¼ˆRadial basis function,RBFï¼‰: $K(x,y)=exp(-\frac{||x-y||^2}{2\sigma^2})$    
* Sigmoidæ ¸:  $K(x,y)=tanh(\kappa x^T  y + \theta)$  
   åŒæ›²æ­£åˆ‡å‡½æ•°$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$  

### outlierçš„å¤„ç† â€”â€” æ¾å¼›å˜é‡   
é€‚å½“å…è®¸æ•°æ®ç‚¹åç¦»è¶…å¹³é¢  
åŸæœ¬çš„çº¦æŸæ¡ä»¶$y_i(\omega^T x_i +b)>=1$æ”¹ä¸º$y_i(\omega^T x_i +b)>=1-\delta_i$  
å…¶ä¸­$\delta_i>0$æ˜¯æ¾å¼›å˜é‡(slack variable)  
å°†ç›®æ ‡å‡½æ•°$\min{\frac{1}{2}||\omega||^2}$æ”¹ä¸º$\min{\frac{1}{2}||\omega||^2+\sum_{i}^{n}C\ast\delta_i}$
 1. $C$ parameter - æƒ©ç½šé¡¹  
 ç”¨äºæ§åˆ¶ç›®æ ‡å‡½æ•°ä¸­ä¸¤é¡¹ï¼ˆâ€œå¯»æ‰¾ margin æœ€å¤§çš„è¶…å¹³é¢â€å’Œâ€œä¿è¯æ•°æ®ç‚¹åå·®é‡æœ€å°â€ï¼‰ä¹‹é—´çš„æƒé‡ã€‚  
 Cä½ï¼šæ¨¡å‹å®¹é”™ç‡é«˜ï¼Œç®€æ´  
 Cé«˜ï¼šæ¨¡å‹å®¹é”™ç‡ä½ï¼Œä½†å¯èƒ½è¿‡æ‹Ÿåˆï¼ˆå³å¯¹è®­ç»ƒé›†æ‹Ÿåˆæ•ˆæœå¥½ï¼Œå¯¹æµ‹è¯•é›†æœªå¿…å¥½ï¼‰  
 2. $\gamma$ parameter
 <br />  
å‚æ•°çš„è°ƒæ•´ï¼ˆparameter tuningï¼‰ - K-fold cross validation

### SVMçš„Rå®ç°  
e1071::svm()  
å‚æ•°è¯´æ˜ï¼š  
  
* Typeï¼šSVMçš„å½¢å¼ï¼Œé€‰é¡¹æœ‰ï¼šC-classificationï¼Œnu-classificationï¼Œone-classification (for novelty detection)ï¼Œeps-regressionï¼Œnu-regressionã€‚  
åé¢ä¸¤è€…ä¸ºåˆ©ç”¨SVMåšå›å½’æ—¶ç”¨ã€‚  
é»˜è®¤ä¸ºCåˆ†ç±»å™¨ï¼Œä½¿ç”¨nuåˆ†ç±»å™¨ä¼šä½¿å†³ç­–è¾¹ç•Œæ›´å…‰æ»‘ä¸€äº›ï¼Œå•ä¸€åˆ†ç±»é€‚ç”¨äºæ‰€æœ‰çš„è®­ç»ƒæ•°æ®æå–è‡ªåŒä¸€ä¸ªç±»é‡Œï¼Œç„¶åSVMå»ºç«‹äº†ä¸€ä¸ªåˆ†ç•Œçº¿ä»¥åˆ†å‰²è¯¥ç±»åœ¨ç‰¹å¾ç©ºé—´ä¸­æ‰€å åŒºåŸŸå’Œå…¶å®ƒç±»åœ¨ç‰¹å¾ç©ºé—´ä¸­æ‰€å åŒºåŸŸã€‚  
* Kernelï¼šåœ¨éçº¿æ€§å¯åˆ†æ—¶ï¼Œæˆ‘ä»¬å¼•å…¥æ ¸å‡½æ•°æ¥åšéçº¿æ€§å¯åˆ†ï¼Œ  
æ ¸å‡½æ•°çš„ä»·å€¼åœ¨äºå®ƒè™½ç„¶ä¹Ÿæ˜¯å°†ç‰¹å¾è¿›è¡Œä»ä½ç»´åˆ°é«˜ç»´çš„è½¬æ¢ï¼Œä½†å®ƒåªéœ€åœ¨ä½ç»´ä¸Šè¿›è¡Œè®¡ç®—ï¼Œå°±å¯å°†å®è´¨ä¸Šçš„åˆ†ç±»æ•ˆæœè¡¨ç°åœ¨äº†é«˜ç»´ä¸Šï¼Œä»è€Œé¿å…ç›´æ¥åœ¨é«˜ç»´ç©ºé—´ä¸­è¿›è¡Œå¤æ‚è®¡ç®—ã€‚   
>Ræä¾›çš„æ ¸ä»‹ç»å¦‚ä¸‹ï¼š  
 * çº¿æ€§æ ¸:$K(u,v)=u' \ast v$  
 * å¤šé¡¹å¼æ ¸:$K(u,v)=(\gamma \ast u' \ast v + coef0)^{degree}$  
 * é«˜æ–¯æ ¸ ï¼ˆ,RBFï¼‰:$K(u,v)=e^{-\gamma \ast |u-v|^2}$    
 * Sigmoidæ ¸: $K(u,v)=tanh(\gamma \ast u' \ast v + coef0)$  
é»˜è®¤ä¸ºé«˜æ–¯æ ¸ï¼ˆRBFï¼‰,<b>ä¼˜å…ˆå»ºè®®é«˜æ–¯æ ¸</b>   

<br />  
* Degree:å¤šé¡¹å¼æ ¸çš„æ¬¡æ•°ï¼Œé»˜è®¤ä¸º3  
* Gamma:é™¤å»çº¿æ€§æ ¸å¤–ï¼Œå…¶ä»–çš„æ ¸çš„å‚æ•°ï¼Œé»˜è®¤ä¸º1/æ•°æ®ç»´æ•°  
* Coef0,ï¼šå¤šé¡¹å¼æ ¸ä¸sigmoidæ ¸çš„å‚æ•°ï¼Œé»˜è®¤ä¸º0  
* Costï¼šCåˆ†ç±»çš„æƒ©ç½šé¡¹Cçš„å–å€¼  
* Nuï¼šnuåˆ†ç±»ï¼Œå•ä¸€åˆ†ç±»ä¸­nuçš„å–å€¼  
* Crossï¼šåšKæŠ˜äº¤å‰éªŒè¯ï¼Œè®¡ç®—åˆ†ç±»æ­£ç¡®æ€§ã€‚


```R
x=seq(-5,5,length.out=100)
dft=data.frame(x=x,y1=1/(1+exp(-x)),y2=1/(1+exp(-5*x)))
opt(3,3)
ggplot(dft)+
  theme_bw()+
  geom_line(aes(x,y1),col="blue")+
  geom_line(aes(x,y2),col="darkgreen")
```




![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_170_1.png)



```R
# e1071::svm
library(e1071)
data(iris)
iris.svm=svm(Species~., data=iris, method="C-classification",
            kernel="radial", cost=10, gamma=.1)
summary(iris.svm)
```

    Warning message:
    "package 'e1071' was built under R version 3.5.3"


    
    Call:
    svm(formula = Species ~ ., data = iris, method = "C-classification", 
        kernel = "radial", cost = 10, gamma = 0.1)
    
    
    Parameters:
       SVM-Type:  C-classification 
     SVM-Kernel:  radial 
           cost:  10 
    
    Number of Support Vectors:  32
    
     ( 3 16 13 )
    
    
    Number of Classes:  3 
    
    Levels: 
     setosa versicolor virginica
    
    
    



```R
iris.svm$SV                 #æŸ¥çœ‹æ”¯æŒå‘é‡çš„æƒ…å†µ
iris.svm$index           #æŸ¥çœ‹æ”¯æŒå‘é‡çš„æ ‡ç­¾
iris.svm$rho               #æŸ¥çœ‹åˆ†ç±»æ—¶çš„æˆªè·b
```


<table>
<thead><tr><th></th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th></tr></thead>
<tbody>
	<tr><th scope=row>16</th><td>-0.17309407</td><td> 3.08045544</td><td>-1.2791040 </td><td>-1.0486668 </td></tr>
	<tr><th scope=row>24</th><td>-0.89767388</td><td> 0.55674567</td><td>-1.1658087 </td><td>-0.9174741 </td></tr>
	<tr><th scope=row>42</th><td>-1.62225369</td><td>-1.73753594</td><td>-1.3923993 </td><td>-1.1798595 </td></tr>
	<tr><th scope=row>51</th><td> 1.39682886</td><td> 0.32731751</td><td> 0.5336209 </td><td> 0.2632600 </td></tr>
	<tr><th scope=row>53</th><td> 1.27606556</td><td> 0.09788935</td><td> 0.6469162 </td><td> 0.3944526 </td></tr>
	<tr><th scope=row>54</th><td>-0.41462067</td><td>-1.73753594</td><td> 0.1370873 </td><td> 0.1320673 </td></tr>
	<tr><th scope=row>55</th><td> 0.79301235</td><td>-0.59039513</td><td> 0.4769732 </td><td> 0.3944526 </td></tr>
	<tr><th scope=row>58</th><td>-1.13920048</td><td>-1.50810778</td><td>-0.2594462 </td><td>-0.2615107 </td></tr>
	<tr><th scope=row>67</th><td>-0.29385737</td><td>-0.13153881</td><td> 0.4203256 </td><td> 0.3944526 </td></tr>
	<tr><th scope=row>69</th><td> 0.43072244</td><td>-1.96696410</td><td> 0.4203256 </td><td> 0.3944526 </td></tr>
	<tr><th scope=row>71</th><td> 0.06843254</td><td> 0.32731751</td><td> 0.5902685 </td><td> 0.7880307 </td></tr>
	<tr><th scope=row>73</th><td> 0.55148575</td><td>-1.27867961</td><td> 0.6469162 </td><td> 0.3944526 </td></tr>
	<tr><th scope=row>77</th><td> 1.15530226</td><td>-0.59039513</td><td> 0.5902685 </td><td> 0.2632600 </td></tr>
	<tr><th scope=row>78</th><td> 1.03453895</td><td>-0.13153881</td><td> 0.7035638 </td><td> 0.6568380 </td></tr>
	<tr><th scope=row>79</th><td> 0.18919584</td><td>-0.36096697</td><td> 0.4203256 </td><td> 0.3944526 </td></tr>
	<tr><th scope=row>84</th><td> 0.18919584</td><td>-0.81982329</td><td> 0.7602115 </td><td> 0.5256453 </td></tr>
	<tr><th scope=row>85</th><td>-0.53538397</td><td>-0.13153881</td><td> 0.4203256 </td><td> 0.3944526 </td></tr>
	<tr><th scope=row>86</th><td> 0.18919584</td><td> 0.78617383</td><td> 0.4203256 </td><td> 0.5256453 </td></tr>
	<tr><th scope=row>99</th><td>-0.89767388</td><td>-1.27867961</td><td>-0.4293892 </td><td>-0.1303181 </td></tr>
	<tr><th scope=row>107</th><td>-1.13920048</td><td>-1.27867961</td><td> 0.4203256 </td><td> 0.6568380 </td></tr>
	<tr><th scope=row>111</th><td> 0.79301235</td><td> 0.32731751</td><td> 0.7602115 </td><td> 1.0504160 </td></tr>
	<tr><th scope=row>119</th><td> 2.24217198</td><td>-1.04925145</td><td> 1.7798692 </td><td> 1.4439941 </td></tr>
	<tr><th scope=row>120</th><td> 0.18919584</td><td>-1.96696410</td><td> 0.7035638 </td><td> 0.3944526 </td></tr>
	<tr><th scope=row>124</th><td> 0.55148575</td><td>-0.81982329</td><td> 0.6469162 </td><td> 0.7880307 </td></tr>
	<tr><th scope=row>127</th><td> 0.43072244</td><td>-0.59039513</td><td> 0.5902685 </td><td> 0.7880307 </td></tr>
	<tr><th scope=row>128</th><td> 0.30995914</td><td>-0.13153881</td><td> 0.6469162 </td><td> 0.7880307 </td></tr>
	<tr><th scope=row>130</th><td> 1.63835547</td><td>-0.13153881</td><td> 1.1567451 </td><td> 0.5256453 </td></tr>
	<tr><th scope=row>132</th><td> 2.48369858</td><td> 1.70388647</td><td> 1.4966310 </td><td> 1.0504160 </td></tr>
	<tr><th scope=row>134</th><td> 0.55148575</td><td>-0.59039513</td><td> 0.7602115 </td><td> 0.3944526 </td></tr>
	<tr><th scope=row>135</th><td> 0.30995914</td><td>-1.04925145</td><td> 1.0434497 </td><td> 0.2632600 </td></tr>
	<tr><th scope=row>139</th><td> 0.18919584</td><td>-0.13153881</td><td> 0.5902685 </td><td> 0.7880307 </td></tr>
	<tr><th scope=row>150</th><td> 0.06843254</td><td>-0.13153881</td><td> 0.7602115 </td><td> 0.7880307 </td></tr>
</tbody>
</table>




<ol class=list-inline>
	<li>16</li>
	<li>24</li>
	<li>42</li>
	<li>51</li>
	<li>53</li>
	<li>54</li>
	<li>55</li>
	<li>58</li>
	<li>67</li>
	<li>69</li>
	<li>71</li>
	<li>73</li>
	<li>77</li>
	<li>78</li>
	<li>79</li>
	<li>84</li>
	<li>85</li>
	<li>86</li>
	<li>99</li>
	<li>107</li>
	<li>111</li>
	<li>119</li>
	<li>120</li>
	<li>124</li>
	<li>127</li>
	<li>128</li>
	<li>130</li>
	<li>132</li>
	<li>134</li>
	<li>135</li>
	<li>139</li>
	<li>150</li>
</ol>




<ol class=list-inline>
	<li>-0.260498081180212</li>
	<li>0.0200653246232933</li>
	<li>-0.0872511895970034</li>
</ol>




```R
opt(6,4)
plot(iris.svm, iris, Petal.Width~Petal.Length, 
     slice=list(Sepal.Width=3,Sepal.Length = 4))
```


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_173_0.png)



```R
cf=coef(iris.svm);cf 
```


    Error in coef.svm(iris.svm): Only implemented for regression or binary classification with linear kernel.
    Traceback:
    

    1. coef(iris.svm)

    2. coef.svm(iris.svm)

    3. stop("Only implemented for regression or binary classification with linear kernel.")



    Error in eval(expr, envir, enclos): object 'cf' not found
    Traceback:
    


# ç¥ç»ç½‘ç»œ  
NNET is essentially about minimizing cost function  
<font color='#F8D86A'>å¤ä¹ è¦ç‚¹ï¼šBPç®—æ³•ï¼Œ**ä¼˜åŒ–æœºåˆ¶**ï¼Œ**PPTä¸­çš„æ–‡å­—/æ€æƒ³éƒ¨åˆ†** </font>

go to https://blog.csdn.net/illikang/article/details/82019945  
https://www.youtube.com/watch?v=aircAruvnKk  
è‡ªå·±å†™ç¥ç»ç½‘ç»œä»£ç ï¼Œå‚è€ƒ https://blog.csdn.net/kMD8d5R/article/details/85086412 ã€ä¸å®Œæ•´ã€‘

the simplest nnet mode - https://www.youtube.com/watch?v=aircAruvnKk  
example used throughout: hand-written digits recognition  
28*28 pixel grid  
   
>some basic ideas to keep in mind:  
1. è®¾è®¡ä¸€ä¸ªç¥ç»ç½‘ç»œæ—¶ï¼Œè¾“å…¥å±‚ä¸è¾“å‡ºå±‚çš„èŠ‚ç‚¹æ•°å¾€å¾€æ˜¯å›ºå®šçš„ï¼Œä¸­é—´å±‚åˆ™å¯ä»¥è‡ªç”±æŒ‡å®šï¼›
2. ç¥ç»ç½‘ç»œç»“æ„å›¾ä¸­çš„æ‹“æ‰‘ä¸ç®­å¤´ä»£è¡¨ç€é¢„æµ‹è¿‡ç¨‹æ—¶æ•°æ®çš„æµå‘ï¼Œè·Ÿè®­ç»ƒæ—¶çš„æ•°æ®æµæœ‰ä¸€å®šçš„åŒºåˆ«ï¼›
3. ç»“æ„å›¾é‡Œçš„å…³é”®ä¸æ˜¯åœ†åœˆï¼ˆä»£è¡¨â€œç¥ç»å…ƒâ€ï¼‰ï¼Œè€Œæ˜¯è¿æ¥çº¿ï¼ˆä»£è¡¨â€œç¥ç»å…ƒâ€ä¹‹é—´çš„è¿æ¥ï¼‰ã€‚æ¯ä¸ªè¿æ¥çº¿å¯¹åº”ä¸€ä¸ªä¸åŒçš„æƒé‡ï¼ˆå…¶å€¼ç§°ä¸ºæƒå€¼ï¼‰ï¼Œè¿™æ˜¯éœ€è¦è®­ç»ƒå¾—åˆ°çš„ã€‚ä¸€ä¸ªç¥ç»ç½‘ç»œçš„è®­ç»ƒç®—æ³•å°±æ˜¯**è®©æƒé‡çš„å€¼è°ƒæ•´åˆ°æœ€ä½³**ï¼Œä»¥ä½¿å¾—æ•´ä¸ªç½‘ç»œçš„é¢„æµ‹æ•ˆæœæœ€å¥½ã€‚  
4. é€šå¸¸æˆ‘ä»¬è¯´$k$å±‚ç¥ç»ç½‘ç»œï¼Œæ˜¯è¯´æœ‰å¤šå°‘ä¸ªè¿æ¥å±‚è€ŒéèŠ‚ç‚¹å±‚ã€‚  

![basic](./nnet_pic00.jpg)

## crash course on HISTORY of NNET  
### MPæ¨¡å‹/ç¥ç»å…ƒæ¨¡å‹  
ï¼ˆå› æå‡ºè€…ä¸ºå¿ƒç†å­¦å®¶McCullochå’Œæ•°å­¦å®¶Pittsè€Œå¾—åï¼Œå‚è€ƒç¥ç»å…ƒç»“æ„ï¼Œä¹Ÿå«ç¥ç»å…ƒæ¨¡å‹ï¼‰  
 MPæ¨¡å‹ä¸­ï¼Œæƒé‡çš„å€¼éƒ½æ˜¯é¢„å…ˆè®¾ç½®çš„ï¼Œå› æ­¤ä¸èƒ½å­¦ä¹ ã€‚  
 * åŒæ—¶æœŸè¿˜æœ‰Hebbæ¨¡å‹ï¼Œæå‡ºäº†è°ƒæ•´æƒé‡ä»¥ä½¿æœºå™¨å­¦ä¹ ï¼Œä¸ºå­¦ä¹ ç®—æ³•å¥ å®šåŸºç¡€ã€‚

### å•å±‚ç¥ç»ç½‘ç»œï¼ˆå•è¿æ¥å±‚ï¼Œä¸¤å±‚ç¥ç»å…ƒï¼Œæ„ŸçŸ¥å™¨æ¨¡å‹ï¼‰
 * é¦–ä¸ªå¯ä»¥å­¦ä¹ çš„äººå·¥ç¥ç»ç½‘ç»œ  
 ![å•å±‚](./nnet_pic02.jpg)  
 * ä»å‰ä¸€å±‚åˆ°åä¸€å±‚çš„è®¡ç®—å¯ä»¥ç”¨çŸ©é˜µè¡¨ç¤ºï¼š$g(W \cdot a) = z$  
 * ä¸ç¥ç»å…ƒæ¨¡å‹ä¸åŒï¼Œæ„ŸçŸ¥å™¨ä¸­çš„æƒå€¼æ˜¯é€šè¿‡è®­ç»ƒå¾—åˆ°çš„ã€‚æ„ŸçŸ¥å™¨ç±»ä¼¼ä¸€ä¸ªé€»è¾‘å›å½’æ¨¡å‹ï¼Œå¯ä»¥åšçº¿æ€§åˆ†ç±»ä»»åŠ¡ã€‚å†³ç­–åˆ†ç•Œå°±æ˜¯åœ¨äºŒç»´çš„æ•°æ®å¹³é¢ä¸­åˆ’å‡ºä¸€æ¡ç›´çº¿ï¼Œå½“æ•°æ®çš„ç»´åº¦æ˜¯3ç»´çš„æ—¶å€™ï¼Œå°±æ˜¯åˆ’å‡ºä¸€ä¸ªå¹³é¢ï¼Œå½“æ•°æ®çš„ç»´åº¦æ˜¯nç»´æ—¶ï¼Œå°±æ˜¯åˆ’å‡ºä¸€ä¸ªn-1ç»´çš„è¶…å¹³é¢ã€‚  
 * æ„ŸçŸ¥å™¨æ¨¡å‹åªèƒ½åšçº¿æ€§åˆ†ç±»ä»»åŠ¡ï¼Œå¯¹XORï¼ˆå¼‚æˆ–ï¼‰è¿™æ ·çš„ç®€å•åˆ†ç±»ä»»åŠ¡éƒ½æ— æ³•è§£å†³ã€‚

### ä¸¤å±‚ç¥ç»ç½‘ç»œï¼ˆå¤šå±‚æ„ŸçŸ¥å™¨ï¼‰  
 * 1986å¹´ï¼ŒRumelharå’ŒHintonç­‰äººæå‡ºäº†åå‘ä¼ æ’­ï¼ˆBackpropagationï¼ŒBPï¼‰ç®—æ³•ï¼Œè§£å†³äº†ä¸¤å±‚ç¥ç»ç½‘ç»œæ‰€éœ€è¦çš„å¤æ‚è®¡ç®—é‡é—®é¢˜ã€‚ã€BPç®—æ³•æ˜¯ä¸¤å±‚ç¥ç»ç½‘ç»œçš„è®¡ç®—æ€è·¯ã€‘  
![ä¸¤å±‚1](./nnet_pic03.jpg)   
![ä¸¤å±‚2](./nnet_pic04.jpg)  
 * è¿›ä¸€æ­¥è€ƒè™‘åç½®ï¼ˆbiasï¼‰ï¼ŒçŸ©é˜µè¡¨ç¤ºä¸ºï¼š  
$g(W(1) \cdot a(1)+b(1)) = a(2)$   
$g(W(2) \cdot a(2)+b(2)) = z$
 * ä¸¤å±‚ç¥ç»ç½‘ç»œä¸­çš„æ¿€æ´»å‡½æ•°(active function)$g()$ä¸å†æ˜¯$sgn$å‡½æ•°ï¼Œè€Œæ˜¯å¹³æ»‘å‡½æ•°$sigmoid$ã€‚  
>ç¥ç»ç½‘ç»œçš„æœ¬è´¨å°±æ˜¯é€šè¿‡å‚æ•°ä¸æ¿€æ´»å‡½æ•°æ¥æ‹Ÿåˆç‰¹å¾ä¸ç›®æ ‡ä¹‹é—´çš„çœŸå®å‡½æ•°å…³ç³»ï¼ˆè¯´ç™½äº†å°±æ˜¯å¤æ‚å‡½æ•°æ‹Ÿåˆï¼‰ã€‚åˆå­¦è€…å¯èƒ½è®¤ä¸ºç”»ç¥ç»ç½‘ç»œçš„ç»“æ„å›¾æ˜¯ä¸ºäº†åœ¨ç¨‹åºä¸­å®ç°è¿™äº›åœ†åœˆä¸çº¿ï¼Œä½†åœ¨ä¸€ä¸ªç¥ç»ç½‘ç»œçš„ç¨‹åºä¸­ï¼Œæ—¢æ²¡æœ‰â€œçº¿â€è¿™ä¸ªå¯¹è±¡ï¼Œä¹Ÿæ²¡æœ‰â€œå•å…ƒâ€è¿™ä¸ªå¯¹è±¡ã€‚å®ç°ä¸€ä¸ªç¥ç»ç½‘ç»œæœ€éœ€è¦çš„æ˜¯çº¿æ€§ä»£æ•°åº“ã€‚  
 * ä¸åªèƒ½åšçº¿æ€§åˆ†ç±»ä»»åŠ¡çš„å•å±‚ç¥ç»ç½‘ç»œä¸åŒï¼Œä¸¤å±‚ç¥ç»ç½‘ç»œå¯ä»¥æ— é™é€¼è¿‘ä»»æ„è¿ç»­å‡½æ•°ã€ä¸‡èƒ½è¿‘ä¼¼å®šç†ï¼ˆUniversal Approximation Theoremï¼‰ã€‘ï¼Œæ•…å¯ä»¥åšéçº¿æ€§åˆ†ç±»ä»»åŠ¡ã€‚ä¸¤å±‚ç¥ç»ç½‘ç»œå¯ä»¥åšéçº¿æ€§åˆ†ç±»çš„å…³é”®--éšè—å±‚ã€‚è”æƒ³åˆ°æˆ‘ä»¬ä¸€å¼€å§‹æ¨å¯¼å‡ºçš„çŸ©é˜µå…¬å¼ï¼Œæˆ‘ä»¬çŸ¥é“ï¼ŒçŸ©é˜µå’Œå‘é‡ç›¸ä¹˜ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯å¯¹å‘é‡çš„åæ ‡ç©ºé—´è¿›è¡Œä¸€ä¸ªå˜æ¢ã€‚å› æ­¤ï¼Œéšè—å±‚çš„å‚æ•°çŸ©é˜µçš„ä½œç”¨å°±æ˜¯ä½¿å¾—æ•°æ®çš„åŸå§‹åæ ‡ç©ºé—´ä»çº¿æ€§ä¸å¯åˆ†ï¼Œè½¬æ¢æˆäº†çº¿æ€§å¯åˆ†ã€‚  
>éšè—å±‚èŠ‚ç‚¹æ•°çš„è®¾ç½®  
ä¸­é—´å±‚çš„èŠ‚ç‚¹æ•°æ˜¯ç”±è®¾è®¡è€…æŒ‡å®šçš„ï¼ŒèŠ‚ç‚¹æ•°ä¼šå½±å“åˆ°æ•´ä¸ªæ¨¡å‹çš„æ•ˆæœã€‚ç›®å‰ä¸šç•Œæ²¡æœ‰å®Œå–„çš„ç†è®ºæ¥æŒ‡å¯¼èŠ‚ç‚¹æ•°çš„è®¾ç½®ã€‚ä¸€èˆ¬æ˜¯æ ¹æ®ç»éªŒæ¥è®¾ç½®ã€‚è¾ƒå¥½çš„æ–¹æ³•å°±æ˜¯é¢„å…ˆè®¾å®šå‡ ä¸ªå¯é€‰å€¼ï¼Œé€šè¿‡åˆ‡æ¢è¿™å‡ ä¸ªå€¼æ¥çœ‹æ•´ä¸ªæ¨¡å‹çš„é¢„æµ‹æ•ˆæœï¼Œé€‰æ‹©æ•ˆæœæœ€å¥½çš„å€¼ä½œä¸ºæœ€ç»ˆé€‰æ‹©ã€‚è¿™ç§æ–¹æ³•åˆå«åšGrid Searchï¼ˆç½‘æ ¼æœç´¢ï¼‰ã€‚


### å¤šå±‚ç¥ç»ç½‘ç»œ  
think about this - æ—¢ç„¶ä¸¤å±‚ç¥ç»ç½‘ç»œå°±èƒ½æ‹Ÿåˆä¸€åˆ‡â€¦ä¸ºä»€ä¹ˆè¿˜éœ€è¦å¤šå±‚ç¥ç»ç½‘ç»œï¼Ÿ


## Layers
First layer of network:
Neurons (ç¥ç»å…ƒ) -> sth. that hold a number

Last layer:
ten neuron each representing one digit(0-9)

two hidden layers 16 neurons each
randomly chosen?

activation of one layers determines the activation of the next 
(or, some groups of neurons firing cause others to fire?)

the brightest neuron in the output layers is the one we go with

examples : the third layers may be constituted by loops, lines(dif location/direction, dif length)
the second may be smaller parts that make up loops blablabla

## Parameters - How does one layer leads to another
each neurons in the next layer are connected to all the neurons in the previous layer  
activations from the 1st layers 784=28$\times$28 numbers (column $\alpha$)   
-> weighted sum (weights - matrix \omega) + bias   
-> activations from the 2nd layers  (also column, resulting from  $\omega \times \alpha$? ) (e.g. first layers are 784=28$\times$28 pixels then the weight is a 16*784 matrix?)  
* all 16 neurons in the 2nd layers have its set of weight and its bias  
  
>æ¿€æ´»å‡½æ•°the active function  
note that the weighted sum can be any real number  
but we want the index of "brightness" to be between 0 and 1  
so we need a function to squeeze the output i.e. the active function - ä½œç”¨ï¼šæŒ¤å‹/å‹ç¼© 
for example a **sigmoid function**   
 * $sigmoid(x)=\sigma(x)$ - å¤šç”¨äºä¸¤å±‚ç¥ç»ç½‘ç»œ
  * é€šå¼ï¼š$\sigma(x)=\frac{1}{1+e^{-\alpha x}}$ï¼Œå¯¼æ•° $\sigma \dot (x) =\frac{\alpha e^{-\alpha x}}{(1+e^{-\alpha x})^2}=\alpha \sigma(x) (1-\sigma(x))$
   * $\alpha$å†³å®šSå‹æ›²çº¿çš„é™¡å³­ç¨‹åº¦
  * $\sigma(x)=\frac{1}{1+e^{-x}}$ å…¶å¯¼æ•°$\sigma \dot (x) = \frac{e^{-x}}{(1+e^{-x})^2}=\sigma(x) (1-\sigma(x))$
  * $\sigma(x)=\frac{e^x}{e^x+e^{-x}}=\frac{1}{1+e^{-2x}}$
 * ReLU - å¤šç”¨äºå¤šå±‚ç¥ç»ç½‘ç»œ
 * Signal - å¤šç”¨äºå•å±‚ç¥ç»ç½‘ç»œ
 * åˆ†æ®µçº¿æ€§å‡½æ•°  
æ¿€æ´»å‡½æ•°é€šå¸¸è¿ç»­ä¸”å¯å¯¼  
æ¿€æ´»å‡½æ•°çš„å¯¼æ•°éå¸¸å…³é”®ï¼è¦å¥½ç®—ï¼  

* bias can be introduced to the sum weighted before entering sigmoid    
* weights and bias  -> unknown parameters to be determined   
  
* we can look at what each layer does by visualizing the weight (not the weight matrix, but each 1*781 row of the matrix, and reshape it into a 28$\times$28 small matrix) ( to see what importance and what directions it gives to each neurons in the previous layer)  

![connections](./nnet_pic01.jpg)  
æˆ‘ä»¬ä½¿ç”¨aæ¥è¡¨ç¤ºè¾“å…¥ï¼Œç”¨wæ¥è¡¨ç¤ºæƒå€¼ã€‚ä¸€ä¸ªè¡¨ç¤ºè¿æ¥çš„æœ‰å‘ç®­å¤´å¯ä»¥è¿™æ ·ç†è§£ï¼šåœ¨åˆç«¯ï¼Œä¼ é€’çš„ä¿¡å·å¤§å°ä»ç„¶æ˜¯aï¼Œç«¯ä¸­é—´æœ‰åŠ æƒå‚æ•°wï¼Œç»è¿‡è¿™ä¸ªåŠ æƒåçš„ä¿¡å·ä¼šå˜æˆa$\times$wï¼Œå› æ­¤åœ¨è¿æ¥çš„æœ«ç«¯ï¼Œä¿¡å·çš„å¤§å°å°±å˜æˆäº†a$\times$wã€‚  
zåœ¨è¾“å…¥å’Œæƒå€¼çš„çº¿æ€§åŠ æƒå’Œï¼ˆ$\sum(a\cdot w)$ï¼‰å åŠ äº†ä¸€ä¸ªæ¿€æ´»å‡½æ•°$g()$çš„å€¼ã€‚åœ¨MPæ¨¡å‹é‡Œï¼Œæ¿€æ´»å‡½æ•°$g()$æ˜¯$sgn$å‡½æ•°ï¼Œæ›´å¸¸ç”¨çš„æ˜¯$sigmoid$å’Œ$ReLU$ç­‰  

## how to achieve that throught an algorithm?
we start all those unknown parameters i.e. weights & bias **randomly**  
so the initial performance would be terrible  
  
>COST FUNCTION  (input: all unknown para including weights and bias, output: how bad this model is)  
  æŸå¤±å‡½æ•°$E(w)=1/2 (T_i-o_i )^2$ï¼š  
  å…¶ä¸­ $T$ï¼ˆTarget,ç›®æ ‡å€¼å³çœŸå®å€¼ï¼‰ ï¼Œ$O$ï¼ˆOutput,æ¨¡å‹è¾“å‡ºï¼‰  
 * the function value will be small if output is correct  
 * would be large if output is trash  
 * average cost(over all sample data) captures how lousy the network is for the full panel of sample  

  
so the prob would then be how to minimize the cost function - å¦‚ä½•ä¼˜åŒ–å‚æ•°ï¼Œèƒ½å¤Ÿè®©æŸå¤±å‡½æ•°çš„å€¼æœ€å°ã€‚  
one possible solution - > **gradient descent æ¢¯åº¦ä¸‹é™**  
  
* å¯¼æ•°tell us which changes in parameters ( or changing which para) matter most  

## interesting stuffs (from the landscape literature)
using structured data to train a network allows one to achieve local minima much more quickly(quick drop in accuracy)  
using a randomly labeled data on the otherhand does not  


## BPç¥ç»ç½‘ç»œ

<font size=3>* ç‰¹ç‚¹ï¼šä¸¤å±‚æ¿€æ´»ï¼Œä»åå‘å‰ï¼ˆå…ˆC-Bå†B-Aï¼‰  
![BP](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/nnet_pic05.png)    
  
* A è¾“å…¥å±‚ï¼Œä¸‹æ ‡i  
 *  a1 è¾“å…¥ï¼š$x_i$  
 *  a2 è¾“å‡ºï¼š\  
  
* B éšè—å±‚ï¼Œä¸‹æ ‡h  
 *  b1 è¾“å…¥ï¼šçº¿æ€§ç»„åˆ$z_h=\sum_{i}(w_{ih} \cdot x_i)$  
 *  b2 è¾“å‡ºï¼š$o_h=\sigma_h (Z_h+\alpha_h)$ ï¼ˆçº¿æ€§ç»„åˆ+åç§»ï¼‰  
	ä¹Ÿå¯ä»¥ç”¨$\phi()$æˆ–$g()$è¡¨ç¤ºæ¿€æ´»å‡½æ•°ï¼Œbut usl. **sigmoidå‡½æ•°**   
      æ•…BPä¸­ç¬¦å·ç›´æ¥æ˜¯$\sigma()$  
    
* C è¾“å‡ºå±‚ï¼Œä¸‹æ ‡k  
 *  c1 è¾“å…¥ï¼šçº¿æ€§ç»„åˆ$y_k=\sum_{h}(w_{hk} \cdot o_h )$  
 *  c2 è¾“å‡ºï¼šsigmoidæ¿€æ´»å‡½æ•°ï¼ˆçº¿æ€§ç»„åˆ+åç§»ï¼‰$o_k^`=\sigma_k (y_k+\alpha_k)$  
</font>

Rå®ç°ï¼šnnet  
ç»˜å›¾ï¼šneural net tools (actually I haven't figure out how to use this package)  
plot.nnet (as used below)


```R
data("iris")
n=nrow(iris)
loc=sample(n,n*.7)
train=iris[loc,]
test=iris[-loc,]
head(train)
```


<table>
<thead><tr><th></th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th></tr></thead>
<tbody>
	<tr><th scope=row>40</th><td>5.1       </td><td>3.4       </td><td>1.5       </td><td>0.2       </td><td>setosa    </td></tr>
	<tr><th scope=row>42</th><td>4.5       </td><td>2.3       </td><td>1.3       </td><td>0.3       </td><td>setosa    </td></tr>
	<tr><th scope=row>62</th><td>5.9       </td><td>3.0       </td><td>4.2       </td><td>1.5       </td><td>versicolor</td></tr>
	<tr><th scope=row>13</th><td>4.8       </td><td>3.0       </td><td>1.4       </td><td>0.1       </td><td>setosa    </td></tr>
	<tr><th scope=row>56</th><td>5.7       </td><td>2.8       </td><td>4.5       </td><td>1.3       </td><td>versicolor</td></tr>
	<tr><th scope=row>80</th><td>5.7       </td><td>2.6       </td><td>3.5       </td><td>1.0       </td><td>versicolor</td></tr>
</tbody>
</table>




```R
library(nnet)
iris.nn=nnet(Species~.,data=train,
  size=2,rang=.1,decay=5e-4,maxit=200)
  #size: éšè—å±‚èŠ‚ç‚¹ä¸ªæ•°ï¼Œä½†åªèƒ½æœ‰ä¸€ä¸ªéšè—å±‚ï¼Œè¿™æ˜¯nnetåŒ…çš„ä¸€å¤§ç¼ºé™·
  #rang: initial random weights
  #decay: é»˜è®¤å€¼0ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
summary(iris.nn)
pred=predict(iris.nn,test,type="class")
```

    # weights:  19
    initial  value 115.418251 
    iter  10 value 50.157787
    iter  20 value 47.436806
    iter  30 value 43.476192
    iter  40 value 23.077417
    iter  50 value 7.812110
    iter  60 value 5.625027
    iter  70 value 5.281502
    iter  80 value 5.042058
    iter  90 value 4.491077
    iter 100 value 3.357326
    iter 110 value 2.698912
    iter 120 value 2.446648
    iter 130 value 2.302550
    iter 140 value 2.206402
    iter 150 value 2.168187
    iter 160 value 2.142581
    iter 170 value 2.137935
    iter 180 value 2.134625
    iter 190 value 2.133921
    iter 200 value 2.133671
    final  value 2.133671 
    stopped after 200 iterations
    


    a 4-2-3 network with 19 weights
    options were - softmax modelling  decay=5e-04
     b->h1 i1->h1 i2->h1 i3->h1 i4->h1 
     -0.27  -0.74  -1.79   2.99   1.62 
     b->h2 i1->h2 i2->h2 i3->h2 i4->h2 
     27.96   1.20   4.29  -5.06 -13.10 
     b->o1 h1->o1 h2->o1 
      4.60 -11.94   7.85 
     b->o2 h1->o2 h2->o2 
    -10.85   5.13  15.39 
     b->o3 h1->o3 h2->o3 
      6.26   6.80 -23.24 



```R
tab=table(test$Species, pred, dnn=c("Target","Output"))
tab
```


                Output
    Target       setosa versicolor virginica
      setosa         12          0         0
      versicolor      0         14         0
      virginica       0          2        17



```R
# plot.nnet from github
# https://gist.githubusercontent.com/fawda123/5086859/raw/17fd6d2adec4dbcf5ce750cbd1f3e0f4be9d8b19/nnet_plot_fun.r
plot.nnet=function(mod.in,nid=T,all.out=T,all.in=T,wts.only=F,rel.rsc=5,circle.cex=5,node.labs=T,
  line.stag=NULL,cex.val=1,alpha.val=1,circle.col='lightgrey',pos.col='black',neg.col='grey',...){

  require(scales)
	
  #gets weights for neural network, output is list
  #if rescaled argument is true, weights are returned but rescaled based on abs value
  nnet.vals=function(mod.in,nid,rel.rsc){

    library(scales)
    layers=mod.in$n
    wts=mod.in$wts
    if(nid) wts=rescale(abs(wts),c(1,rel.rsc))

    indices=matrix(seq(1,layers[1]*layers[2]+layers[2]),ncol=layers[2])
    out.ls=list()
    for(i in 1:ncol(indices)){
      out.ls[[paste('hidden',i)]]=wts[indices[,i]]
      }

    if(layers[3]==1) out.ls[['out 1']]=wts[(max(indices)+1):length(wts)]
    else{
      out.indices=matrix(seq(max(indices)+1,length(wts)),ncol=layers[3])
      for(i in 1:ncol(out.indices)){
        out.ls[[paste('out',i)]]=wts[out.indices[,i]]
        }
      }

    out.ls

    }

  wts=nnet.vals(mod.in,nid=F)

  if(wts.only) return(wts)

  #par(mar=numeric(4),oma=numeric(4),family='serif')
  library(scales)
  struct=mod.in$n
  x.range=c(0,100)
  y.range=c(0,100)
  #these are all proportions from 0-1
	if(is.null(line.stag)) line.stag=0.011*circle.cex/2
  layer.x=seq(0.17,0.9,length=3)
  bias.x=c(mean(layer.x[1:2]),mean(layer.x[2:3]))
  bias.y=0.95
	in.col=bord.col=circle.col
  circle.cex=circle.cex
	
  #get variable names from nnet object
	if(is.null(mod.in$call$formula)){
		x.names=colnames(eval(mod.in$call$x))
		y.names=colnames(eval(mod.in$call$y))
		}
	else{
		forms=eval(mod.in$call$formula)
		dat.names=model.frame(forms,data=eval(mod.in$call$data))
		y.names=as.character(forms)[2]
		x.names=names(dat.names)[!names(dat.names) %in% y.names]
		}
	
	#initiate plot
  plot(x.range,y.range,type='n',axes=F,ylab='',xlab='',...)

  #function for getting y locations for input, hidden, output layers
  #input is integer value from 'struct'
  get.ys=function(lyr){
    spacing=diff(c(0*diff(y.range),0.9*diff(y.range)))/max(struct)
    seq(0.5*(diff(y.range)+spacing*(lyr-1)),0.5*(diff(y.range)-spacing*(lyr-1)),
      length=lyr)
    }

  #function for plotting nodes
  #'layer' specifies which layer, integer from 'struct'
  #'x.loc' indicates x location for layer, integer from 'layer.x'
  #'layer.name' is string indicating text to put in node
  layer.points=function(layer,x.loc,layer.name,cex=cex.val){
		x=rep(x.loc*diff(x.range),layer)
    y=get.ys(layer)
    points(x,y,pch=21,cex=circle.cex,col=in.col,bg=bord.col)
    if(node.labs) text(x,y,paste(layer.name,1:layer,sep=''),cex=cex.val)
    if(layer.name=='I' & node.labs){
      text(x-line.stag*diff(x.range),y,x.names,pos=2,cex=cex.val)
    	}	
    if(layer.name=='O' & node.labs)
      text(x+line.stag*diff(x.range),y,y.names,pos=4,cex=cex.val)
    }

  #function for plotting bias points
  #'bias.x' is vector of values for x locations
  #'bias.y' is vector for y location
  #'layer.name' is  string indicating text to put in node
  bias.points=function(bias.x,bias.y,layer.name,cex,...){
    for(val in 1:length(bias.x)){
      points(
        diff(x.range)*bias.x[val],
        bias.y*diff(y.range),
        pch=21,col=in.col,bg=bord.col,cex=circle.cex
        )
      if(node.labs)
        text(
          diff(x.range)*bias.x[val],
          bias.y*diff(y.range),
          paste(layer.name,val,sep=''),
          cex=cex.val
        )
      }
    }

  #function creates lines colored by direction and width as proportion of magnitude
  #use 'all.in' argument if you want to plot connection lines for only a single input node
  layer.lines=function(mod.in,h.layer,layer1=1,layer2=2,out.layer=F,nid,rel.rsc,all.in,pos.col,
  	neg.col,...){

    x0=rep(layer.x[layer1]*diff(x.range)+line.stag*diff(x.range),struct[layer1])
    x1=rep(layer.x[layer2]*diff(x.range)-line.stag*diff(x.range),struct[layer1])

    if(out.layer==T){

      y0=get.ys(struct[layer1])
      y1=rep(get.ys(struct[layer2])[h.layer],struct[layer1])
      src.str=paste('out',h.layer)

      wts=nnet.vals(mod.in,nid=F,rel.rsc)
      wts=wts[grep(src.str,names(wts))][[1]][-1]
      wts.rs=nnet.vals(mod.in,nid=T,rel.rsc)
      wts.rs=wts.rs[grep(src.str,names(wts.rs))][[1]][-1]

      cols=rep(pos.col,struct[layer1])
      cols[wts<0]=neg.col

      if(nid) segments(x0,y0,x1,y1,col=cols,lwd=wts.rs)
      else segments(x0,y0,x1,y1)

      }
      
    else{

      if(is.logical(all.in)) all.in=h.layer
      else all.in=which(x.names==all.in)

      y0=rep(get.ys(struct[layer1])[all.in],struct[2])
      y1=get.ys(struct[layer2])
      src.str='hidden'

      wts=nnet.vals(mod.in,nid=F,rel.rsc)
      wts=unlist(lapply(wts[grep(src.str,names(wts))],function(x) x[all.in+1]))
      wts.rs=nnet.vals(mod.in,nid=T,rel.rsc)
      wts.rs=unlist(lapply(wts.rs[grep(src.str,names(wts.rs))],function(x) x[all.in+1]))

      cols=rep(pos.col,struct[layer2])
      cols[wts<0]=neg.col

      if(nid) segments(x0,y0,x1,y1,col=cols,lwd=wts.rs)
      else segments(x0,y0,x1,y1)

      }

    }

  bias.lines=function(bias.x,mod.in,nid,rel.rsc,all.out,pos.col,neg.col,...){

    if(is.logical(all.out)) all.out=1:struct[3]
    else all.out=which(y.names==all.out)
    
    for(val in 1:length(bias.x)){

      wts=nnet.vals(mod.in,nid=F,rel.rsc)
      wts.rs=nnet.vals(mod.in,nid=T,rel.rsc)

      if(val==1){
        wts=wts[grep('out',names(wts),invert=T)]
        wts.rs=wts.rs[grep('out',names(wts.rs),invert=T)]
        }

      if(val==2){
        wts=wts[grep('out',names(wts))]
        wts.rs=wts.rs[grep('out',names(wts.rs))]
        }

      cols=rep(pos.col,length(wts))
      cols[unlist(lapply(wts,function(x) x[1]))<0]=neg.col
      wts.rs=unlist(lapply(wts.rs,function(x) x[1]))

      if(nid==F){
        wts.rs=rep(1,struct[val+1])
        cols=rep('black',struct[val+1])
        }

      if(val==1){
        segments(
          rep(diff(x.range)*bias.x[val]+diff(x.range)*line.stag,struct[val+1]),
          rep(bias.y*diff(y.range),struct[val+1]),
          rep(diff(x.range)*layer.x[val+1]-diff(x.range)*line.stag,struct[val+1]),
          get.ys(struct[val+1]),
          lwd=wts.rs,
          col=cols
          )
        }

      if(val==2){
        segments(
          rep(diff(x.range)*bias.x[val]+diff(x.range)*line.stag,struct[val+1]),
          rep(bias.y*diff(y.range),struct[val+1]),
          rep(diff(x.range)*layer.x[val+1]-diff(x.range)*line.stag,struct[val+1]),
          get.ys(struct[val+1])[all.out],
          lwd=wts.rs[all.out],
          col=cols[all.out]
          )
        }

      }
    }

  #use functions to plot connections between layers
  #bias lines
  bias.lines(bias.x,mod.in,nid=nid,rel.rsc=rel.rsc,all.out=all.out,pos.col=alpha(pos.col,alpha.val),
  	neg.col=alpha(neg.col,alpha.val))

  #layer lines, makes use of arguments to plot all or for individual layers
  #starts with input-hidden
  #uses 'all.in' argument to plot connection lines for all input nodes or a single node
  if(is.logical(all.in)){
    mapply(
      function(x) layer.lines(mod.in,x,layer1=1,layer2=2,nid=nid,rel.rsc=rel.rsc,all.in=all.in,
      	pos.col=alpha(pos.col,alpha.val),neg.col=alpha(neg.col,alpha.val)),
      1:struct[1]
      )
    }
  else{
    node.in=which(x.names==all.in)
    layer.lines(mod.in,node.in,layer1=1,layer2=2,nid=nid,rel.rsc=rel.rsc,all.in=all.in,
    	pos.col=alpha(pos.col,alpha.val),neg.col=alpha(neg.col,alpha.val))
    }

  #lines for hidden-output
  #uses 'all.out' argument to plot connection lines for all output nodes or a single node
  if(is.logical(all.out))
    mapply(
      function(x) layer.lines(mod.in,x,layer1=2,layer2=3,out.layer=T,nid=nid,rel.rsc=rel.rsc,
        all.in=all.in,pos.col=alpha(pos.col,alpha.val),neg.col=alpha(neg.col,alpha.val)),
      1:struct[3]
      )
  else{
    all.out=which(y.names==all.out)
    layer.lines(mod.in,all.out,layer1=2,layer2=3,out.layer=T,nid=nid,rel.rsc=rel.rsc,
    	pos.col=pos.col,neg.col=neg.col)
    }

  #use functions to plot nodes
  layer.points(struct[1],layer.x[1],'I')
  layer.points(struct[2],layer.x[2],'H')
  layer.points(struct[3],layer.x[3],'O')
  bias.points(bias.x,bias.y,'B')

  }
```


```R
plot.nnet(iris.nn,cex=.7)
```


![png](https://github.com/yanyul987/yanyul987.github.io/tree/master/_posts/image/2019-6-22-DataMining/output_193_0.png)


è®¾ç½®æ›´å¤šå‚æ•°  
è§https://www.r-bloggers.com/visualizing-neural-networks-from-the-nnet-package/  
![table](./nnet_plot_pic01.png)

## å·ç§¯ç¥ç»ç½‘ç»œ
