---
published: true
title: æ•°æ®æŒ–æ˜æœŸæœ«å¤ä¹ æ±‡æ€»ï¼ˆå››ï¼‰â€”â€”æ ‘
category: Data Mining
tags: 
  - DataMining
  - Tree
layout: post
excerpt_separator: <!--more-->

---

# æ ‘

## Intuition  
![tree](/assets/images/201906DataMining/tree_pic01.png)

## å•æ£µæ ‘

### Information Gain  
>def: æ‹†åˆ†å‰å’Œæ‹†åˆ†åçš„ç†µçš„å·®å€¼  
expected entropy drop after split  

$I=-(p_1 \log_2{p_1}+ ... +p_k \log_2{p_k})=-\sum_{i=1}^k p_i \log_2{p_i}$  
$Gain(X)=I(n,n_1)-E(X)$  
å…¶ä¸­$I(n,n_1)=-(p \log_2{p}+(1-p) \log_2{(1-p)})$ï¼Œ$p=\frac{n_1}{n}$  
$E(X)=\frac{m_1}{n}\ast I(m_1,m_{11})+\frac{m_2}{n}\ast I(m_2,m_{21})+ ... + \frac{m_k}{n}\ast I(m_k,m_{k1})$  

<!--more-->
  
<br />  
* An Example  
![infogain](/assets/images/201906DataMining/information_gain.jpg)  
**æ‹†åˆ†å‰**çš„ç†µå€¼ï¼ˆä¸æŒ‰å¹´é¾„åŒºåˆ†æ—¶ï¼Œ16ä¸ªæ ·æœ¬ï¼Œ4æœ‰12æ— ï¼‰--$I(16,4)$  
$I(16,4)=-(4/16 * log_2{4/16}+12/16 * log_2{12/16})=0.8113$  
**æ‹†åˆ†å**çš„ç†µå€¼ï¼ˆæŒ‰å¹´é¾„åŒºåˆ†ï¼Œ6/16åœ¨ç¬¬ä¸€ç»„ï¼Œ10/16åœ¨ç¬¬äºŒç»„ï¼Œç¬¬ä¸€ç»„1æœ‰5æ— ï¼Œç¬¬äºŒç»„3æœ‰10æ— ï¼‰  
$E(å¹´é¾„)=(6/16)*I(6,1)+(10/16)*I(10,3)=0.7946$  
$Gain(å¹´é¾„)=I(16,4)-E(å¹´é¾„)=0.0167$  
<font color='#9a4238'>ä¿¡æ¯å¢ç›Šè¶Šå¤§è¶Šå¥½ **ç­‰ä»·äº** æ‹†åˆ†åçš„ç†µå€¼è¶Šå°è¶Šå¥½ï¼ˆæ‹†åˆ†å‰çš„ç†µå€¼å¯¹äºæ‰€æœ‰ä¸‹çº§èŠ‚ç‚¹è€Œè¨€éƒ½ä¸€æ ·ï¼‰</font>  


```R
I=function(n,a){
    out=-((a/n)*log(a/n)/log(2)+
         (1-a/n)*log(1-a/n)/log(2))
}
# I(16,4)
a=I(16,4);a
# E(å¹´é¾„)
b=(6/16)*I(6,1)+(10/16)*I(10,3);b
# Gain(å¹´é¾„)
gain1=a-b;gain1
```


0.811278124459133



0.794565220137316



0.0167129043218172



```R
gain2=I(16,4)-9/16*I(9,1)-7/16*I(7,3);gain2
gain3=I(16,4)-6/16*I(6,1)-7/16*I(7,2)-3/16*I(3,1);gain3
```


0.0971580016328469



0.0177239987078822


### Gini Index  


* the same example using Gini Index  
![info_gain](/assets/images/201906DataMining/information_gain.jpg)  

$Gini(split_j)=weighted\, sum\,  of\,  Gini(node)=\frac{n_1}{n} Gini(n_1,n_{11})+ ... +\frac{n_k}{n} Gini(n_k,n_{k1})$  
å…¶ä¸­$Gini(n,a)=1-(\frac{a}{n})^2-(1-\frac{a}{n})^2$  
>worst case: $Gini(n,\frac{n}{2})=1-1/4-1/4=1/2$  
best case: $Gini(n,n)=Gini(n,0)=1-1-0=0$  
  
<font color='#9a4238'>$Gini$ç³»æ•°è¶Šå°è¶Šå¥½</font>  


```R
gini=function(n,a){
    out=1-(a/n)^2-(1-a/n)^2;
}
gini1=6/16*gini(6,1)+10/16*gini(10,3);gini1
gini2=9/16*gini(9,1)+7/16*gini(7,3);gini2
gini3=6/16*gini(6,1)+7/16*gini(7,2)+3/16*gini(3,1);gini3
```


0.366666666666667



0.325396825396825



0.366071428571429



```R
rm(list=ls())
library(rpart)
library(rpart.plot)

data(iris)
n=nrow(iris)
loc=sample(n,.7*n)
train=iris[loc,]
test=iris[-loc,]
head(train)
```


<table>
<thead><tr><th></th><th scope="col">Sepal.Length</th><th scope="col">Sepal.Width</th><th scope="col">Petal.Length</th><th scope="col">Petal.Width</th><th scope="col">Species</th></tr></thead>
<tbody>
	<tr><th scope="row">9</th><td>4.4       </td><td>2.9       </td><td>1.4       </td><td>0.2       </td><td>setosa    </td></tr>
	<tr><th scope="row">43</th><td>4.4       </td><td>3.2       </td><td>1.3       </td><td>0.2       </td><td>setosa    </td></tr>
	<tr><th scope="row">77</th><td>6.8       </td><td>2.8       </td><td>4.8       </td><td>1.4       </td><td>versicolor</td></tr>
	<tr><th scope="row">44</th><td>5.0       </td><td>3.5       </td><td>1.6       </td><td>0.6       </td><td>setosa    </td></tr>
	<tr><th scope="row">105</th><td>6.5       </td><td>3.0       </td><td>5.8       </td><td>2.2       </td><td>virginica </td></tr>
	<tr><th scope="row">87</th><td>6.7       </td><td>3.1       </td><td>4.7       </td><td>1.5       </td><td>versicolor</td></tr>
</tbody>
</table>



### ç”Ÿæˆ


```R
model1=rpart(Species~., data=train, control=rpart.control(cp=.05))
summary(model1)
```

    Call:
    rpart(formula = Species ~ ., data = train, control = rpart.control(cp = 0.05))
      n= 105 
    
             CP nsplit  rel error    xerror       xstd
    1 0.5294118      0 1.00000000 1.0882353 0.06873741
    2 0.4117647      1 0.47058824 0.5588235 0.07241452
    3 0.0500000      2 0.05882353 0.1323529 0.04218454
    
    Variable importance
     Petal.Width Petal.Length Sepal.Length  Sepal.Width 
              35           30           19           16 
    
    Node number 1: 105 observations,    complexity param=0.5294118
      predicted class=setosa      expected loss=0.647619  P(node) =1
        class counts:    37    36    32
       probabilities: 0.352 0.343 0.305 
      left son=2 (37 obs) right son=3 (68 obs)
      Primary splits:
          Petal.Length < 2.6  to the left,  improve=35.98431, (0 missing)
          Petal.Width  < 0.8  to the left,  improve=35.98431, (0 missing)
          Sepal.Length < 5.45 to the left,  improve=22.82005, (0 missing)
          Sepal.Width  < 3.15 to the right, improve=17.26667, (0 missing)
      Surrogate splits:
          Petal.Width  < 0.8  to the left,  agree=1.000, adj=1.000, (0 split)
          Sepal.Length < 5.45 to the left,  agree=0.905, adj=0.730, (0 split)
          Sepal.Width  < 3.15 to the right, agree=0.848, adj=0.568, (0 split)
    
    Node number 2: 37 observations
      predicted class=setosa      expected loss=0  P(node) =0.352381
        class counts:    37     0     0
       probabilities: 1.000 0.000 0.000 
    
    Node number 3: 68 observations,    complexity param=0.4117647
      predicted class=versicolor  expected loss=0.4705882  P(node) =0.647619
        class counts:     0    36    32
       probabilities: 0.000 0.529 0.471 
      left son=6 (40 obs) right son=7 (28 obs)
      Primary splits:
          Petal.Width  < 1.65 to the left,  improve=26.682350, (0 missing)
          Petal.Length < 4.75 to the left,  improve=24.857240, (0 missing)
          Sepal.Length < 5.75 to the left,  improve= 7.686275, (0 missing)
          Sepal.Width  < 3.15 to the left,  improve= 4.374156, (0 missing)
      Surrogate splits:
          Petal.Length < 5.05 to the left,  agree=0.882, adj=0.714, (0 split)
          Sepal.Length < 6.15 to the left,  agree=0.721, adj=0.321, (0 split)
          Sepal.Width  < 2.95 to the left,  agree=0.706, adj=0.286, (0 split)
    
    Node number 6: 40 observations
      predicted class=versicolor  expected loss=0.1  P(node) =0.3809524
        class counts:     0    36     4
       probabilities: 0.000 0.900 0.100 
    
    Node number 7: 28 observations
      predicted class=virginica   expected loss=0  P(node) =0.2666667
        class counts:     0     0    28
       probabilities: 0.000 0.000 1.000 
    
    


```R
# è¾“å‡ºèŠ‚ç‚¹ï¼ˆæ–‡å­—ï¼‰
path.rpart(model1,node=c(1,2,3))

# ç»˜å›¾æ³•1
opt(6,4)
rpart.plot(model1,cex=.8)

# ç»˜å›¾æ³•2
if (FALSE){
    plot(model1,asp=4,compress=T)
    text(model1,cex=1,use.n=T)
}
```

    
     node number: 1 
       root
    
     node number: 2 
       root
       Petal.Length< 2.6
    
     node number: 3 
       root
       Petal.Length>=2.6
    


![png](/assets/images/201906DataMining/output_126_1.png)


### å‰ªæ

#### é¢„å‰ªæ pre-pruning  
é€šè¿‡æå‰åœæ­¢æ ‘çš„æ„å»ºå¯¹æ ‘å‰ªæï¼Œä¸€æ—¦åœæ­¢ï¼ŒèŠ‚ç‚¹å°±æ˜¯æ ‘å¶ï¼Œè¯¥æ ‘å¶æŒæœ‰å­é›†å…ƒç´ ä¸­æœ€é¢‘ç¹çš„ç±»ã€‚åœæ­¢å†³ç­–æ ‘ç”Ÿé•¿æœ€ç®€å•çš„æ–¹æ³•æœ‰ï¼š  
>1. å®šä¹‰ä¸€ä¸ªé«˜åº¦ï¼Œå½“å†³ç­–æ ‘è¾¾åˆ°è¯¥é«˜åº¦æ—¶å°±åœæ­¢å†³ç­–æ ‘çš„ç”Ÿé•¿
2. è¾¾åˆ°æŸä¸ªèŠ‚ç‚¹çš„å®ä¾‹å…·æœ‰ç›¸åŒçš„ç‰¹å¾å‘é‡ï¼ŒåŠæ—¶è¿™äº›å®ä¾‹ä¸å±äºåŒä¸€ç±»ï¼Œä¹Ÿå¯ä»¥åœæ­¢å†³ç­–æ ‘çš„ç”Ÿé•¿ã€‚è¿™ä¸ªæ–¹æ³•å¯¹äºå¤„ç†æ•°æ®çš„æ•°æ®å†²çªé—®é¢˜æ¯”è¾ƒæœ‰æ•ˆã€‚
3. å®šä¹‰ä¸€ä¸ªé˜ˆå€¼ï¼Œå½“è¾¾åˆ°æŸä¸ªèŠ‚ç‚¹çš„å®ä¾‹ä¸ªæ•°å°äºé˜ˆå€¼æ—¶å°±å¯ä»¥åœæ­¢å†³ç­–æ ‘çš„ç”Ÿé•¿
4. å®šä¹‰ä¸€ä¸ªé˜ˆå€¼ï¼Œé€šè¿‡è®¡ç®—æ¯æ¬¡æ‰©å¼ å¯¹ç³»ç»Ÿæ€§èƒ½çš„å¢ç›Š(ä¾‹å¦‚information gain > c?)ï¼Œå¹¶æ¯”è¾ƒå¢ç›Šå€¼ä¸è¯¥é˜ˆå€¼å¤§å°æ¥å†³å®šæ˜¯å¦åœæ­¢å†³ç­–æ ‘çš„ç”Ÿé•¿ã€‚


#### åå‰ªæ post-pruning  
é¦–å…ˆ**æ„é€ å®Œæ•´çš„å†³ç­–æ ‘ï¼Œå…è®¸æ ‘è¿‡åº¦æ‹Ÿåˆè®­ç»ƒæ•°æ®**ï¼Œç„¶åå¯¹é‚£äº›ç½®ä¿¡åº¦ä¸å¤Ÿçš„ç»“ç‚¹å­æ ‘ç”¨å¶å­ç»“ç‚¹æ¥ä»£æ›¿ï¼Œè¯¥å¶å­çš„ç±»æ ‡å·ç”¨è¯¥ç»“ç‚¹å­æ ‘ä¸­æœ€é¢‘ç¹çš„ç±»æ ‡è®°ã€‚ç›¸æ¯”äºå…ˆå‰ªæï¼Œè¿™ç§æ–¹æ³•æ›´å¸¸ç”¨ï¼Œæ­£æ˜¯å› ä¸ºåœ¨å…ˆå‰ªææ–¹æ³•ä¸­ç²¾ç¡®åœ°ä¼°è®¡ä½•æ—¶åœæ­¢æ ‘å¢é•¿å¾ˆå›°éš¾ã€‚å¸¸è§çš„åå‰ªææ–¹æ³•ï¼š  
>1. Reduced-Error Pruning(REP,é”™è¯¯ç‡é™ä½å‰ªæï¼‰
2. Pesimistic-Error Pruning(PEP,æ‚²è§‚é”™è¯¯å‰ªæï¼‰
3. Cost-Complexity Pruningï¼ˆCCPï¼Œä»£ä»·å¤æ‚åº¦å‰ªæ)
4. EBP(Error-Based Pruning)ï¼ˆåŸºäºé”™è¯¯çš„å‰ªæï¼‰  
<br />  

* Reduced Error Pruning (REP)  
REPæ–¹æ³•æ˜¯ä¸€ç§æ¯”è¾ƒç®€å•çš„åå‰ªæçš„æ–¹æ³•ï¼Œåœ¨è¯¥æ–¹æ³•ä¸­ï¼Œå¯ç”¨çš„æ•°æ®è¢«åˆ†æˆä¸¤ä¸ªæ ·ä¾‹é›†åˆï¼š**ä¸€ä¸ªè®­ç»ƒé›†ç”¨æ¥å½¢æˆå­¦ä¹ åˆ°çš„å†³ç­–æ ‘ï¼Œä¸€ä¸ªåˆ†ç¦»çš„éªŒè¯é›†ç”¨æ¥è¯„ä¼°è¿™ä¸ªå†³ç­–æ ‘åœ¨åç»­æ•°æ®ä¸Šçš„ç²¾åº¦**ï¼Œç¡®åˆ‡åœ°è¯´æ˜¯ç”¨æ¥è¯„ä¼°ä¿®å‰ªè¿™ä¸ªå†³ç­–æ ‘çš„å½±å“ã€‚  
è¿™ä¸ªæ–¹æ³•çš„åŠ¨æœºæ˜¯ï¼šå³ä½¿å­¦ä¹ å™¨å¯èƒ½ä¼šè¢«è®­ç»ƒé›†ä¸­çš„éšæœºé”™è¯¯å’Œå·§åˆè§„å¾‹æ‰€è¯¯å¯¼ï¼Œä½†**éªŒè¯é›†åˆä¸å¤§å¯èƒ½è¡¨ç°å‡ºåŒæ ·çš„éšæœºæ³¢åŠ¨**ã€‚æ‰€ä»¥éªŒè¯é›†å¯ä»¥ç”¨æ¥å¯¹è¿‡åº¦æ‹Ÿåˆè®­ç»ƒé›†ä¸­çš„è™šå‡ç‰¹å¾æä¾›é˜²æŠ¤æ£€éªŒã€‚  
REPæ˜¯æœ€ç®€å•çš„åå‰ªææ–¹æ³•ä¹‹ä¸€ï¼Œä¸è¿‡ç”±äºä½¿ç”¨ç‹¬ç«‹çš„æµ‹è¯•é›†ï¼ŒåŸå§‹å†³ç­–æ ‘ç›¸æ¯”ï¼Œä¿®æ”¹åçš„å†³ç­–æ ‘**å¯èƒ½åå‘äºè¿‡åº¦ä¿®å‰ª**ã€‚
<br />  
  
* Minimal cost-complexity  pruning ä»£ä»·-å¤æ‚åº¦å‰ªæ  
$R_\alpha (T) = R(T)+\alpha |\tilde{T}|$  
å…¶ä¸­$T$ä»£è¡¨æ ‘æ¨¡å‹ï¼Œ$R(T)$æ˜¯è¯¯å·®å¤§å°ï¼Œ$|\tilde{T}|$è¡¨ç¤ºæ ‘çš„å¶èŠ‚ç‚¹ä¸ªæ•°ï¼Œå³è¡¨ç¤ºæ ‘çš„å¤æ‚åº¦ã€‚  
$\alpha$æ˜¯å¯¹å¤æ‚åº¦çš„æƒ©ç½šç³»æ•°ï¼Œå¯ä»¥ç”±ç”¨æˆ·è®¾ç½®ï¼š  
 * $\alpha$è¶Šå¤§ï¼Œå¶èŠ‚ç‚¹çš„ä¸ªæ•°å¯¹æŸå¤±å‡½æ•°çš„å½±å“è¶Šå¤§ï¼Œå‰ªæä¹‹åçš„å†³ç­–æ ‘æ›´å®¹æ˜“é€‰æ‹©å¤æ‚åº¦è¾ƒå°çš„æ ‘ï¼›
 * $\alpha$è¶Šå°ï¼Œè¡¨ç¤ºå¶èŠ‚ç‚¹çš„ä¸ªæ•°å¯¹æŸå¤±å‡½æ•°çš„å½±å“è¶Šå°ã€‚



* CCPç®—æ³•çš„æ­¥éª¤  
 1. å¯¹äºå®Œå…¨å†³ç­–æ ‘$ğ‘‡$çš„æ¯ä¸ªéå¶ç»“ç‚¹è®¡ç®—$\alpha$å€¼ï¼Œå¾ªç¯å‰ªæ‰å…·æœ‰æœ€å°$\alpha$å€¼çš„å­æ ‘ï¼Œç›´åˆ°å‰©ä¸‹æ ¹èŠ‚ç‚¹ã€‚åœ¨è¯¥æ­¥å¯å¾—åˆ°**ä¸€ç³»åˆ—çš„å‰ªææ ‘$ï½›ğ‘‡_0ï¼Œğ‘‡_1ï¼Œğ‘‡_2, â€¦, ğ‘‡_ğ‘šï½$**,å…¶ä¸­$ğ‘‡_0$ä¸ºåŸæœ‰çš„å®Œå…¨å†³ç­–æ ‘ï¼Œ$ğ‘‡_ğ‘š$ä¸ºæ ¹ç»“ç‚¹ï¼Œ$ğ‘‡_(ğ‘–+1)$ä¸ºå¯¹$ğ‘‡_ğ‘–$è¿›è¡Œå‰ªæçš„ç»“æœï¼›
 2. **ä»å­æ ‘åºåˆ—ä¸­**ï¼Œæ ¹æ®çœŸå®çš„è¯¯å·®ä¼°è®¡é€‰æ‹©æœ€ä½³å†³ç­–æ ‘ã€‚

#### é¢„å‰ªæå’Œåå‰ªæçš„æ¯”è¾ƒ
* åå‰ªæå†³ç­–æ ‘é€šå¸¸æ¯”é¢„å‰ªæå†³ç­–æ ‘ä¿ç•™äº†æ›´å¤šçš„åˆ†æ”¯ï¼›
* åå‰ªæå†³ç­–æ ‘çš„æ¬ æ‹Ÿåˆé£é™©å¾ˆå°ï¼Œæ³›åŒ–æ€§èƒ½å¾€å¾€ä¼˜äºé¢„å‰ªæå†³ç­–æ ‘ï¼›
* åå‰ªæå†³ç­–æ ‘è®­ç»ƒæ—¶é—´å¼€é”€æ¯”æœªå‰ªæå†³ç­–æ ‘å’Œé¢„å‰ªæå†³ç­–æ ‘éƒ½è¦å¤§çš„å¤šã€‚

#### ä¸€å€æ ‡å‡†å·®è§„åˆ™  
åœ¨å‰ªæç†è®ºä¸­ï¼Œæ¯”è¾ƒè‘—åçš„è§„åˆ™å°±æ˜¯1-SEï¼ˆ1æ ‡å‡†å·®ï¼‰è§„åˆ™ã€‚  
é¦–å…ˆè¦ä¿è¯é¢„æµ‹è¯¯å·®ï¼ˆé€šè¿‡äº¤å‰éªŒè¯è·å¾—ï¼Œåœ¨ç¨‹åºä¸­è¡¨ç¤ºä¸ºxerrorï¼‰å°½é‡å°ï¼Œä½†ä¸ä¸€å®šè¦å–æœ€å°å€¼ï¼Œè€Œæ˜¯å…è®¸å®ƒåœ¨â€œæœ€å°çš„è¯¯å·®Â±ä¸€ä¸ªæ ‡å‡†å·®â€çš„èŒƒå›´å†…ï¼Œç„¶ååœ¨æ­¤èŒƒå›´å†…é€‰å–å°½é‡å°çš„å¤æ‚æ€§å‚é‡ï¼Œè¿›è€Œä»¥å®ƒä¸ºä¾æ®è¿›è¡Œå‰ªæã€‚  
è¿™ä¸ªè§„åˆ™ä½“ç°äº†**å…¼é¡¾æ ‘çš„è§„æ¨¡ï¼ˆå¤æ‚æ€§ï¼‰å’Œè¯¯å·®å¤§å°çš„æ€æƒ³**ã€‚ä¸€èˆ¬è¯´æ¥ï¼Œéšç€æ‹†åˆ†çš„å¢å¤šï¼Œå¤æ‚æ€§å‚é‡ä¼šå•è°ƒä¸‹é™ï¼ˆçº¯åº¦è¶Šæ¥è¶Šé«˜ï¼‰ï¼Œä½†æ˜¯é¢„æµ‹è¯¯å·®åˆ™ä¼šå…ˆé™åå‡ï¼Œè¿™æ ·ï¼Œæˆ‘ä»¬å°±**æ— æ³•ä½¿å¤æ‚æ€§å’Œè¯¯å·®åŒæ—¶é™åˆ°æœ€ä½**ï¼Œå› æ­¤å…è®¸è¯¯å·®å¯ä»¥åœ¨ä¸€ä¸ªæ ‡å‡†å·®å†…æ³¢åŠ¨ã€‚

#### å‰ªæçš„Rå®ç°


```R
if (FALSE){ #æ³•1
    model2=snip.rpart(model1,toss=2)
    opt(6,4)
    rpart.plot(model2)
}
if (TRUE) { #æ³•2
    model2=prune(model1, cp=.01)
    opt(6,4)
    rpart.plot(model2,cex=.8)
}
```


![png](/assets/images/201906DataMining/output_134_0.png)


### æ£€éªŒ


```R
# æ··æ·†çŸ©é˜µ
class=predict(model2, newdata=test, type="class")
table(class, test$Species, dnn=c("Pred","True"))
```


                True
    Pred         setosa versicolor virginica
      setosa         12          0         0
      versicolor      0         14         2
      virginica       0          1        16


å¤šæ£µæ ‘ï¼šæŒæ¡åŸºæœ¬æ€æƒ³ã€é€‚ç”¨æƒ…å†µï¼ˆå¼ºåˆ†ç±»å™¨ï¼Ÿå¼±åˆ†ç±»å™¨ï¼Ÿå¤šå¼±å¤šå¼ºï¼Ÿï¼‰

## boosting

Gradient Boosting | R - https://blog.csdn.net/dingming001/article/details/72989547  
XGBoost | R - https://blog.csdn.net/qq_26074991/article/details/51737030  
XGBoost - https://blog.csdn.net/qq_34264472/article/details/53363384  
XGBoost - https://blog.csdn.net/langyichao1/article/details/70183127

## bagging


**æ­¥éª¤**  
è®¾è®­ç»ƒæ ·æœ¬é›†åˆLä¸º${(x_n,y_n),n=1,2,3,...,N}$ï¼Œ$x_n$ä¸º$p$ç»´å‘é‡ï¼Œ$y_n$ä¸ºå› å˜é‡ï¼Œæ˜¯å–å€¼${1,2,...,J}$çš„åˆ†ç±»å˜é‡ã€‚  
æ„å»º$M$æ£µå†³ç­–æ ‘$H_m(x,L_m),m=1,2,...,M$ï¼Œç»„åˆ$M$æ£µæ ‘å¾—åˆ°æœ€ç»ˆåˆ†ç±»å™¨$H_{agg}$ã€‚  
  
1. å¯¹æ ·æœ¬é›†åˆ$L$è¿›è¡ŒBootstrapæŠ½æ ·å¾—åˆ°è®­ç»ƒæ ·æœ¬é›†$L_m$  
>å¯¹${(x_n,y_n),n=1,2,...,N}$ä¸­çš„$N$ä¸ªæ ·æœ¬ç‚¹è¿›è¡Œæ¦‚ç‡ä¸º$\frac{1}{N}$çš„ç­‰æ¦‚ç‡æœ‰æ”¾å›æŠ½æ ·ï¼Œæ ·æœ¬é‡ä¸º$N$ã€‚  

å¯¹$L_m$æ„å»ºåˆ†ç±»å™¨ï¼ˆå†³ç­–æ ‘ï¼‰$H_m(x,Lm)$  
2. ç»„åˆ$M$æ£µå†³ç­–æ ‘å¾—åˆ°æœ€ç»ˆåˆ†ç±»å™¨$H_B$ï¼Œ$H_B$å¯¹æ ·æœ¬xçš„é¢„æµ‹ä¸º$argmax_j \sum_{m=1}^M {I(H_m(x,L_m)=j)}$, $I()$ä¸ºç¤ºæ€§å‡½æ•°  

* å¦‚æœç”ŸæˆåŸºé¢„æµ‹å™¨çš„ç®—æ³•æ˜¯ä¸ç¨³å®šçš„(unstable)ï¼Œé€šè¿‡baggingå¾—åˆ°çš„æœ€ç»ˆé¢„æµ‹æ¨¡å‹è™½ç„¶å¤±å»äº†ç®€å•çš„å¯è§£é‡Šçš„æ ‘å‹ç»“æ„ï¼Œä½†é¢„æµ‹ç²¾åº¦å¾€å¾€ä¼šå¤§å¤§é«˜äºå•ä¸ªåŸºé¢„æµ‹å™¨çš„é¢„æµ‹ç²¾åº¦ã€‚  
* Baggingç®—æ³•å¯¹äºåŸºé¢„æµ‹å™¨ä¸ç¨³å®šçš„æƒ…å†µå¾ˆæœ‰ä½œç”¨ï¼Œå¯¹äºç¨³å®šçš„åŸºé¢„æµ‹å™¨ï¼Œbaggingå¹¶ä¸æœ‰æ•ˆã€‚  
* ä»åå·®æ–¹å·®åˆ†è§£çš„è§’åº¦çœ‹ï¼ŒBaggingç®—æ³•å¯ä»¥æé«˜ä¸ç¨³å®šåŸºé¢„æµ‹å™¨çš„é¢„æµ‹ç²¾åº¦ï¼Œå®è´¨ä¸Šæ˜¯å‡å°äº†é¢„æµ‹çš„æ–¹å·®(variance)ï¼Œä½†å¹¶æ²¡æœ‰é™ä½åå·®(bias)ã€‚  
> * ä¸ç¨³å®šçš„ç®—æ³•: å†³ç­–æ ‘ï¼Œç¥ç»ç½‘ç»œï¼ŒMARS (multivariate splines)ï¼Œå’Œå­é›†å›å½’(subset regression)ç­‰ç­‰ï¼›  
* ç¨³å®šçš„ç®—æ³•: å²­å›å½’(ridge regression)ï¼Œæœ€è¿‘é‚»æ–¹æ³•(K-nearest neighbor)ï¼Œå’Œçº¿æ€§åˆ¤åˆ«æ–¹æ³•(Linear discriminate)ç­‰ç­‰ã€‚  


* æ¯æ¬¡è¿›è¡ŒbootstrapæŠ½æ ·çš„æ—¶å€™ï¼Œæˆ‘ä»¬é€‰æ‹©çš„æ ·æœ¬é‡ç›¸ç­‰äºåŸå§‹è®­ç»ƒé›†çš„æ ·æœ¬é‡ã€‚å› ä¸ºbootstrapæ˜¯æœ‰æ”¾å›çš„é‡å¤æŠ½æ ·ï¼Œæ‰€ä»¥æœ‰äº›æ ·æœ¬ç‚¹è¢«æŠ½ä¸­çš„æ¬¡æ•°è¶…è¿‡ä¸€æ¬¡ï¼Œæœ‰äº›æ ·æœ¬ç‚¹æ²¡æœ‰è¢«æŠ½ä¸­ã€‚æ ¹æ®bootstrapæŠ½æ ·çš„ç†è®ºï¼Œå½“æ ·æœ¬é‡ä¸º$N$æ—¶ï¼Œå¤§çº¦æœ‰37%çš„æ ·æœ¬ç‚¹æ²¡æœ‰è¢«æŠ½ä¸­ã€‚  
* ç ”ç©¶è¡¨æ˜ï¼Œæé«˜bootstrapæŠ½æ ·çš„æ ·æœ¬é‡å¹¶ä¸èƒ½æ”¹å–„baggingç®—æ³•çš„é¢„æµ‹ç²¾åº¦ã€‚  

## AdaBoost

è‡ªä»AdaBoost æ–¹æ³•æå‡ºä¹‹åï¼Œå¤§é‡æ¨¡æ‹Ÿå’Œåº”ç”¨ç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•å¯ä»¥å¤§å¤§é™ä½åŸºåˆ†ç±»å™¨(æ¯”å¦‚å†³ç­–æ ‘)çš„é¢„æµ‹è¯¯å·®ï¼ŒBreiman(1996)æ›¾æŒ‡å‡ºAdaBoost æ–¹æ³•æ˜¯ç°æœ‰çš„åˆ†ç±»æ–¹æ³•ä¸­æœ€å¥½çš„ä¸€ç§ã€‚Breiman(1996)è®¤ä¸ºæœ€ä¸»è¦çš„åŸå› æ˜¯AdaBoostæ–¹æ³•çš„**è‡ªé€‚åº”çš„é‡æ–°æŠ½æ ·çš„æ–¹æ³•**ï¼Œè€Œä¸æ˜¯å…¶å…·ä½“å®šä¹‰çš„æ›´æ–°p(n)çš„å…¬å¼ã€‚

<font color='#9a4238'>äºŒåˆ†ç±»Adaboostç®—æ³•æ˜¯æœ€å°åŒ–æŒ‡æ•°æŸå¤±$L(y,f(x))=\exp{(-yf(x))}$çš„åˆ†æ­¥å‘å‰å¯åŠ æ¨¡å‹ã€‚</font>


```R
rm(list=ls())
library(adabag)

data(iris)
n=nrow(iris)
loc=sample(n, .7*n)
train=iris[loc,]
test=iris[-loc,]
```

    Warning message:
    "package 'adabag' was built under R version 3.5.3"Loading required package: rpart
    Loading required package: caret
    Warning message:
    "package 'caret' was built under R version 3.5.3"Loading required package: lattice
    Loading required package: ggplot2
    Loading required package: foreach
    Warning message:
    "package 'foreach' was built under R version 3.5.3"Loading required package: doParallel
    Warning message:
    "package 'doParallel' was built under R version 3.5.3"Loading required package: iterators
    Warning message:
    "package 'iterators' was built under R version 3.5.3"Loading required package: parallel
    


```R
if (TRUE){
    err=c()
    for (i in 1:20){ #åŸºåˆ†ç±»å™¨ä¸ªæ•°ä»1åˆ°20
      print(paste0("i=",i))
      iada=boosting(Species~., data=train, mfinal=i)
      ipred=predict.boosting(iada, newdata=test)
      err=c(err, ipred$error)
    }
    derr=data.frame(i=1:20, error=err); derr
}

mycol=RColorBrewer::brewer.pal(5,"Set1")[2]
fig0=ggplot(derr)+
  theme_bw()+
  geom_point(aes(i, error), shape=19, size=2, col=mycol)+
  geom_line(aes(i, error), col=mycol)+
  scale_x_continuous(breaks=seq(1,20,1))+
  xlab("The Number of Basic Classifiers")+ylab("Error")
fig0
```


<table>
<thead><tr><th scope="col">i</th><th scope="col">error</th></tr></thead>
<tbody>
	<tr><td> 1        </td><td>0.06666667</td></tr>
	<tr><td> 2        </td><td>0.02222222</td></tr>
	<tr><td> 3        </td><td>0.06666667</td></tr>
	<tr><td> 4        </td><td>0.06666667</td></tr>
	<tr><td> 5        </td><td>0.02222222</td></tr>
	<tr><td> 6        </td><td>0.04444444</td></tr>
	<tr><td> 7        </td><td>0.02222222</td></tr>
	<tr><td> 8        </td><td>0.06666667</td></tr>
	<tr><td> 9        </td><td>0.02222222</td></tr>
	<tr><td>10        </td><td>0.04444444</td></tr>
	<tr><td>11        </td><td>0.08888889</td></tr>
	<tr><td>12        </td><td>0.06666667</td></tr>
	<tr><td>13        </td><td>0.04444444</td></tr>
	<tr><td>14        </td><td>0.06666667</td></tr>
	<tr><td>15        </td><td>0.06666667</td></tr>
	<tr><td>16        </td><td>0.06666667</td></tr>
	<tr><td>17        </td><td>0.04444444</td></tr>
	<tr><td>18        </td><td>0.04444444</td></tr>
	<tr><td>19        </td><td>0.04444444</td></tr>
	<tr><td>20        </td><td>0.04444444</td></tr>
</tbody>
</table>






![png](/assets/images/201906DataMining/output_148_2.png)



```R
#é€‰å–i=9çš„æ¨¡å‹
iada=boosting(Species~., data=train, mfinal=10)
ipred=predict.boosting(iada, newdata=test)

ipred$confusion #æ··æ·†çŸ©é˜µ
#ç›¸å½“äºtable(dft$Species1,dft$Species0, dnn=c("Observed","Predicted"))
```


                   Observed Class
    Predicted Class setosa versicolor virginica
         setosa         15          0         0
         versicolor      0         11         3
         virginica       0          0        16



```R
importanceplot(iada)
```


![png](/assets/images/201906DataMining/output_150_0.png)


## Random Forest

ä¸baggingåªåœ¨æ•°æ®ï¼ˆè¡Œï¼‰çš„å±‚é¢ä¸Šéšæœºä¸åŒï¼Œ<font color='#9a4238'>éšæœºæ£®æ—åœ¨å˜é‡ï¼ˆåˆ—ï¼‰çš„ä½¿ç”¨å’Œæ•°æ®ï¼ˆè¡Œï¼‰çš„ä½¿ç”¨ä¸Šéƒ½è¿›è¡ŒéšæœºåŒ–ï¼Œç”Ÿæˆå¤šé¢—åˆ†ç±»æ ‘ï¼Œæœ€åæ±‡æ€»å…¶ç»“æœã€‚</font> è¿™æ ·éšæœºæ£®æ—ç®—æ³•å°±å¯ä»¥**é™ä½æ ‘ä¸æ ‘ä¹‹é—´çš„ç›¸å…³å…³ç³»**ï¼Œå¹¶ä¸”å°½å¯èƒ½çš„æ§åˆ¶å¹³å‡æ–¹å·®ã€‚  

<br />  
å¯¹äºæ¯æ£µæ ‘$ğ‘=1,2,â€¦,ğµ$,  
1. ä»å…¨éƒ¨è®­ç»ƒæ ·æœ¬å•å…ƒä¸­ï¼Œé‡‡ç”¨bootstrapæŠ½æ ·æ–¹æ³•æŠ½å–nä¸ªæ ·æœ¬å•å…ƒæ„æˆbootstrapæ ·æœ¬é›† $ğ‘^âˆ—$, åŸºäº$ğ‘^âˆ—$æ„é€ ä¸€æ£µæ ‘$ğ»_ğ‘$ï¼Œ  
2. å¯¹æ ‘ä¸Šçš„**æ¯ä¸ªèŠ‚ç‚¹**ï¼Œé‡å¤ä»¥ä¸‹æ­¥éª¤ï¼Œç›´åˆ°èŠ‚ç‚¹çš„æ ·æœ¬æ•°è¾¾åˆ°æŒ‡å®šçš„æœ€å°é™å®šå€¼ğ‘›_ğ‘šğ‘–ğ‘›ï¼š  
 * ä»å…¨éƒ¨pä¸ªå˜é‡ä¸­éšæœºé€‰å–mä¸ªï¼›
 * ä»mä¸ªéšæœºå˜é‡ä¸­å–æœ€ä½³çš„åˆ†æå˜é‡ï¼›
 * åœ¨è¿™ä¸€èŠ‚ç‚¹åˆ†è£‚æˆä¸¤ä¸ªå­èŠ‚ç‚¹ã€‚  
 
<br />
 éšæœºæ£®æ—åœ¨è¿ç®—é‡æ²¡æœ‰æ˜¾è‘—æé«˜çš„å‰æä¸‹æé«˜äº†é¢„æµ‹ç²¾åº¦ã€‚éšæœºæ£®æ—å¯¹å¤šå…ƒçº¿æ€§ä¸æ•æ„Ÿï¼Œç»“æœå¯¹ç¼ºå¤±æ•°æ®å’Œéå¹³è¡¡çš„æ•°æ®æ¯”è¾ƒç¨³å¥ï¼Œå¯ä»¥å¾ˆå¥½åœ°é¢„æµ‹å¤šè¾¾å‡ åƒä¸ªè§£é‡Šå˜é‡çš„ä½œç”¨ï¼ˆé€‚ç”¨äºå˜é‡å¤šæƒ…å½¢ï¼‰ã€‚  


```R
rm(list=ls())
library(randomForest)

data(iris)
n=nrow(iris)
loc=sample(n,.7*n)
train=iris[loc,]
test=iris[-loc,]
```


```R
# set.seed(71) # why do we need to set seed here?
iris.rf <- randomForest(Species ~ ., data=train, importance=TRUE, proximity=TRUE)
print(iris.rf)
```

    
    Call:
     randomForest(formula = Species ~ ., data = train, importance = TRUE,      proximity = TRUE) 
                   Type of random forest: classification
                         Number of trees: 500
    No. of variables tried at each split: 2
    
            OOB estimate of  error rate: 3.81%
    Confusion matrix:
               setosa versicolor virginica class.error
    setosa         34          0         0  0.00000000
    versicolor      0         35         2  0.05405405
    virginica       0          2        32  0.05882353
    


```R
#Look at variable importance:
round(importance(iris.rf), 2)
```


<table>
<thead><tr><th></th><th scope="col">setosa</th><th scope="col">versicolor</th><th scope="col">virginica</th><th scope="col">MeanDecreaseAccuracy</th><th scope="col">MeanDecreaseGini</th></tr></thead>
<tbody>
	<tr><th scope="row">Sepal.Length</th><td> 6.74</td><td> 8.64</td><td> 6.16</td><td>10.42</td><td> 6.41</td></tr>
	<tr><th scope="row">Sepal.Width</th><td> 3.98</td><td> 1.87</td><td> 0.93</td><td> 3.15</td><td> 1.51</td></tr>
	<tr><th scope="row">Petal.Length</th><td>22.24</td><td>31.32</td><td>26.03</td><td>33.15</td><td>31.05</td></tr>
	<tr><th scope="row">Petal.Width</th><td>22.14</td><td>32.65</td><td>30.06</td><td>34.41</td><td>30.34</td></tr>
</tbody>
</table>




```R
ipred=predict(iris.rf, newdata=test)
table(ipred, test$Species, dnn=c("Pred","True"))
```


                True
    Pred         setosa versicolor virginica
      setosa         16          0         0
      versicolor      0         11         0
      virginica       0          2        16
